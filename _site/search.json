[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About SFU Summer School 2023",
    "section": "",
    "text": "This spring school is offered by Simon Fraser University on behalf of Western Universities and the Digital Research Alliance of Canada. It is hosted by SFU’s Big Data Hub and is open to all researchers at SFU and other Canadian post-secondary institutions."
  },
  {
    "objectID": "about.html#registration",
    "href": "about.html#registration",
    "title": "About SFU Summer School 2023",
    "section": "Registration",
    "text": "Registration\nRegistration will open soon and a link to the registration page will be posted here."
  },
  {
    "objectID": "about.html#instructors",
    "href": "about.html#instructors",
    "title": "About SFU Summer School 2023",
    "section": "Instructors",
    "text": "Instructors\n\n\n\n\n\n\nAlex Razoumov earned his PhD in computational astrophysics from the University of British Columbia and held postdoctoral positions in Urbana-Champaign, San Diego, Oak Ridge, and Halifax. He spent five years as HPC Analyst in SHARCNET and in 2014 moved back to Vancouver to focus on scientific visualization and training researchers to use advanced computing tools. Alex is currently at Simon Fraser University.\n\n\n\n\n\n\n\n\n\n\n\n\nEvolutionary and behavioural ecologist by training, Software/Data Carpentry instructor, and open source advocate, Marie-Hélène Burle develops and delivers training for researchers on high-performance computing tools (R, Python, Julia, Git, Bash scripting, machine learning, parallel scientific programming, and HPC) for Simon Fraser University and the Digital Research Alliance of Canada."
  },
  {
    "objectID": "bash/index.html",
    "href": "bash/index.html",
    "title": "Bash",
    "section": "",
    "text": "Date:\nMonday, June 19, 2023\nTime:\n9am–noon\nInstructor:\nAlex Razoumov (Simon Fraser University)\nPrerequisites:\nThis introductory course does not require any previous experience.\nSoftware:\nWe will provide access to one of our Linux systems. To make use of it, attendees will need a remote secure shell (SSH) client installed on their computer. On Windows we recommend the free Home Edition of MobaXterm. On Mac and Linux computers, SSH is usually pre-installed (try typing ssh in a terminal to make sure it is there).\n\n Course notes can be found here."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "Please email us at: training at westdri dot ca.\nTo email the UVic organizing team, please use: xxx."
  },
  {
    "objectID": "hpc/index.html",
    "href": "hpc/index.html",
    "title": "HPC",
    "section": "",
    "text": "Date:\nMonday, June 19, 2023\nTime:\n2pm–5pm\nInstructor:\nAlex Razoumov (Simon Fraser University)\nPrerequisites:\nWorking knowledge of the Bash shell.\nSoftware:\nWe will provide access to one of our Linux systems. To make use of it, attendees will need a remote secure shell (SSH) client installed on their computer. On Windows we recommend the free Home Edition of MobaXterm. On Mac and Linux computers, SSH is usually pre-installed (try typing ssh in a terminal to make sure it is there).\n\n Course notes can be found here."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SFU Summer School 2023",
    "section": "",
    "text": "Each day we will have morning (9am – noon) and afternoon (2pm – 5pm) sessions. There will be short coffee breaks at 10:30am and 3:30pm with coffee and snacks covered by the registration fee. Participants will have to find lunch on their own.\nYou can attend as many or as few of the courses as you want. All sessions will be in-person at the Big Data Hub at SFU’s main (Burnaby) campus, in the Applied Sciences Building (top floor).\n\n\nTo participate in the hands-on exercises, you will need to bring your own laptop. We will provide guest accounts on our HPC training cluster. For more specific requirements, please see the individual course pages.\n\n\n\n\nMondayJune 19, 2023\n\n\n\n\nBash\nHalf-day introduction to Bash & the Unix shell\n\n\n\n\nHPC\nHalf-day introduction to high-performance research computing\n\n\n\n\n\n\nTuesdayJune 20, 2023\n\n\n\n\nIntro R\nHalf-day introduction to programming in R\n\n\n\n\nParallel R\nHalf-day introduction to high-performance R\n\n\n\n\n\n\nWednesdayJune 21, 2023\n\n\n\n\nParallel Julia\nFull-day introduction to parallel programming in Julia\n\n\n\n\n\n\nThursdayJune 22, 2023\n\n\n\n\nDeep learning with PyTorch\nFull-day introduction to deep learning with the PyTorch framework\n\n\n\n\n\n\nFridayJune 23, 2023\n\n\n\n\nScientific visualization with ParaView\nFull-day introduction to scientific visualization with ParaView\n\n\n\n\n \n\n\nHosted by:"
  },
  {
    "objectID": "julia/index.html",
    "href": "julia/index.html",
    "title": "Parallel Julia",
    "section": "",
    "text": "Date:\nWednesday, June 21, 2023\nTime:\n9am–5pm (with a two-hour break from noon to 2pm)\nInstructor:\nAlex Razoumov (Simon Fraser University)\nPrerequisites:  Basic working knowledge of HPC (how to submit Slurm jobs and view their output). Some scientific programming experience (in any language) would be ideal, but we will start slowly explaining basic principles along the way. No previous parallel programming experience needed.\nSoftware:\nWe will provide access to one of our Linux systems. To make use of it, attendees will need a remote secure shell (SSH) client installed on their computer. On Windows we recommend the free Home Edition of MobaXterm. On Mac and Linux computers, SSH is usually pre-installed (try typing ssh in a terminal to make sure it is there). You don’t need to install Julia on your computer, unless you want to.\n\n Course notes can be found here."
  },
  {
    "objectID": "ml/autograd.html",
    "href": "ml/autograd.html",
    "title": "Automatic differentiation",
    "section": "",
    "text": "PyTorch has automatic differentiation capabilities—meaning that it can track all the operations performed on tensors during the forward pass and compute all the gradients automatically for the backpropagation—thanks to its package torch.autograd.\nLet’s have a look at this."
  },
  {
    "objectID": "ml/autograd.html#some-definitions",
    "href": "ml/autograd.html#some-definitions",
    "title": "Automatic differentiation",
    "section": "Some definitions",
    "text": "Some definitions\nDerivative of a function:\nRate of change of a function with a single variable w.r.t. its variable.\nPartial derivative:\nRate of change of a function with multiple variables w.r.t. one variable while other variables are considered as constants.\nGradient:\nVector of partial derivatives of function with several variables.\nDifferentiation:\nCalculation of the derivatives of a function.\nChain rule:\nFormula to calculate the derivatives of composite functions.\nAutomatic differentiation:\nAutomatic computation of partial derivatives by algorithms."
  },
  {
    "objectID": "ml/autograd.html#backpropagation",
    "href": "ml/autograd.html#backpropagation",
    "title": "Automatic differentiation",
    "section": "Backpropagation",
    "text": "Backpropagation\nFirst, we need to talk about backpropagation: the backward pass following each forward pass and which adjusts the model’s parameters to minimize the output of the loss function.\nThe last 2 videos of 3Blue1Brown neural network series explains backpropagation and its manual calculation very well.\n\nWhat is backpropagation? (14 min)\n\n\nThere is one minor terminological error in this video: they call the use of mini-batches stochastic gradient descent. In fact, this is called mini-batch gradient descent. Stochastic gradient descent uses a single example at each iteration.\n\n\n\nHow does backpropagation work? (10 min)"
  },
  {
    "objectID": "ml/autograd.html#automatic-differentiation",
    "href": "ml/autograd.html#automatic-differentiation",
    "title": "Automatic differentiation",
    "section": "Automatic differentiation",
    "text": "Automatic differentiation\nIf we had to do all this manually, it would be absolute hell. Thankfully, many tools—including PyTorch—can do this automatically.\n\nTracking computations\nFor the automation of the calculation of all those derivatives through chain rules, PyTorch needs to track computations during the forward pass.\nPyTorch does not however track all the computations on all the tensors (this would be extremely memory intensive!). To start tracking computations on a vector, set the requires_grad attribute to True:\n\nimport torch\n\nx = torch.ones(2, 4, requires_grad=True)\nx\n\ntensor([[1., 1., 1., 1.],\n        [1., 1., 1., 1.]], requires_grad=True)\n\n\n\nThe grad_fun attribute\nWhenever a tensor is created by an operation involving a tracked tensor, it has a grad_fun attribute:\n\ny = x + 1\ny\n\ntensor([[2., 2., 2., 2.],\n        [2., 2., 2., 2.]], grad_fn=&lt;AddBackward0&gt;)\n\n\n\ny.grad_fn\n\n&lt;AddBackward0 at 0x7fc6e43a8370&gt;\n\n\n\n\nJudicious tracking\nYou don’t want to track more than is necessary. There are multiple ways to avoid tracking what you don’t want.\nYou can stop tracking computations on a tensor with the method detach:\n\nx\n\ntensor([[1., 1., 1., 1.],\n        [1., 1., 1., 1.]], requires_grad=True)\n\n\n\nx.detach_()\n\ntensor([[1., 1., 1., 1.],\n        [1., 1., 1., 1.]])\n\n\nYou can change its requires_grad flag:\n\nx = torch.zeros(2, 3, requires_grad=True)\nx\n\ntensor([[0., 0., 0.],\n        [0., 0., 0.]], requires_grad=True)\n\n\n\nx.requires_grad_(False)\n\ntensor([[0., 0., 0.],\n        [0., 0., 0.]])\n\n\nAlternatively, you can wrap any code you don’t want to track under with torch.no_grad():\n\nx = torch.ones(2, 4, requires_grad=True)\n\nwith torch.no_grad():\n    y = x + 1\n\ny\n\ntensor([[2., 2., 2., 2.],\n        [2., 2., 2., 2.]])\n\n\n\nCompare this with what we just did above.\n\n\n\n\nCalculating gradients\nLet’s calculate gradients manually, then use autograd, in a very simple case: imagine that \\(x\\), \\(y\\), and \\(z\\) are tensors containing the parameters of a model and that the error \\(e\\) could be calculated with the equation:\n\\[e=2x^4-y^3+3z^2\\]\n\nManual derivative calculation\nLet’s see how we would do this manually.\nFirst, we need the model parameters tensors:\n\nx = torch.tensor([1., 2.])\ny = torch.tensor([3., 4.])\nz = torch.tensor([5., 6.])\n\nWe calculate \\(e\\) following the above equation:\n\ne = 2*x**4 - y**3 + 3*z**2\n\nThe gradients of the error \\(e\\) w.r.t. the parameters \\(x\\), \\(y\\), and \\(z\\) are:\n\\[\\frac{de}{dx}=8x^3\\] \\[\\frac{de}{dy}=-3y^2\\] \\[\\frac{de}{dz}=6z\\]\nWe can calculate them with:\n\ngradient_x = 8*x**3\ngradient_x\n\ntensor([ 8., 64.])\n\n\n\ngradient_y = -3*y**2\ngradient_y\n\ntensor([-27., -48.])\n\n\n\ngradient_z = 6*z\ngradient_z\n\ntensor([30., 36.])\n\n\n\n\nAutomatic derivative calculation\nFor this method, we need to define our model parameters with requires_grad set to True:\n\nx = torch.tensor([1., 2.], requires_grad=True)\ny = torch.tensor([3., 4.], requires_grad=True)\nz = torch.tensor([5., 6.], requires_grad=True)\n\n\\(e\\) is calculated in the same fashion (except that here, all the computations on \\(x\\), \\(y\\), and \\(z\\) are tracked):\n\ne = 2*x**4 - y**3 + 3*z**2\n\nThe backward propagation is done automatically with:\n\ne.backward(torch.tensor([1., 1.]))\n\nAnd we have our 3 partial derivatives:\n\nprint(x.grad)\nprint(y.grad)\nprint(z.grad)\n\ntensor([ 8., 64.])\ntensor([-27., -48.])\ntensor([30., 36.])\n\n\n\n\nComparison\nThe result is the same, as can be tested with:\n\n8*x**3 == x.grad\n\ntensor([True, True])\n\n\n\n-3*y**2 == y.grad\n\ntensor([True, True])\n\n\n\n6*z == z.grad\n\ntensor([True, True])\n\n\nOf course, calculating the gradients manually here was extremely easy, but imagine how tedious and lengthy it would be to write the chain rules to calculate the gradients of all the composite functions in a neural network manually…"
  },
  {
    "objectID": "ml/checkpoints.html",
    "href": "ml/checkpoints.html",
    "title": "Saving/loading models and checkpointing",
    "section": "",
    "text": "After you have trained your model, obviously you will want to save it to use it thereafter. You will then need to load it in any session in which you want to use it.\nIn addition to saving or loading a fully trained model, it is important to know how to create regular checkpoints: training ML models takes a long time and a cluster crash or countless other issues might interrupt the training process. You don’t want to have to restart from scratch if that happens."
  },
  {
    "objectID": "ml/checkpoints.html#savingloading-models",
    "href": "ml/checkpoints.html#savingloading-models",
    "title": "Saving/loading models and checkpointing",
    "section": "Saving/loading models",
    "text": "Saving/loading models\n\nSaving models\nYou can save a model by serializing its internal state dictionary. The state dictionary is a Python dictionary that contains the learnable parameters of your model (weights and biases).\ntorch.save(model.state_dict(), \"model.pt\")\n\n\nLoading models\nTo recreate your model, you first need to recreate its structure:\nmodel = TheModelClass(*args, **kwargs)\nThen you can load the state dictionary containing the parameters values into it:\nmodel.load_state_dict(torch.load(\"model.pt\"))\nAssuming you want to use your model for inference, you also must run:\nmodel.eval()\n\nIf instead you want to do more training on your model, you would of course run model.train() instead."
  },
  {
    "objectID": "ml/checkpoints.html#checkpointing",
    "href": "ml/checkpoints.html#checkpointing",
    "title": "Saving/loading models and checkpointing",
    "section": "Checkpointing",
    "text": "Checkpointing\n\nCreating a checkpoint\ntorch.save({\n    'epoch': epoch,\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    'loss': loss,\n    ...\n}, \"model.pt\")\n\n\nResuming training from a checkpoint\nRecreate the state of your model from the checkpoint:\nmodel = TheModelClass(*args, **kwargs)\noptimizer = TheOptimizerClass(*args, **kwargs)\n\ncheckpoint = torch.load(\"model.pt\")\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\nepoch = checkpoint['epoch']\nloss = checkpoint['loss']\nResume training:\nmodel.train()"
  },
  {
    "objectID": "ml/choosing_frameworks.html",
    "href": "ml/choosing_frameworks.html",
    "title": "Which framework to choose?",
    "section": "",
    "text": "With the growing popularity of machine learning, many frameworks have appeared in various languages. One of the questions you might be facing is: which tool should I choose?\nThe main focus here is on the downsides of proprietary tools."
  },
  {
    "objectID": "ml/choosing_frameworks.html#points-worth-considering",
    "href": "ml/choosing_frameworks.html#points-worth-considering",
    "title": "Which framework to choose?",
    "section": "Points worth considering",
    "text": "Points worth considering\nThere are a several points you might want to consider in making that choice. For instance, what tools do people use in your field? (what tools are used in the papers you read?) What tools are your colleagues and collaborators using?\nLooking a bit further into the future, whether you are considering staying in academia or working in industry could also influence your choice. If the former, paying attention to the literature is key, if the latter, it may be good to have a look at the trends in job postings.\nSome frameworks offer collections of already-made toolkits. They are thus easy to start using and do not require a lot of programming experience. On the other hand, they may feel like black boxes and they are not very customizable. Scikit-learn and Keras—usually run on top of TensorFlow—fall in that category. Lower level tools allow you full control and tuning of your models, but can come with a steeper learning curve.\nPyTorch, developed by Facebook’s AI Research lab, has seen a huge increase in popularity in research in recent years due to its highly pythonic syntax, very convenient tensors, just-in-time (JIT) compilation, dynamic computation graphs, and because it is free and open-source.\nSeveral libraries are now adding a higher level on top of PyTorch: fastai, which we will use in this course, PyTorch Lightning, and PyTorch Ignite. fastai, in addition to the convenience of being able to write a model in a few lines of code, allows to dive as low as you choose into the PyTorch code, thus making it unconstrained by the optional ease of use. It also adds countless functionality. The downside of using this added layer is that it can make it less straightforward to install on a machine or to tweak and customize.\nThe most popular machine learning library currently remains TensorFlow, developed by the Google Brain Team. While it has a Python API, its syntax can be more obscure.\nJulia’s syntax is well suited for the implementation of mathematical models, GPU kernels can be written directly in Julia, and Julia’s speed is attractive in computation hungry fields. So Julia has also seen the development of many ML packages such as Flux or Knet. The user base of Julia remains quite small however.\nMy main motivation in writing this section however is to raise awareness about one question that should really be considered: whether the tool you decide to learn and use in your research is open-source or proprietary."
  },
  {
    "objectID": "ml/choosing_frameworks.html#proprietary-tools-a-word-of-caution",
    "href": "ml/choosing_frameworks.html#proprietary-tools-a-word-of-caution",
    "title": "Which framework to choose?",
    "section": "Proprietary tools: a word of caution",
    "text": "Proprietary tools: a word of caution\nAs a student, it is tempting to have the following perspective:\n\nMy university pays for this very expensive license. I have free access to this very expensive tool. It would be foolish not to make use of it while I can!\n\nWhen there are no equivalent or better open-source tools, that might be true. But when superior open-source tools exist, these university licenses are more of a trap than a gift.\nHere are some of the reasons you should be wary of proprietary tools:\n\nResearchers who do not have access to the tool cannot reproduce your methods\nLarge Canadian universities may offer a license for the tool, but grad students in other countries, independent researchers, researchers in small organizations, etc. may not have access to a free license (or tens of thousands of dollars to pay for it).\n\n\nOnce you graduate, you may not have access to the tool anymore\nOnce you leave your Canadian institution, you may become one of those researchers who do not have access to that tool. This means that you will not be able to re-run your thesis analyses, re-use your methods, and apply the skills you learnt. The time you spent learning that expensive tool you could play with for free may feel a lot less like a gift then.\n\n\nYour university may stop paying for a license\nAs commercial tools fall behind successful open-source ones, some universities may decide to stop purchasing a license. It happened during my years at SFU with an expensive and clunky citation manager which, after having been promoted for years by the library through countless workshops, got abandoned in favour of a much better free and open-source one.\n\n\nYou may get locked-in\nProprietary tools often come with proprietary formats and, depending on the tool, it may be painful (or impossible) to convert your work to another format. When that happens, you are locked-in.\n\n\nProprietary tools are often black boxes\nIt is often impossible to see the source code of proprietary software.\n\n\nLong-term access\nIt is often very difficult to have access to old versions of proprietary tools (and this can be necessary to reproduce old studies). When companies disappear, the tools they produced usually disappear with them. open-source tools, particularly those who have their code under version control in repositories such as GitHub, remain fully accessible (including all stages of development), and if they get abandoned, their maintenance can be taken over or restarted by others.\n\n\nThe licenses you have access to may be limiting and a cause of headache\nFor instance, the Alliance does not have an unlimited number of MATLAB licenses. Since these licenses come with complex rules (one license needed for each node, additional licenses for each toolbox, additional licenses for newer tools, etc.), it can quickly become a nightmare to navigate through it all. You may want to have a look at some of the comments in this thread.\n\n\nProprietary tools fall behind popular open-source tools\nEven large teams of software engineers cannot compete against an active community of researchers developing open-source tools. When open-source tools become really popular, the number of users contributing to their development vastly outnumbers what any company can provide. The testing, licensing, and production of proprietary tools are also too slow to keep up with quickly evolving fields of research. (Of course, open-source tools which do not take off and remain absolutely obscure do not see the benefit of a vast community.)\n\n\nProprietary tools often fail to address specialized edge cases needed in research\nIt is not commercially sound to develop cutting edge capabilities so specialized in a narrow subfield that they can only target a minuscule number of customers. But this is often what research needs. With open-source tools, researchers can develop the capabilities that fit their very specific needs. So while commercial tools are good and reliable for large audiences, they are often not the best in research. This explains the success of R over tools such as SASS or Stata in the past decade."
  },
  {
    "objectID": "ml/choosing_frameworks.html#conclusion",
    "href": "ml/choosing_frameworks.html#conclusion",
    "title": "Which framework to choose?",
    "section": "Conclusion",
    "text": "Conclusion\nAll that said, sometimes you don’t have a choice over the tool to use for your research as this may be dictated by the culture in your field or by your supervisor. But if you are free to choose and if superior or equal open-source alternatives exist and are popular, do not fall in the trap of thinking that because your university and the Alliance pay for a license, you should make use of it. It may be free for you—for now—but it can have hidden costs."
  },
  {
    "objectID": "ml/high_level_frameworks.html",
    "href": "ml/high_level_frameworks.html",
    "title": "High-level frameworks for PyTorch",
    "section": "",
    "text": "Several popular higher-level frameworks are built on top of PyTorch and make the code easier to write and run:\nThe following tag trends on Stack Overflow might give an idea of the popularity of these frameworks over time (catalyst doesn’t have any Stack Overflow tag):\nIf this data is to be believed, ignite never really took off (it also has a lower number of stars on GitHub), fast-ai was extremely popular when it came out, but its usage is going down, and PyTorch-lightning is currently the most popular."
  },
  {
    "objectID": "ml/high_level_frameworks.html#should-you-use-one-and-which-one",
    "href": "ml/high_level_frameworks.html#should-you-use-one-and-which-one",
    "title": "High-level frameworks for PyTorch",
    "section": "Should you use one (and which one)?",
    "text": "Should you use one (and which one)?\nLearning raw PyTorch is probably the best option for research. PyTorch is stable and here to stay. Higher-level frameworks may rise and drop in popularity and today’s popular one may see little usage tomorrow.\nRaw PyTorch is also the most flexible, the closest to the actual computations happening in your model, and probably the easiest to debug.\nDepending on your deep learning trajectory, you might find some of these tools useful though:\n\nIf you work in industry, you might want or need to get results quickly\nSome operations (e.g. parallel execution on multiple GPUs) can be tricky in raw PyTorch, while being extremely streamlined when using e.g. PyTorch-lightning\nEven in research, it might make sense to spend more time thinking about the structure of your model and the functioning of a network instead of getting bogged down in writing code\n\n\nBefore moving to any of these tools, it is probably a good idea to get a good knowledge of raw PyTorch: use these tools to simplify your workflow, not cloud your understanding of what your code is doing."
  },
  {
    "objectID": "ml/hpc.html",
    "href": "ml/hpc.html",
    "title": "Machine learning on production clusters",
    "section": "",
    "text": "This section is a summary of relevant information while using Python in an HPC context for deep learning."
  },
  {
    "objectID": "ml/hpc.html#run-code-in-a-job",
    "href": "ml/hpc.html#run-code-in-a-job",
    "title": "Machine learning on production clusters",
    "section": "Run code in a job",
    "text": "Run code in a job\nWhen you ssh into one of the Alliance clusters, you log into the login node.\nEverybody using a cluster uses that node to enter the cluster. Do not run anything computationally intensive on this node or you would make the entire cluster very slow for everyone. To run your code, you need to start an interactive job or submit a batch job to Slurm (the job scheduler used by the Alliance clusters)."
  },
  {
    "objectID": "ml/hpc.html#plots",
    "href": "ml/hpc.html#plots",
    "title": "Machine learning on production clusters",
    "section": "Plots",
    "text": "Plots\nDo not run code that displays plots on screen. Instead, have them written to files."
  },
  {
    "objectID": "ml/hpc.html#data",
    "href": "ml/hpc.html#data",
    "title": "Machine learning on production clusters",
    "section": "Data",
    "text": "Data\n\nCopy files to/from the cluster\n\nFew files\nIf you need to copy files to or from the cluster, you can use scp from your local machine.\n\nCopy file from your computer to the cluster\n[local]$ scp &lt;/local/path/to/file&gt; &lt;user&gt;@&lt;hostname&gt;:&lt;path/in/cluster&gt;\n\nExpressions between the &lt; and &gt; signs need to be replaced by the relevant information (without those signs).\n\n\n\nCopy file from the cluster to your computer\n[local]$ scp &lt;user&gt;@&lt;hostname&gt;:&lt;cluster/path/to/file&gt; &lt;/local/path&gt;\n\n\n\nLarge amount of data\nUse Globus for large data transfers.\n\nThe Alliance is starting to store classic ML datasets on its clusters. So if your research uses a common dataset, it may be worth inquiring whether it might be available before downloading a copy.\n\n\n\n\nLarge collections of files\nThe Alliance clusters are optimized for very large files and are slowed by large collections of small files. Datasets with many small files need to be turned into single-file archives with tar. Failing to do so will affect performance not just for you, but for all users of the cluster.\n$ tar cf &lt;data&gt;.tar &lt;path/to/dataset/directory&gt;/*\n\n\nIf you want to also compress the files, replace tar cf with tar czf\nAs a modern alternative to tar, you can use Dar"
  },
  {
    "objectID": "ml/hpc.html#interactive-jobs",
    "href": "ml/hpc.html#interactive-jobs",
    "title": "Machine learning on production clusters",
    "section": "Interactive jobs",
    "text": "Interactive jobs\nInteractive jobs are useful for code testing and development. They are not however the most efficient way to run code, so you should limit their use to testing and development.\nYou start an interactive job with:\n$ salloc --account=def-&lt;account&gt; --cpus-per-task=&lt;n&gt; --gres=gpu:&lt;n&gt; --mem=&lt;mem&gt; --time=&lt;time&gt;\nOur training cluster does not have GPUs, so for this workshop, do not use the --gres=gpu:&lt;n&gt; option.\nFor the workshop, you also don’t have to worry about the --account=def-&lt;account&gt; option (or, if you want, you can use --account=def-sponsor00).\nOur training cluster has a total of 60 CPUs on 5 compute nodes. Since there are many of you in this workshop, please be very mindful when running interactive jobs: if you request a lot of CPUs for a long time, the other workshop attendees won’t be able to use the cluster anymore until your interactive job requested time ends (even if you aren’t running any code).\nHere are my suggestions so that we don’t run into this problem:\n\nOnly start interactive jobs when you need to understand what Python is doing at every step, or to test, explore, and develop code (so where an interactive Python shell is really beneficial). Once you have a model, submit a batch job to Slurm instead\nWhen running interactive jobs on this training cluster, only request 1 CPU (so --cpus-per-task=1)\nOnly request the time that you will really use (e.g. for the lesson on Python tensors, maybe 30 min to 1 hour seems reasonable)\nIf you don’t need your job allocation anymore before it runs out, you can relinquish it with Ctrl+d\n\n\nBe aware that, on Cedar, you are not allowed to submit jobs from ~/home. Instead, you have to submit jobs from ~/scratch or ~/project."
  },
  {
    "objectID": "ml/hpc.html#batch-jobs",
    "href": "ml/hpc.html#batch-jobs",
    "title": "Machine learning on production clusters",
    "section": "Batch jobs",
    "text": "Batch jobs\nAs soon as you have a working Python script, you want to submit a batch job instead of running an interactive job. To do that, you need to write an sbatch script.\n\nJob script\n\nHere is an example script:\n\n#!/bin/bash\n#SBATCH --job-name=&lt;name&gt;*            # job name\n#SBATCH --account=def-&lt;account&gt;\n#SBATCH --time=&lt;time&gt;                 # max walltime in D-HH:MM or HH:MM:SS\n#SBATCH --cpus-per-task=&lt;number&gt;      # number of cores\n#SBATCH --gres=gpu:&lt;type&gt;:&lt;number&gt;    # type and number of GPU(s) per node\n#SBATCH --mem=&lt;mem&gt;                   # max memory (default unit is MB) per node\n#SBATCH --output=%x_%j.out*           # file name for the output\n#SBATCH --error=%x_%j.err*            # file name for errors\n#SBATCH --mail-user=&lt;email_address&gt;*\n#SBATCH --mail-type=ALL*\n\n# Load modules\n# (Do not use this in our workshop since we aren't using GPUs)\n# (Note: loading the Python module is not necessary\n# when you activate a Python virtual environment)\n# module load cudacore/.10.1.243 cuda/10 cudnn/7.6.5\n\n# Create a variable with the directory for your ML project\nSOURCEDIR=~/&lt;path/project/dir&gt;\n\n# Activate your Python virtual environment\nsource ~/env/bin/activate\n\n# Transfer and extract data to a compute node\nmkdir $SLURM_TMPDIR/data\ntar xf ~/projects/def-&lt;user&gt;/&lt;data&gt;.tar -C $SLURM_TMPDIR/data\n\n# Run your Python script on the data\npython $SOURCEDIR/&lt;script&gt;.py $SLURM_TMPDIR/data\n\n\n%x will get replaced by the script name and %j by the job number\nIf you compressed your data with tar czf, you need to extract it with tar xzf\nSBATCH options marked with a * are optional\nThere are various other options for email notifications\n\n\nYou may wonder why we transferred data to a compute node. This makes any I/O operation involving your data a lot faster, so it will speed up your code. Here is how this works:\nFirst, we create a temporary data directory in $SLURM_TMPDIR:\n$ mkdir $SLURM_TMPDIR/data\n\nThe variable $SLURM_TMPDIR is created by Slurm on the compute node where a job is running. Its path is /localscratch/&lt;user&gt;.&lt;jobid&gt;.0. Anything in it gets deleted when the job is done.\n\nThen we extract the data into it:\n$ tar xf ~/projects/def-&lt;user&gt;/&lt;data&gt;.tar -C $SLURM_TMPDIR/data\nIf your data is not in a tar file, you can simply copy it to the compute node running your job:\n$ cp -r ~/projects/def-&lt;user&gt;/&lt;data&gt; $SLURM_TMPDIR/data\n\n\nJob handling\n\nSubmit a job\n$ cd &lt;/dir/containing/job&gt;\n$ sbatch &lt;jobscript&gt;.sh\n\n\nCheck the status of your job(s)\n$ sq\n\nPD = pending\nR = running\nCG = completing (Slurm is doing the closing processes)\nNo information = your job has finished running\n\n\n\nCancel a job\n$ scancel &lt;jobid&gt;\n\n\nDisplay efficiency measures of a completed job\n$ seff &lt;jobid&gt;"
  },
  {
    "objectID": "ml/hpc.html#gpus",
    "href": "ml/hpc.html#gpus",
    "title": "Machine learning on production clusters",
    "section": "GPU(s)",
    "text": "GPU(s)\n\nGPU types\nSeveral Alliance clusters have GPUs. Their numbers and types differ:\n From the Alliance Wiki\nThe default is 12G P100, but you can request another type with SBATCH --gres=gpu:&lt;type&gt;:&lt;number&gt; (example: --gres=gpu:p100l:1 to request a 16G P100 on Cedar). Please refer to the Alliance Wiki for more details.\n\n\nNumber of GPU(s)\nTry running your model on a single GPU first.\nIt is very likely that you do not need more than one GPU. Asking for more than you need will greatly increase your waiting time until your job is run. The lesson on distributed computing with PyTorch gives a few information as to when you might benefit from using several GPUs and provides some links to more resources. We will also offer workshops on distributed ML in the future. In any event, you should test your model before asking for several GPUs.\n\n\nCPU/GPU ratio\nHere are the Alliance recommendations:\nBéluga:\nNo more than 10 CPU per GPU.\nCedar:\nP100 GPU: no more than 6 CPU per GPU.\nV100 GPU: no more than 8 CPU per GPU.\nGraham:\nNo more than 16 CPU per GPU."
  },
  {
    "objectID": "ml/hpc.html#code-testing",
    "href": "ml/hpc.html#code-testing",
    "title": "Machine learning on production clusters",
    "section": "Code testing",
    "text": "Code testing\nIt might be wise to test your code in an interactive job before submitting a really big batch job to Slurm.\n\nActivate your Python virtual environment\n$ source ~/env/bin/activate\n\n\nStart an interactive job\n\nExample:\n\n$ salloc --account=def-&lt;account&gt; --gres=gpu:1 --cpus-per-task=6 --mem=32000 --time=0:30:0\n\n\nPrepare the data\nCreate a temporary data directory in $SLURM_TMPDIR:\n(env) $ mkdir $SLURM_TMPDIR/data\n\nThe variable $SLURM_TMPDIR is created by Slurm on the compute node where a job is running. Its path is /localscratch/&lt;user&gt;.&lt;jobid&gt;.0. Anything in it gets deleted when the job is done.\n\nExtract the data into it:\n(env) $ tar xf ~/projects/def-&lt;user&gt;/&lt;data&gt;.tar -C $SLURM_TMPDIR/data\n\n\nTry to run your code\nPlay in Python to test your code:\n(env) $ python\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; ...\nTo exit the virtual environment, run:\n(env) $ deactivate"
  },
  {
    "objectID": "ml/hpc.html#checkpoints",
    "href": "ml/hpc.html#checkpoints",
    "title": "Machine learning on production clusters",
    "section": "Checkpoints",
    "text": "Checkpoints\nLong jobs should have a checkpoint at least every 24 hours. This ensures that an outage won’t lead to days of computation lost and it will help get the job started by the scheduler sooner.\nFor instance, you might want to have checkpoints every n epochs (choose n so that n epochs take less than 24 hours to run).\nIn PyTorch, you can create dictionaries with all the information necessary and save them as .tar files with torch.save(). You can then load them back with torch.load().\nThe information you want to save in each checkpoint includes the model’s state_dict, the optimizer’s state_dict, the epoch at which you stopped, the latest training loss, and anything else needed to restart training where you left off.\n\nFor example, saving a checkpoint during training could look something like this:\n\ntorch.save({\n    'epoch': &lt;last epoch run&gt;,\n    'model_state_dict': net.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    'loss': &lt;latest loss&gt;,\n}, &lt;path/to/checkpoint-file.tar&gt;)\n\nTo restart, initialize the model and optimizer, load the dictionary, and resume training:\n\n# Initialize the model and optimizer\nmodel = &lt;your model&gt;\noptimizer = &lt;your optimizer&gt;\n\n# Load the dictionary\ncheckpoint = torch.load(&lt;path/to/checkpoint-file.tar&gt;)\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\nepoch = checkpoint['epoch']\nloss = checkpoint['loss']\n\n# Resume training\nmodel.train()"
  },
  {
    "objectID": "ml/hpc.html#tensorboard-on-the-cluster",
    "href": "ml/hpc.html#tensorboard-on-the-cluster",
    "title": "Machine learning on production clusters",
    "section": "TensorBoard on the cluster",
    "text": "TensorBoard on the cluster\nTensorBoard allows to visually track your model metrics (e.g. loss, accuracy, model graph, etc.). It requires a lot of processing power however, so if you want to use it on an Alliance cluster, do not run it from the login node. Instead, run it as part of your job. This section guides you through the whole workflow.\n\nLaunch TensorBoard\nFirst, you need to launch TensorBoard in the background (with a trailing &) before running your Python script. To do so, ad to your sbatch script:\ntensorboard --logdir=/tmp/&lt;your log dir&gt; --host 0.0.0.0 &\n\nExample:\n\n#!/bin/bash\n#SBATCH ...\n...\n\ntensorboard --logdir=/tmp/&lt;your log dir&gt; --host 0.0.0.0 &\npython $SOURCEDIR/&lt;script&gt;.py $SLURM_TMPDIR/data\n\n\nCreate a connection between the compute node and your computer\nOnce the job is running, you need to create a connection between the compute node running TensorBoard and your computer.\nFirst, you need to find the hostname of the compute node running the Tensorboard server. This is the value under NODELIST for your job when you run:\n$ sq\nThen, from your computer, enter this ssh command:\n[local]$ ssh -N -f -L localhost:6006:&lt;node hostname&gt;:6006 &lt;user&gt;@&lt;cluster&gt;.computecanada.ca\n\nReplace &lt;node hostname&gt; by the compute node hostname you just identified, &lt;user&gt; by your user name, and &lt;cluster&gt; by the name of the Alliance cluster hostname—e.g. beluga, cedar, graham.\n\n\n\nAccess TensorBoard\nYou can now open a browser (on your computer) and go to http://localhost:6006 to monitor your model running on a compute node in the cluster!"
  },
  {
    "objectID": "ml/hpc.html#running-several-similar-jobs",
    "href": "ml/hpc.html#running-several-similar-jobs",
    "title": "Machine learning on production clusters",
    "section": "Running several similar jobs",
    "text": "Running several similar jobs\nA number of ML tasks (e.g. hyperparameter optimization) require running several instances of similar jobs. Grouping them into a single job with GLOST or GNU Parallel reduces the stress on the scheduler."
  },
  {
    "objectID": "ml/index.html",
    "href": "ml/index.html",
    "title": "Deep learning with PyTorch",
    "section": "",
    "text": "Date:\nThursday, June 22, 2023\nTime:\n9am–5pm (with a two-hour break from noon to 2pm)\nInstructor:\nMarie-Hélène Burle (Simon Fraser University)\nPrerequisites:\nBasic knowledge of Python.\nSoftware:\nWe will provide access to one of our Linux systems. To make use of it, attendees will need a remote secure shell (SSH) client installed on their computer. On Windows we recommend the free Home Edition of MobaXterm. On Mac and Linux computers, SSH is usually pre-installed (try typing ssh in a terminal to make sure it is there).\n\n Start course ➤"
  },
  {
    "objectID": "ml/intro.html",
    "href": "ml/intro.html",
    "title": "Introduction to machine learning",
    "section": "",
    "text": "This presentation gives a high-level overview of machine learning.\nI will define concepts, present the different types of learning, and explain the basic functioning of neural networks.\n\nSlides (Click and wait: the presentation might take a few instants to load)"
  },
  {
    "objectID": "ml/intro_slides.html#ml-allows-to-achieve-previously-impossible-tasks",
    "href": "ml/intro_slides.html#ml-allows-to-achieve-previously-impossible-tasks",
    "title": "Introduction to machine learning",
    "section": "ML allows to achieve previously impossible tasks",
    "text": "ML allows to achieve previously impossible tasks\n\nLet’s take the example of image recognition:\n\nIn typical computing, a programmer writes code that gives a computer detailed instructions of what to do.\nCoding all the possible ways—pixel by pixel—that an image can represent, say, a dog is an impossibly large task: there are many breeds of dogs, the image can be a picture, a blurred picture, a drawing, a cartoon, the dog can be in all sorts of positions, wearing clothes, etc.\nThere just aren’t enough resources to make the traditional programming approach able to create a computer program that can identify a dog in images.\nBy feeding a very large number of dog images to a neural network however, we can train that network to recognize dogs in images that it has never seen (without explicitly programming how it does this!)."
  },
  {
    "objectID": "ml/intro_slides.html#old-concept-new-computing-power",
    "href": "ml/intro_slides.html#old-concept-new-computing-power",
    "title": "Introduction to machine learning",
    "section": "Old concept … new computing power",
    "text": "Old concept … new computing power\nThe concept is everything but new: Arthur Samuel came up with it in 1949 and built a self-learning Checkers-playing program in 1959.\n\n\nMachine learning consists of feeding vast amounts of data to algorithms to strengthen pathways, so the excitement for the approach became somewhat dormant due to the lack of computing power and the lack of training data at the time.\nThe advent of powerful computers, GPUs, and massive amounts of data have brought the old concept to the forefront.\n\n\n\n\n\n\nFrom xkcd.com"
  },
  {
    "objectID": "ml/intro_slides.html#supervised-learning",
    "href": "ml/intro_slides.html#supervised-learning",
    "title": "Introduction to machine learning",
    "section": "Supervised learning",
    "text": "Supervised learning\n\nRegression is a form of supervised learning with continuous outputs\nClassification is supervised learning with discrete outputs\n\nSupervised learning uses training data in the form of example input/output pairs.\nGoal\nFind the relationship between inputs and outputs."
  },
  {
    "objectID": "ml/intro_slides.html#unsupervised-learning",
    "href": "ml/intro_slides.html#unsupervised-learning",
    "title": "Introduction to machine learning",
    "section": "Unsupervised learning",
    "text": "Unsupervised learning\nClustering, social network analysis, market segmentation, PCA … are all forms of unsupervised learning.\nUnsupervised learning uses unlabelled data.\nGoal\nFind structure within the data."
  },
  {
    "objectID": "ml/intro_slides.html#reinforcement-learning",
    "href": "ml/intro_slides.html#reinforcement-learning",
    "title": "Introduction to machine learning",
    "section": "Reinforcement learning",
    "text": "Reinforcement learning\nThe algorithm explores by performing random actions and these actions are rewarded or punished (bonus points or penalties).\nThis is how algorithms learn to play games."
  },
  {
    "objectID": "ml/intro_slides.html#decide-on-an-architecture",
    "href": "ml/intro_slides.html#decide-on-an-architecture",
    "title": "Introduction to machine learning",
    "section": "Decide on an architecture",
    "text": "Decide on an architecture\n\nThe architecture won’t change during training.\nThe type of architecture you choose (e.g. CNN, Transformer) depends on the type of data you have (e.g. vision, textual). The depth and breadth of your network depend on the amount of data and computing resource you have."
  },
  {
    "objectID": "ml/intro_slides.html#set-some-initial-parameters",
    "href": "ml/intro_slides.html#set-some-initial-parameters",
    "title": "Introduction to machine learning",
    "section": "Set some initial parameters",
    "text": "Set some initial parameters\n\nYou can initialize them randomly or get much better ones through transfer learning.\nWhile the parameters are also part of the model, those will change during training."
  },
  {
    "objectID": "ml/intro_slides.html#get-some-labelled-data",
    "href": "ml/intro_slides.html#get-some-labelled-data",
    "title": "Introduction to machine learning",
    "section": "Get some labelled data",
    "text": "Get some labelled data\n\nWhen we say that we need a lot of data for machine learning, we mean “lots of labelled data” as this is what gets used for training models."
  },
  {
    "objectID": "ml/intro_slides.html#make-sure-to-keep-some-data-for-testing",
    "href": "ml/intro_slides.html#make-sure-to-keep-some-data-for-testing",
    "title": "Introduction to machine learning",
    "section": "Make sure to keep some data for testing",
    "text": "Make sure to keep some data for testing\n\nThose data won’t be used for training the model. Often people keep around 20% of their data for testing."
  },
  {
    "objectID": "ml/intro_slides.html#pass-data-and-parameters-through-the-architecture",
    "href": "ml/intro_slides.html#pass-data-and-parameters-through-the-architecture",
    "title": "Introduction to machine learning",
    "section": "Pass data and parameters through the architecture",
    "text": "Pass data and parameters through the architecture\n\nThe train data are the inputs and the process of calculating the outputs is the forward pass."
  },
  {
    "objectID": "ml/intro_slides.html#the-outputs-of-the-model-are-predictions",
    "href": "ml/intro_slides.html#the-outputs-of-the-model-are-predictions",
    "title": "Introduction to machine learning",
    "section": "The outputs of the model are predictions",
    "text": "The outputs of the model are predictions"
  },
  {
    "objectID": "ml/intro_slides.html#compare-those-predictions-to-the-train-labels",
    "href": "ml/intro_slides.html#compare-those-predictions-to-the-train-labels",
    "title": "Introduction to machine learning",
    "section": "Compare those predictions to the train labels",
    "text": "Compare those predictions to the train labels\n\nSince our data was labelled, we know what the true outputs are."
  },
  {
    "objectID": "ml/intro_slides.html#calculate-train-loss",
    "href": "ml/intro_slides.html#calculate-train-loss",
    "title": "Introduction to machine learning",
    "section": "Calculate train loss",
    "text": "Calculate train loss\n\nThe deviation of our predictions from the true outputs gives us a measure of training loss."
  },
  {
    "objectID": "ml/intro_slides.html#adjust-parameters",
    "href": "ml/intro_slides.html#adjust-parameters",
    "title": "Introduction to machine learning",
    "section": "Adjust parameters",
    "text": "Adjust parameters\n\nThe parameters get automatically adjusted to reduce the training loss through the mechanism of backpropagation. This is the actual training part.\nThis process is repeated many times. Training models is pretty much a giant for loop."
  },
  {
    "objectID": "ml/intro_slides.html#from-model-to-program",
    "href": "ml/intro_slides.html#from-model-to-program",
    "title": "Introduction to machine learning",
    "section": "From model to program",
    "text": "From model to program\n\nRemember that the model architecture is fixed, but that the parameters change at each iteration of the training process."
  },
  {
    "objectID": "ml/intro_slides.html#evaluate-the-model",
    "href": "ml/intro_slides.html#evaluate-the-model",
    "title": "Introduction to machine learning",
    "section": "Evaluate the model",
    "text": "Evaluate the model\n\nWe can now use the testing set (which was never used to train the model) to evaluate our model: if we pass the test inputs through our program, we get some predictions that we can compare to the test labels (which are the true outputs).\nThis gives us the test loss: a measure of how well our model performs."
  },
  {
    "objectID": "ml/intro_slides.html#use-the-model",
    "href": "ml/intro_slides.html#use-the-model",
    "title": "Introduction to machine learning",
    "section": "Use the model",
    "text": "Use the model\n\nNow that we have a program, we can use it on unlabelled inputs to get what people ultimately want: unknown outputs.\nThis is when we put our model to actual use to solve some problem.\n\n\n\n Back to the course"
  },
  {
    "objectID": "ml/mnist.html",
    "href": "ml/mnist.html",
    "title": "Example: classifying the MNIST dataset",
    "section": "",
    "text": "Here is an example you can try on your own after the workshop: the classification of the MNIST dataset—a classic of machine learning."
  },
  {
    "objectID": "ml/mnist.html#the-mnist-dataset",
    "href": "ml/mnist.html#the-mnist-dataset",
    "title": "Example: classifying the MNIST dataset",
    "section": "The MNIST dataset",
    "text": "The MNIST dataset\nThe MNIST is a classic dataset commonly used for testing machine learning systems. It consists of pairs of images of handwritten digits and their corresponding labels.\nThe images are composed of 28x28 pixels of greyscale RGB codes ranging from 0 to 255 and the labels are the digits from 0 to 9 that each image represents.\nThere are 60,000 training pairs and 10,000 testing pairs.\nThe goal is to build a neural network which can learn from the training set to properly identify the handwritten digits and which will perform well when presented with the testing set that it has never seen. This is a typical case of supervised learning.\n\nNow, let’s explore the MNIST with PyTorch."
  },
  {
    "objectID": "ml/mnist.html#download-unzip-and-transform-the-data",
    "href": "ml/mnist.html#download-unzip-and-transform-the-data",
    "title": "Example: classifying the MNIST dataset",
    "section": "Download, unzip, and transform the data",
    "text": "Download, unzip, and transform the data\n\nWhere to store the data in the cluster\nWe will all use the same data. It would make little sense to all download it in our home directory.\nOn the Alliance clusters, a good place to store data shared amongst members of a project is in the /project file system.\nYou usually belong to /project/def-&lt;group&gt;, where &lt;group&gt; is the name of your PI. You can access it from your home directory through the symbolic link ~/projects/def-&lt;group&gt;.\nIn our training cluster, we are all part of the group def-sponsor00, accessible through /project/def-sponsor00 (or the hyperlink ~/projects/def-sponsor00).\nWe will thus all access the MNIST data in ~/projects/def-sponsor00/data.\n\n\nHow to obtain the data?\nThe dataset can be downloaded directly from the MNIST website, but the PyTorch package TorchVision has tools to download and transform several classic vision datasets, including the MNIST.\nhelp(torchvision.datasets.MNIST)\nHelp on class MNIST in module torchvision.datasets.mnist:\n\nclass MNIST(torchvision.datasets.vision.VisionDataset)\n\n |  MNIST(root: str, train: bool = True, \n |    transform: Optional[Callable] = None,\n |    target_transform: Optional[Callable] = None, \n |    download: bool = False) -&gt; None\n |   \n |  Args:\n |    root (string): Root directory of dataset where \n |      MNIST/raw/train-images-idx3-ubyte and \n |      MNIST/raw/t10k-images-idx3-ubyte exists.\n |    train (bool, optional): If True, creates dataset from \n |      train-images-idx3-ubyte, otherwise from t10k-images-idx3-ubyte.\n |    download (bool, optional): If True, downloads the dataset from the \n |      internet and puts it in root directory. If dataset is already \n |      downloaded, it is not downloaded again.\n |    transform (callable, optional): A function/transform that takes in \n |      an PIL image and returns a transformed version.\n |      E.g, transforms.RandomCrop\n |    target_transform (callable, optional): A function/transform that \n |      takes in the target and transforms it.\nNote that here too, the root argument sets the location of the downloaded data and we will use /project/def-sponsor00/data/.\n\n\nPrepare the data\nFirst, let’s load the needed libraries:\n\nimport torch\nfrom torchvision import datasets, transforms\nfrom matplotlib import pyplot as plt\n\nThe MNIST dataset already consists of a training and a testing sets, so we don’t have to split the data manually. Instead, we can directly create 2 different objects with the same function (train=True selects the train set and train=False selects the test set).\nWe will transform the raw data to tensors and normalize them using the mean and standard deviation of the MNIST training set: 0.1307 and 0.3081 respectively (even though the mean and standard deviation of the test data are slightly different, it is important to normalize the test data with the values of the training data to apply the same treatment to both sets).\nSo we first need to define a transformation:\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n])\n\n\n\nWe can now create our data objects\n\nTraining data\n\nRemember that train=True selects the training set of the MNIST.\n\n\ntrain_data = datasets.MNIST(\n    '~/projects/def-sponsor00/data',\n    train=True, download=True, transform=transform)\n\n\n\nTest data\n\ntrain=False selects the test set.\n\n\ntest_data = datasets.MNIST(\n    '~/projects/def-sponsor00/data',\n    train=False, transform=transform)"
  },
  {
    "objectID": "ml/mnist.html#exploring-the-data",
    "href": "ml/mnist.html#exploring-the-data",
    "title": "Example: classifying the MNIST dataset",
    "section": "Exploring the data",
    "text": "Exploring the data\n\nData inspection\nFirst, let’s check the size of train_data:\n\nprint(len(train_data))\n\n60000\n\n\nThat makes sense since the MNIST’s training set has 60,000 pairs. train_data has 60,000 elements and we should expect each element to be of size 2 since it is a pair. Let’s double-check with the first element:\n\nprint(len(train_data[0]))\n\n2\n\n\nSo far, so good. We can print that first pair:\n\nprint(train_data[0])\n\n(tensor([[[-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.3860, -0.1951,\n          -0.1951, -0.1951,  1.1795,  1.3068,  1.8032, -0.0933,  1.6887,\n           2.8215,  2.7197,  1.1923, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.0424,  0.0340,  0.7722,  1.5359,  1.7396,  2.7960,\n           2.7960,  2.7960,  2.7960,  2.7960,  2.4396,  1.7650,  2.7960,\n           2.6560,  2.0578,  0.3904, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n           0.1995,  2.6051,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,\n           2.7960,  2.7960,  2.7960,  2.7706,  0.7595,  0.6195,  0.6195,\n           0.2886,  0.0722, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.1951,  2.3633,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,\n           2.0960,  1.8923,  2.7197,  2.6433, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242,  0.5940,  1.5614,  0.9377,  2.7960,  2.7960,  2.1851,\n          -0.2842, -0.4242,  0.1231,  1.5359, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.2460, -0.4115,  1.5359,  2.7960,  0.7213,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242,  1.3450,  2.7960,  1.9942,\n          -0.3988, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.2842,  1.9942,  2.7960,\n           0.4668, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.0213,  2.6433,\n           2.4396,  1.6123,  0.9504, -0.4115, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.6068,\n           2.6306,  2.7960,  2.7960,  1.0904, -0.1060, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n           0.1486,  1.9432,  2.7960,  2.7960,  1.4850, -0.0806, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.2206,  0.7595,  2.7833,  2.7960,  1.9560, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242,  2.7451,  2.7960,  2.7451,  0.3904,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n           0.1613,  1.2305,  1.9051,  2.7960,  2.7960,  2.2105, -0.3988,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.0722,  1.4596,\n           2.4906,  2.7960,  2.7960,  2.7960,  2.7578,  1.8923, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.1187,  1.0268,  2.3887,  2.7960,\n           2.7960,  2.7960,  2.7960,  2.1342,  0.5686, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.1315,  0.4159,  2.2869,  2.7960,  2.7960,  2.7960,\n           2.7960,  2.0960,  0.6068, -0.3988, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.1951,\n           1.7523,  2.3633,  2.7960,  2.7960,  2.7960,  2.7960,  2.0578,\n           0.5940, -0.3097, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242,  0.2758,  1.7650,  2.4524,\n           2.7960,  2.7960,  2.7960,  2.7960,  2.6815,  1.2686, -0.2842,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242,  1.3068,  2.7960,  2.7960,\n           2.7960,  2.2742,  1.2941,  1.2559, -0.2206, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242]]]), 5)\n\n\nAnd you can see that it is a tuple with:\n\nprint(type(train_data[0]))\n\n&lt;class 'tuple'&gt;\n\n\nWhat is that tuple made of?\n\nprint(type(train_data[0][0]))\nprint(type(train_data[0][1]))\n\n&lt;class 'torch.Tensor'&gt;\n&lt;class 'int'&gt;\n\n\nIt is made of the tensor for the first image (remember that we transformed the images into tensors when we created the objects train_data and test_data) and the integer of the first label (which you can see is 5 when you print train_data[0][1]).\nSo since train_data[0][0] is the tensor representing the image of the first pair, let’s check its size:\n\nprint(train_data[0][0].size())\n\ntorch.Size([1, 28, 28])\n\n\nThat makes sense: a color image would have 3 layers of RGB values (so the size in the first dimension would be 3), but because the MNIST has black and white images, there is a single layer of values—the values of each pixel on a gray scale—so the first dimension has a size of 1. The 2nd and 3rd dimensions correspond to the width and length of the image in pixels, hence 28 and 28.\n\n\nYour turn:\n\nRun the following:\nprint(train_data[0][0][0])\nprint(train_data[0][0][0][0])\nprint(train_data[0][0][0][0][0])\nAnd think about what each of them represents.\nThen explore the test_data object.\n\n\n\nPlotting an image from the data\nFor this, we will use pyplot from matplotlib.\nFirst, we select the image of the first pair and we resize it from 3 to 2 dimensions by removing its dimension of size 1 with torch.squeeze:\nimg = torch.squeeze(train_data[0][0])\nThen, we plot it with pyplot, but since we are in a cluster, instead of showing it to screen with plt.show(), we save it to file:\nplt.imshow(img, cmap='gray')\nThis is what that first image looks like:\n\nAnd indeed, it matches the first label we explored earlier (train_data[0][1]).\n\n\nPlotting an image with its pixel values\nWe can plot it with more details by showing the value of each pixel in the image. One little twist is that we need to pick a threshold value below which we print the pixel values in white otherwise they would not be visible (black on near black background). We also round the pixel values to one decimal digit so as not to clutter the result.\nimgplot = plt.figure(figsize = (12, 12))\nsub = imgplot.add_subplot(111)\nsub.imshow(img, cmap='gray')\nwidth, height = img.shape\nthresh = img.max() / 2.5\nfor x in range(width):\n    for y in range(height):\n        val = round(img[x][y].item(), 1)\n        sub.annotate(str(val), xy=(y, x),\n                     horizontalalignment='center',\n                     verticalalignment='center',\n                     color='white' if img[x][y].item() &lt; thresh else 'black')"
  },
  {
    "objectID": "ml/mnist.html#batch-processing",
    "href": "ml/mnist.html#batch-processing",
    "title": "Example: classifying the MNIST dataset",
    "section": "Batch processing",
    "text": "Batch processing\nPyTorch provides the torch.utils.data.DataLoader class which combines a dataset and an optional sampler and provides an iterable (while training or testing our neural network, we will iterate over that object). It allows, among many other things, to set the batch size and shuffle the data.\nSo our last step in preparing the data is to pass it through DataLoader.\n\nCreate DataLoaders\n\nTraining data\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=20, shuffle=True)\n\n\nTest data\ntest_loader = torch.utils.data.DataLoader(\n    test_data, batch_size=20, shuffle=False)\n\n\n\nPlot a full batch of images with their labels\nNow that we have passed our data through DataLoader, it is easy to select one batch from it. Let’s plot an entire batch of images with their labels.\nFirst, we need to get one batch of training images and their labels:\ndataiter = iter(train_loader)\nbatchimg, batchlabel = dataiter.next()\nThen, we can plot them:\nbatchplot = plt.figure(figsize=(20, 5))\nfor i in torch.arange(20):\n    sub = batchplot.add_subplot(2, 10, i+1, xticks=[], yticks=[])\n    sub.imshow(torch.squeeze(batchimg[i]), cmap='gray')\n    sub.set_title(str(batchlabel[i].item()), fontsize=25)"
  },
  {
    "objectID": "ml/mnist.html#time-to-build-a-nn-to-classify-the-mnist",
    "href": "ml/mnist.html#time-to-build-a-nn-to-classify-the-mnist",
    "title": "Example: classifying the MNIST dataset",
    "section": "Time to build a NN to classify the MNIST",
    "text": "Time to build a NN to classify the MNIST\nLet’s build a multi-layer perceptron (MLP): the simplest neural network. It is a feed-forward (i.e. no loop), fully-connected (i.e. each neuron of one layer is connected to all the neurons of the adjacent layers) neural network with a single hidden layer.\n\n\nLoad packages\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchvision import datasets, transforms\nfrom torch.optim.lr_scheduler import StepLR\nThe torch.nn.functional module contains all the functions of the torch.nn package.\nThese functions include loss functions, activation functions, pooling functions…\n\n\nCreate a SummaryWriter instance for TensorBoard\nwriter = SummaryWriter()\n\n\nDefine the architecture of the network\n# To build a model, create a subclass of torch.nn.Module:\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(784, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    # Method for the forward pass:\n    def forward(self, x):\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)\n        return output\nPython’s class inheritance gives our subclass all the functionality of torch.nn.Module while allowing us to customize it.\n\n\nDefine a training function\ndef train(model, device, train_loader, optimizer, epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()  # reset the gradients to 0\n        output = model(data)\n        loss = F.nll_loss(output, target)  # negative log likelihood\n        writer.add_scalar(\"Loss/train\", loss, epoch)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 10 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))\n\n\nDefine a testing function\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            # Sum up batch loss:\n            test_loss += F.nll_loss(output, target, reduction='sum').item()\n            # Get the index of the max log-probability:\n            pred = output.argmax(dim=1, keepdim=True)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n\n    # Print a summary\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n\n\nDefine a function main() which runs our network\ndef main():\n    epochs = 1\n    torch.manual_seed(1)\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n    ])\n\n    train_data = datasets.MNIST(\n        '~/projects/def-sponsor00/data',\n        train=True, download=True, transform=transform)\n\n    test_data = datasets.MNIST(\n        '~/projects/def-sponsor00/data',\n        train=False, transform=transform)\n\n    train_loader = torch.utils.data.DataLoader(train_data, batch_size=64)\n    test_loader = torch.utils.data.DataLoader(test_data, batch_size=1000)\n    model = Net().to(device)  # create instance of our network and send it to device\n    optimizer = optim.Adadelta(model.parameters(), lr=1.0)\n    scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n\n    for epoch in range(1, epochs + 1):\n        train(model, device, train_loader, optimizer, epoch)\n        test(model, device, test_loader)\n        scheduler.step()\n\n\nRun the network\nmain()\n\n\nWrite pending events to disk and close the TensorBoard\nwriter.flush()\nwriter.close()\nThe code is working. Time to actually train our model!\nJupyter is a fantastic tool. It has a major downside however: when you launch a Jupyter server, you are running a job on a compute node. If you want to play for 8 hours in Jupyter, you are requesting an 8 hour job. Now, most of the time you spend on Jupyter is spent typing, running bits and pieces of code, or doing nothing at all. If you ask for GPUs, many CPUs, and lots of RAM, all of it will remain idle almost all of the time. It is a really suboptimal use of the Alliance resources.\nIn addition, if you ask for lots of resources for a long time, you will have to wait a long time in the queue before they get allocated to you.\nLastly, you will go through your allocation quickly.\nA much better strategy is to develop and test your code (with very little data, few epochs, etc.) in an interactive job (with salloc) or in Jupyter, then, launch an sbatch job to actually train your model. This ensures that heavy duty resources such as GPU(s) are only allocated to you when you are actually needing and using them."
  },
  {
    "objectID": "ml/mnist.html#lets-train-and-test-our-model",
    "href": "ml/mnist.html#lets-train-and-test-our-model",
    "title": "Example: classifying the MNIST dataset",
    "section": "Let’s train and test our model",
    "text": "Let’s train and test our model\n\nLog in a cluster\nOpen a terminal and SSH to a cluster.\n\n\nLoad necessary modules\nFirst, we need to load the Python and CUDA modules. This is done with the Lmod tool through the module command. Here are some key Lmod commands:\n# Get help on the module command\n$ module help\n\n# List modules that are already loaded\n$ module list\n\n# See which modules are available for a tool\n$ module avail &lt;tool&gt;\n\n# Load a module\n$ module load &lt;module&gt;[/&lt;version&gt;]\nHere are the modules we need:\n$ module load nixpkgs/16.09 gcc/7.3.0 cuda/10.0.130 cudnn/7.6 python/3.8.2\n\n\nInstall Python packages\nYou also need the Python packages matplotlib, torch, torchvision, and tensorboard.\nOn the Alliance clusters, you need to create a virtual environment in which you install packages with pip, then activate that virtual environment.\n\nDo not use Anaconda.\nWhile Anaconda is a great tool on personal computers, it is not an appropriate tool when working on the Alliance clusters: binaries are unoptimized for those clusters and library paths are inconsistent with their architecture. Anaconda installs packages in $HOME where it creates a very large number of small files. It can also create conflicts by modifying .bashrc.\n\nCreate a virtual environment:\n$ virtualenv --no-download ~/env\nActivate the virtual environment:\n$ source ~/env/bin/activate\nUpdate pip:\n(env) $ pip install --no-index --upgrade pip\nInstall the packages you need in the virtual environment:\n(env) $ pip install --no-cache-dir --no-index matplotlib torch torchvision tensorboard\nIf you want to exit the virtual environment, you can press Ctrl-D or run:\n(env) $ deactivate\n\n\nWrite a Python script\nCreate a directory for this project and cd into it:\nmkdir mnist\ncd mnist\nStart a Python script with the text editor of your choice:\nnano nn.py\nIn it, copy-paste the code we played with in Jupyter, but this time have it run for 10 epochs:\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchvision import datasets, transforms\nfrom torch.optim.lr_scheduler import StepLR\n\nwriter = SummaryWriter()\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(784, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)\n        return output\n\ndef train(model, device, train_loader, optimizer, epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        writer.add_scalar(\"Loss/train\", loss, epoch)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 10 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))\n\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction='sum').item()\n            pred = output.argmax(dim=1, keepdim=True)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n\ndef main():\n    epochs = 10  # don't forget to change the number of epochs\n    torch.manual_seed(1)\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n    ])\n\n    train_data = datasets.MNIST(\n        '~/projects/def-sponsor00/data',\n        train=True, download=True, transform=transform)\n\n    test_data = datasets.MNIST(\n        '~/projects/def-sponsor00/data',\n        train=False, transform=transform)\n\n    train_loader = torch.utils.data.DataLoader(train_data, batch_size=64)\n    test_loader = torch.utils.data.DataLoader(test_data, batch_size=1000)\n    model = Net().to(device)\n    optimizer = optim.Adadelta(model.parameters(), lr=1.0)\n    scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n\n    for epoch in range(1, epochs + 1):\n        train(model, device, train_loader, optimizer, epoch)\n        test(model, device, test_loader)\n        scheduler.step()\n\nmain()\n\nwriter.flush()\nwriter.close()\n\n\nWrite a Slurm script\nWrite a shell script with the text editor of your choice:\nnano nn.sh\nThis is what you want in that script:\n#!/bin/bash\n#SBATCH --time=5:0\n#SBATCH --cpus-per-task=1\n#SBATCH --gres=gpu:1\n#SBATCH --mem=4G\n#SBATCH --output=%x_%j.out\n#SBATCH --error=%x_%j.err\n\npython ~/mnist/nn.py\n\n--time accepts these formats: “min”, “min:s”, “h:min:s”, “d-h”, “d-h:min” & “d-h:min:s”\n%x will get replaced by the script name & %j by the job number\n\n\n\nSubmit a job\nFinally, you need to submit your job to Slurm:\n$ sbatch ~/mnist/nn.sh\nYou can check the status of your job with:\n$ sq\n\nPD = pending\nR = running\nCG = completing (Slurm is doing the closing processes)\nNo information = your job has finished running\n\nYou can cancel it with:\n$ scancel &lt;jobid&gt;\nOnce your job has finished running, you can display efficiency measures with:\n$ seff &lt;jobid&gt;"
  },
  {
    "objectID": "ml/mnist.html#lets-explore-our-models-metrics-with-tensorboard",
    "href": "ml/mnist.html#lets-explore-our-models-metrics-with-tensorboard",
    "title": "Example: classifying the MNIST dataset",
    "section": "Let’s explore our model’s metrics with TensorBoard",
    "text": "Let’s explore our model’s metrics with TensorBoard\nTensorBoard is a web visualization toolkit developed by TensorFlow which can be used with PyTorch.\nBecause we have sent our model’s metrics logs to TensorBoard as part of our code, a directory called runs with those logs was created in our ~/mnist directory.\n\nLaunch TensorBoard\nTensorBoard requires too much processing power to be run on the login node. When you run long jobs, the best strategy is to launch it in the background as part of the job. This allows you to monitor your model as it is running (and cancel it if things don’t look right).\n\nExample:\n\n#!/bin/bash\n#SBATCH ...\n#SBATCH ...\n\ntensorboard --logdir=runs --host 0.0.0.0 &\npython ~/mnist/nn.py\nBecause we only have 1 GPU and are taking turns running our jobs, we need to keep our jobs very short here. So we will launch a separate job for TensorBoard. This time, we will launch an interactive job:\nsalloc --time=1:0:0 --mem=2000M\nTo launch TensorBoard, we need to activate our Python virtual environment (TensorBoard was installed by pip):\nsource ~/projects/def-sponsor00/env/bin/activate\nThen we can launch TensorBoard in the background:\ntensorboard --logdir=~/mnist/runs --host 0.0.0.0 &\nNow, we need to create a connection with SSH tunnelling between your computer and the compute note running your TensorBoard job.\n\n\nConnect to TensorBoard from your computer\nFrom a new terminal on your computer, run:\nssh -NfL localhost:6006:&lt;hostname&gt;:6006 userxxx@uu.c3.ca\n\nReplace &lt;hostname&gt; by the name of the compute node running your salloc job. You can find it by looking at your prompt (your prompt shows &lt;username&gt;@&lt;hostname&gt;).\nReplace &lt;userxxx&gt; by your user name.\n\nNow, you can open a browser on your computer and access TensorBoard at http://localhost:6006."
  },
  {
    "objectID": "ml/nn.html",
    "href": "ml/nn.html",
    "title": "Introduction to neural networks",
    "section": "",
    "text": "3Blue1Brown by Grant Sanderson has a series of 4 videos on neural networks which is easy to watch, fun, and does an excellent job at explaining the functioning of a simple neural network. In this section, we will watch the first 2 videos as an introduction to what neural networks are and how they learn."
  },
  {
    "objectID": "ml/nn.html#what-are-nn-19-min",
    "href": "ml/nn.html#what-are-nn-19-min",
    "title": "Introduction to neural networks",
    "section": "What are NN? (19 min)",
    "text": "What are NN? (19 min)\n\n\nAs you develop your own ML models, if you find that your mathematical background is shaky, 3blue1brown also has an excellent series of videos on linear algebra and an equally great one on calculus."
  },
  {
    "objectID": "ml/nn.html#how-do-nn-learn-21-min",
    "href": "ml/nn.html#how-do-nn-learn-21-min",
    "title": "Introduction to neural networks",
    "section": "How do NN learn? (21 min)",
    "text": "How do NN learn? (21 min)"
  },
  {
    "objectID": "ml/nn.html#nn-vs-biological-neurons-and-types-of-nn",
    "href": "ml/nn.html#nn-vs-biological-neurons-and-types-of-nn",
    "title": "Introduction to neural networks",
    "section": "NN vs biological neurons and types of NN",
    "text": "NN vs biological neurons and types of NN\nSlides (Click and wait: the presentation might take a few instants to load)"
  },
  {
    "objectID": "ml/nn_slides.html#fully-connected-neural-networks",
    "href": "ml/nn_slides.html#fully-connected-neural-networks",
    "title": "NN vs biological neurons Types of NN",
    "section": "Fully connected neural networks",
    "text": "Fully connected neural networks\n\n\n\n\n\nFrom Glosser.ca, Wikipedia\n\n\n\nEach neuron receives inputs from every neuron of the previous layer and passes its output to every neuron of the next layer."
  },
  {
    "objectID": "ml/nn_slides.html#convolutional-neural-networks",
    "href": "ml/nn_slides.html#convolutional-neural-networks",
    "title": "NN vs biological neurons Types of NN",
    "section": "Convolutional neural networks",
    "text": "Convolutional neural networks\n\n\nFrom Programming Journeys by Rensu Theart\n\nConvolutional neural networks (CNN) are used for spatially structured data (e.g. images).\nImages have huge input sizes and would require a very large number of neurons in a fully connected neural net. In convolutional layers, neurons receive input from a subarea (called local receptive field) of the previous layer. This greatly reduces the number of parameters. Optionally, pooling (combining the outputs of neurons in a subarea) reduces the data dimensions."
  },
  {
    "objectID": "ml/nn_slides.html#recurrent-neural-networks",
    "href": "ml/nn_slides.html#recurrent-neural-networks",
    "title": "NN vs biological neurons Types of NN",
    "section": "Recurrent neural networks",
    "text": "Recurrent neural networks\n\n\nFrom fdeloche, Wikipedia\n\nRecurrent neural networks (RNN) such as Long Short-Term Memory (LSTM) are used for chain structured data (e.g. text).\nThey are not feedforward networks (i.e. networks for which the information moves only in the forward direction without any loop)."
  },
  {
    "objectID": "ml/nn_slides.html#transformers",
    "href": "ml/nn_slides.html#transformers",
    "title": "NN vs biological neurons Types of NN",
    "section": "Transformers",
    "text": "Transformers\nA combination of two RNNs (the encoder and the decoder) is used in sequence to sequence models for translation or picture captioning.\nIn 2014 the concept of attention (giving added weight to important words) was developed, greatly improving the ability of such models to process a lot of data.\nThe problem with recurrence is that it is not easily to parallelize (and thus to run fast on GPUs).\nIn 2017, a new model—the transformer—was proposed: by using only attention mechanisms and no recurrence, the transformer achieves better results in an easily parallelizable fashion.\nWith the addition of transfer learning, powerful transformers emerged in the field of NLP (e.g. Bidirectional Encoder Representations from Transformers (BERT) from Google and Generative Pre-trained Transformer-3 (GPT-3) from OpenAI)."
  },
  {
    "objectID": "ml/pytorch.html",
    "href": "ml/pytorch.html",
    "title": "The PyTorch API",
    "section": "",
    "text": "PyTorch is a free and open-source machine learning and scientific computing framework based on Torch. While Torch uses a scripting language based on Lua, PyTorch has a Python and a C++ interface.\nCreated by Meta AI (formerly Facebook, Inc.) in 2017, it is now a project of The Linux Foundation.\nPyTorch is widely used in academia and research. Part of its popularity stems from the fact that the Python interface is truly pythonic in nature, making it easier to learn than other popular frameworks such as TensorFlow.\nThe PyTorch API is vast and complex. This section links to the key components to get you started."
  },
  {
    "objectID": "ml/pytorch.html#domain-specific-libraries",
    "href": "ml/pytorch.html#domain-specific-libraries",
    "title": "The PyTorch API",
    "section": "Domain-specific libraries",
    "text": "Domain-specific libraries\nPyTorch is a large framework with domain-specific libraries:\n\nTorchVision for computer vision,\nTorchText for natural languages,\nTorchAudio for audio and signal processing.\n\nThese libraries contain standard datasets and utilities specific to the data in those fields."
  },
  {
    "objectID": "ml/pytorch.html#loading-data",
    "href": "ml/pytorch.html#loading-data",
    "title": "The PyTorch API",
    "section": "Loading data",
    "text": "Loading data\ntorch.utils.data contains everything you need create data loaders (iterables that present the data to a model)."
  },
  {
    "objectID": "ml/pytorch.html#building-models",
    "href": "ml/pytorch.html#building-models",
    "title": "The PyTorch API",
    "section": "Building models",
    "text": "Building models\ntorch.nn contains the elements you need to build your model architecture and chose a loss function."
  },
  {
    "objectID": "ml/pytorch.html#training",
    "href": "ml/pytorch.html#training",
    "title": "The PyTorch API",
    "section": "Training",
    "text": "Training\nTraining a model consists of optimizing the model parameters.\ntorch.autograd contains the tools for automatic differentiation (to compute the gradients, that is the tensors containing the partial derivatives of the error with respect to the parameters of the functions in the model) and torch.optim contains optimization algorithms that can be used for gradient descent."
  },
  {
    "objectID": "ml/resources.html",
    "href": "ml/resources.html",
    "title": "Resources",
    "section": "",
    "text": "This section contains a list of general machine learning resources, resources specific to PyTorch, as well as resources for Python and fastai."
  },
  {
    "objectID": "ml/resources.html#machine-learning",
    "href": "ml/resources.html#machine-learning",
    "title": "Resources",
    "section": "Machine learning",
    "text": "Machine learning\nAlliance wiki ML page\n\nOpen-access preprints\nArxiv Sanity Preserver by Andrej Karpathy\nML papers in the computer science category on arXiv\nML papers in the stats category on arXiv\nDistill ML research online journal\n\n\nAdvice and sources\nAdvice and sources from ML research student\n\n\nGetting help\nStack Overflow [machine-learning] tag\nStack Overflow [deep-learning] tag\nStack Overflow [supervised-learning] tag\nStack Overflow [unsupervised-learning] tag\nStack Overflow [semisupervised-learning] tag\nStack Overflow [reinforcement-learning] tag\nStack Overflow [transfer-learning] tag\nStack Overflow [machine-learning-model] tag\nStack Overflow [learning-rate] tag\nStack Overflow [bayesian-deep-learning] tag\n\n\nFree introductory courses\ndeeplearning.ai\nfast.ai\nGoogle\n\n\nLists of open datasets\nbenchmarks.ai\nAIBench\nkaggle\nWikipedia"
  },
  {
    "objectID": "ml/resources.html#pytorch",
    "href": "ml/resources.html#pytorch",
    "title": "Resources",
    "section": "PyTorch",
    "text": "PyTorch\nAlliance wiki PyTorch page\n\n\nDocumentation\nPyTorch website\nPyTorch documentation\nPyTorch tutorials\nPyTorch online courses\nPyTorch examples\n\n\nGetting help\nPyTorch Discourse forum\nStack Overflow [pytorch] tag\nStack Overflow [pytorch-dataloader] tag\nStack Overflow [pytorch-ignite] tag\n\n\nPre-trained models\nPyTorch Hub"
  },
  {
    "objectID": "ml/resources.html#python",
    "href": "ml/resources.html#python",
    "title": "Resources",
    "section": "Python",
    "text": "Python\nAlliance wiki Python page\n\nIDE\nProject Jupyter\nList of IDEs with description\nComparison of IDEs\nEmacs Python IDE\n\n\nShell\nIPython\nbpython\nptpython\n\n\nGetting help\nStack Overflow [python] tag"
  },
  {
    "objectID": "ml/resources.html#fastai",
    "href": "ml/resources.html#fastai",
    "title": "Resources",
    "section": "fastai",
    "text": "fastai\n\nDocumentation\nManual\nTutorials\nPeer-reviewed paper\n\n\nBook\nPaperback version\nFree MOOC version of part 1 of the book\nJupyter notebooks version of the book\n\n\nGetting help\nDiscourse forum"
  },
  {
    "objectID": "ml/tensors.html",
    "href": "ml/tensors.html",
    "title": "PyTorch tensors",
    "section": "",
    "text": "Before information can be processed by algorithms, it needs to be converted to floating point numbers. Indeed, you don’t pass a sentence or an image through a model; instead you input numbers representing a sequence of words or pixel values.\nAll these floating point numbers need to be stored in a data structure. The most suited structure is multidimensional (to hold several layers of information) and homogeneous—all data of the same type—for efficiency.\nPython already has several multidimensional array structures (e.g. NumPy’s ndarray) but the particularities of deep learning call for special characteristics such as the ability to run operations on GPUs and/or in a distributed fashion, the ability to keep track of computation graphs for automatic differentiation, and different defaults (lower precision for improved training performance).\nThe PyTorch tensor is a Python data structure with these characteristics that can easily be converted to/from NumPy’s ndarray and integrates well with other Python libraries such as Pandas.\nIn this section, we will explore the basics of PyTorch tensors."
  },
  {
    "objectID": "ml/tensors.html#importing-pytorch",
    "href": "ml/tensors.html#importing-pytorch",
    "title": "PyTorch tensors",
    "section": "Importing PyTorch",
    "text": "Importing PyTorch\nFirst of all, we need to import the torch library:\n\nimport torch\n\nWe can check its version with:\n\ntorch.__version__\n\n'2.0.1'"
  },
  {
    "objectID": "ml/tensors.html#creating-tensors",
    "href": "ml/tensors.html#creating-tensors",
    "title": "PyTorch tensors",
    "section": "Creating tensors",
    "text": "Creating tensors\nThere are many ways to create tensors:\n\ntorch.tensor:   Input individual values\ntorch.arange:   1D tensor with a sequence of integers\ntorch.linspace:  1D linear scale tensor\ntorch.logspace:  1D log scale tensor\ntorch.rand:     Random numbers from a uniform distribution on [0, 1)\ntorch.randn:     Numbers from the standard normal distribution\ntorch.randperm:   Random permutation of integers\ntorch.empty:     Uninitialized tensor\ntorch.zeros:     Tensor filled with 0\ntorch.ones:     Tensor filled with 1\ntorch.eye:       Identity matrix\n\n\nFrom input values\n\nt = torch.tensor(3)\n\n\n\nYour turn:\n\nWithout using the shape descriptor, try to get the shape of the following tensors:\ntorch.tensor([0.9704, 0.1339, 0.4841])\n\ntorch.tensor([[0.9524, 0.0354],\n        [0.9833, 0.2562],\n        [0.0607, 0.6420]])\n\ntorch.tensor([[[0.4604, 0.2699],\n         [0.8360, 0.0317],\n         [0.3289, 0.1171]]])\n\ntorch.tensor([[[[0.0730, 0.8737],\n          [0.2305, 0.4719],\n          [0.0796, 0.2745]]],\n\n        [[[0.1534, 0.9442],\n          [0.3287, 0.9040],\n          [0.0948, 0.1480]]]])\n\nLet’s create a random tensor with a single element:\n\nt = torch.rand(1)\nt\n\ntensor([0.2453])\n\n\nWe can extract the value from a tensor with one element:\n\nt.item()\n\n0.24530160427093506\n\n\nAll these tensors have a single element, but an increasing number of dimensions:\n\ntorch.rand(1)\n\ntensor([0.0539])\n\n\n\ntorch.rand(1, 1)\n\ntensor([[0.1673]])\n\n\n\ntorch.rand(1, 1, 1)\n\ntensor([[[0.8521]]])\n\n\n\ntorch.rand(1, 1, 1, 1)\n\ntensor([[[[0.4083]]]])\n\n\n\nYou can tell the number of dimensions of a tensor easily by counting the number of opening square brackets.\n\n\ntorch.rand(1, 1, 1, 1).dim()\n\n4\n\n\nTensors can have multiple elements in one dimension:\n\ntorch.rand(6)\n\ntensor([0.8350, 0.6043, 0.7573, 0.4312, 0.7121, 0.4655])\n\n\n\ntorch.rand(6).dim()\n\n1\n\n\nAnd multiple elements in multiple dimensions:\n\ntorch.rand(2, 3, 4, 5)\n\ntensor([[[[0.5859, 0.2400, 0.3767, 0.4702, 0.9221],\n          [0.7233, 0.6061, 0.1631, 0.7917, 0.0443],\n          [0.1168, 0.4800, 0.7009, 0.7263, 0.1385],\n          [0.8393, 0.6878, 0.6244, 0.3847, 0.0753]],\n\n         [[0.8900, 0.3066, 0.4179, 0.4101, 0.2723],\n          [0.3421, 0.8357, 0.0095, 0.6633, 0.6996],\n          [0.5569, 0.8118, 0.5589, 0.1866, 0.7879],\n          [0.9157, 0.3878, 0.9922, 0.6381, 0.8169]],\n\n         [[0.0905, 0.1064, 0.2240, 0.4941, 0.5899],\n          [0.6486, 0.5417, 0.6501, 0.5308, 0.0921],\n          [0.7476, 0.6017, 0.0442, 0.8713, 0.7512],\n          [0.0471, 0.4094, 0.3592, 0.0056, 0.2595]]],\n\n\n        [[[0.0631, 0.2496, 0.8283, 0.6834, 0.0634],\n          [0.7097, 0.6103, 0.5564, 0.7229, 0.7310],\n          [0.3733, 0.0948, 0.7278, 0.6178, 0.1813],\n          [0.1389, 0.2747, 0.5813, 0.3785, 0.6673]],\n\n         [[0.9272, 0.8388, 0.9478, 0.9429, 0.4678],\n          [0.6505, 0.8999, 0.3476, 0.9522, 0.2604],\n          [0.0610, 0.6362, 0.9197, 0.1989, 0.9982],\n          [0.0363, 0.6803, 0.1684, 0.5105, 0.7155]],\n\n         [[0.6207, 0.5458, 0.0307, 0.0814, 0.8699],\n          [0.0394, 0.2205, 0.4426, 0.1263, 0.1217],\n          [0.6955, 0.6588, 0.8799, 0.8124, 0.3122],\n          [0.9162, 0.5047, 0.9164, 0.4405, 0.9604]]]])\n\n\n\ntorch.rand(2, 3, 4, 5).dim()\n\n4\n\n\n\ntorch.rand(2, 3, 4, 5).numel()\n\n120\n\n\n\ntorch.ones(2, 4)\n\ntensor([[1., 1., 1., 1.],\n        [1., 1., 1., 1.]])\n\n\n\nt = torch.rand(2, 3)\ntorch.zeros_like(t)             # Matches the size of t\n\ntensor([[0., 0., 0.],\n        [0., 0., 0.]])\n\n\n\ntorch.ones_like(t)\n\ntensor([[1., 1., 1.],\n        [1., 1., 1.]])\n\n\n\ntorch.randn_like(t)\n\ntensor([[-0.0162, -0.6145,  0.2569],\n        [-1.0712, -0.3585,  0.2040]])\n\n\n\ntorch.arange(2, 10, 3)    # From 2 to 10 in increments of 3\n\ntensor([2, 5, 8])\n\n\n\ntorch.linspace(2, 10, 3)  # 3 elements from 2 to 10 on the linear scale\n\ntensor([ 2.,  6., 10.])\n\n\n\ntorch.logspace(2, 10, 3)  # Same on the log scale\n\ntensor([1.0000e+02, 1.0000e+06, 1.0000e+10])\n\n\n\ntorch.randperm(3)\n\ntensor([1, 0, 2])\n\n\n\ntorch.eye(3)\n\ntensor([[1., 0., 0.],\n        [0., 1., 0.],\n        [0., 0., 1.]])"
  },
  {
    "objectID": "ml/tensors.html#conversion-tofrom-numpy",
    "href": "ml/tensors.html#conversion-tofrom-numpy",
    "title": "PyTorch tensors",
    "section": "Conversion to/from NumPy",
    "text": "Conversion to/from NumPy\nPyTorch tensors can be converted to NumPy ndarrays and vice-versa in a very efficient manner as both objects share the same memory.\n\nFrom PyTorch tensor to NumPy ndarray\n\nt = torch.rand(2, 3)\nt\n\ntensor([[0.3260, 0.9113, 0.9419],\n        [0.4003, 0.0047, 0.3370]])\n\n\n\nt_np = t.numpy()\nt_np\n\narray([[0.32604218, 0.9112866 , 0.94185126],\n       [0.40029067, 0.00469434, 0.33698457]], dtype=float32)\n\n\n\n\nFrom NumPy ndarray to PyTorch tensor\n\nimport numpy as np\na = np.random.rand(2, 3)\na\n\narray([[0.17371149, 0.49316171, 0.12868488],\n       [0.17542802, 0.39272544, 0.27179452]])\n\n\n\na_pt = torch.from_numpy(a)\na_pt\n\ntensor([[0.1737, 0.4932, 0.1287],\n        [0.1754, 0.3927, 0.2718]], dtype=torch.float64)\n\n\n\nNote the different default data types."
  },
  {
    "objectID": "ml/tensors.html#indexing-tensors",
    "href": "ml/tensors.html#indexing-tensors",
    "title": "PyTorch tensors",
    "section": "Indexing tensors",
    "text": "Indexing tensors\n\nt = torch.rand(3, 4)\nt\n\ntensor([[0.4746, 0.6279, 0.7102, 0.2380],\n        [0.0749, 0.7746, 0.6106, 0.1359],\n        [0.6712, 0.0485, 0.7267, 0.3550]])\n\n\n\nt[:, 2]\n\ntensor([0.7102, 0.6106, 0.7267])\n\n\n\nt[1, :]\n\ntensor([0.0749, 0.7746, 0.6106, 0.1359])\n\n\n\nt[2, 3]\n\ntensor(0.3550)\n\n\n\nA word of caution about indexing\nWhile indexing elements of a tensor to extract some of the data as a final step of some computation is fine, you should not use indexing to run operations on tensor elements in a loop as this would be extremely inefficient.\nInstead, you want to use vectorized operations."
  },
  {
    "objectID": "ml/tensors.html#vectorized-operations",
    "href": "ml/tensors.html#vectorized-operations",
    "title": "PyTorch tensors",
    "section": "Vectorized operations",
    "text": "Vectorized operations\nSince PyTorch tensors are homogeneous (i.e. made of a single data type), as with NumPy’s ndarrays, operations are vectorized and thus fast.\nNumPy is mostly written in C, PyTorch in C++. With either library, when you run vectorized operations on arrays/tensors, you don’t use raw Python (slow) but compiled C/C++ code (much faster).\nHere is an excellent post explaining Python vectorization & why it makes such a big difference."
  },
  {
    "objectID": "ml/tensors.html#data-types",
    "href": "ml/tensors.html#data-types",
    "title": "PyTorch tensors",
    "section": "Data types",
    "text": "Data types\n\nDefault data type\nSince PyTorch tensors were built with efficiency in mind for neural networks, the default data type is 32-bit floating points.\nThis is sufficient for accuracy and much faster than 64-bit floating points.\n\nBy contrast, NumPy ndarrays use 64-bit as their default.\n\n\nt = torch.rand(2, 4)\nt.dtype\n\ntorch.float32\n\n\n\n\nSetting data type at creation\nThe type can be set with the dtype argument:\n\nt = torch.rand(2, 4, dtype=torch.float64)\nt\n\ntensor([[0.5718, 0.4599, 0.2844, 0.5194],\n        [0.6349, 0.7234, 0.1780, 0.0882]], dtype=torch.float64)\n\n\n\nPrinted tensors display attributes with values ≠ default values.\n\n\nt.dtype\n\ntorch.float64\n\n\n\n\nChanging data type\n\nt = torch.rand(2, 4)\nt.dtype\n\ntorch.float32\n\n\n\nt2 = t.type(torch.float64)\nt2.dtype\n\ntorch.float64\n\n\n\n\nList of data types\n\n\n\n\n\n\n\ndtype\nDescription\n\n\n\n\ntorch.float16 / torch.half\n16-bit / half-precision floating-point\n\n\ntorch.float32 / torch.float\n32-bit / single-precision floating-point\n\n\ntorch.float64 / torch.double\n64-bit / double-precision floating-point\n\n\ntorch.uint8\nunsigned 8-bit integers\n\n\ntorch.int8\nsigned 8-bit integers\n\n\ntorch.int16 / torch.short\nsigned 16-bit integers\n\n\ntorch.int32 / torch.int\nsigned 32-bit integers\n\n\ntorch.int64 / torch.long\nsigned 64-bit integers\n\n\ntorch.bool\nboolean"
  },
  {
    "objectID": "ml/tensors.html#simple-operations",
    "href": "ml/tensors.html#simple-operations",
    "title": "PyTorch tensors",
    "section": "Simple operations",
    "text": "Simple operations\n\nt1 = torch.tensor([[1, 2], [3, 4]])\nt1\n\ntensor([[1, 2],\n        [3, 4]])\n\n\n\nt2 = torch.tensor([[1, 1], [0, 0]])\nt2\n\ntensor([[1, 1],\n        [0, 0]])\n\n\nOperation performed between elements at corresponding locations:\n\nt1 + t2\n\ntensor([[2, 3],\n        [3, 4]])\n\n\nOperation applied to each element of the tensor:\n\nt1 + 1\n\ntensor([[2, 3],\n        [4, 5]])\n\n\n\nReduction\n\nt = torch.ones(2, 3, 4);\nt\n\ntensor([[[1., 1., 1., 1.],\n         [1., 1., 1., 1.],\n         [1., 1., 1., 1.]],\n\n        [[1., 1., 1., 1.],\n         [1., 1., 1., 1.],\n         [1., 1., 1., 1.]]])\n\n\n\nt.sum()   # Reduction over all entries\n\ntensor(24.)\n\n\n\nOther reduction functions (e.g. mean) behave the same way.\n\nReduction over a specific dimension:\n\nt.sum(0)\n\ntensor([[2., 2., 2., 2.],\n        [2., 2., 2., 2.],\n        [2., 2., 2., 2.]])\n\n\n\nt.sum(1)\n\ntensor([[3., 3., 3., 3.],\n        [3., 3., 3., 3.]])\n\n\n\nt.sum(2)\n\ntensor([[4., 4., 4.],\n        [4., 4., 4.]])\n\n\nReduction over multiple dimensions:\n\nt.sum((0, 1))\n\ntensor([6., 6., 6., 6.])\n\n\n\nt.sum((0, 2))\n\ntensor([8., 8., 8.])\n\n\n\nt.sum((1, 2))\n\ntensor([12., 12.])\n\n\n\n\nIn-place operations\nWith operators post-fixed with _:\n\nt1 = torch.tensor([1, 2])\nt1\n\ntensor([1, 2])\n\n\n\nt2 = torch.tensor([1, 1])\nt2\n\ntensor([1, 1])\n\n\n\nt1.add_(t2)\nt1\n\ntensor([2, 3])\n\n\n\nt1.zero_()\nt1\n\ntensor([0, 0])\n\n\n\nWhile reassignments will use new addresses in memory, in-place operations will use the same addresses.\n\n\n\nTensor views\nt = torch.tensor([[1, 2, 3], [4, 5, 6]]); print(t)\nt.size()\nt.view(6)\nt.view(3, 2)\nt.view(3, -1) # Same: with -1, the size is inferred from other dimensions\n\nNote the difference\n\nt1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\nt1\n\ntensor([[1, 2, 3],\n        [4, 5, 6]])\n\n\n\nt2 = t1.t()\nt2\n\ntensor([[1, 4],\n        [2, 5],\n        [3, 6]])\n\n\n\nt3 = t1.view(3, 2)\nt3\n\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])\n\n\n\n\n\nLogical operations\n\nt1 = torch.randperm(5)\nt1\n\ntensor([1, 2, 0, 4, 3])\n\n\n\nt2 = torch.randperm(5)\nt2\n\ntensor([1, 2, 4, 0, 3])\n\n\nTest each element:\n\nt1 &gt; 3\n\ntensor([False, False, False,  True, False])\n\n\nTest corresponding pairs of elements:\n\nt1 &lt; t2\n\ntensor([False, False,  True, False, False])"
  },
  {
    "objectID": "ml/tensors.html#device-attribute",
    "href": "ml/tensors.html#device-attribute",
    "title": "PyTorch tensors",
    "section": "Device attribute",
    "text": "Device attribute\nTensor data can be placed in the memory of various processor types:\n\nthe RAM of CPU,\nthe RAM of a GPU with CUDA support,\nthe RAM of a GPU with AMD’s ROCm support,\nthe RAM of an XLA device (e.g. Cloud TPU) with the torch_xla package.\n\nThe values for the device attributes are:\n\nCPU:  'cpu',\nGPU (CUDA & AMD’s ROCm):  'cuda',\nXLA:  xm.xla_device().\n\nThis last option requires to load the torch_xla package first:\nimport torch_xla\nimport torch_xla.core.xla_model as xm\n\nCreating a tensor on a specific device\nBy default, tensors are created on the CPU.\nYou can create a tensor on an accelerator by specifying the device attribute (our current training cluster does not have GPUs, so don’t run this on it):\nt_gpu = torch.rand(2, device='cuda')\n\n\nCopying a tensor to a specific device\nYou can also make copies of a tensor on other devices:\n# Make a copy of t on the GPU\nt_gpu = t.to(device='cuda')\nt_gpu = t.cuda()             # Alternative syntax\n\n# Make a copy of t_gpu on the CPU\nt = t_gpu.to(device='cpu')\nt = t_gpu.cpu()              # Alternative syntax\n\n\nMultiple GPUs\nIf you have multiple GPUs, you can optionally specify which one a tensor should be created on or copied to:\nt1 = torch.rand(2, device='cuda:0')  # Create a tensor on 1st GPU\nt2 = t1.to(device='cuda:0')          # Make a copy of t1 on 1st GPU\nt3 = t1.to(device='cuda:1')          # Make a copy of t1 on 2nd GPU\nOr the equivalent short forms:\nt2 = t1.cuda(0)\nt3 = t1.cuda(1)"
  },
  {
    "objectID": "ml/workflow.html",
    "href": "ml/workflow.html",
    "title": "Overall workflow",
    "section": "",
    "text": "This classic PyTorch tutorial goes over the entire workflow to create and train a simple image classifier.\nLet’s go over it step by step."
  },
  {
    "objectID": "ml/workflow.html#the-data",
    "href": "ml/workflow.html#the-data",
    "title": "Overall workflow",
    "section": "The data",
    "text": "The data\nCIFAR-10 from the Canadian Institute for Advanced Research is a classic dataset of 60,000 color images falling into 10 classes (6,000 images in each class):\n\nairplane\nautomobile\nbird\ncat\ndeer\ndog\nfrog\nhorse\nship\ntruck\n\nThe images are of size 32x32 pixels (tiny!), which makes it very lightweight, quick to load and easy to play with.\n\nCreate a DataLoader\nA DataLoader is an iterable feeding data to a model at each iteration. The data loader transforms the data to the proper format, sets the batch size, whether the data is shuffled or not, and how the I/O is parallelized. You can create DataLoaders with the torch.utils.data.DataLoader class.\nLet’s create 2 DataLoaders: one for the train set and one for the test set.\n\nLoad packages\n\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\n\n\n\nCreate a transform object\nThe CIFAR-10 images in the TorchVision library are Image objects (from the PIL.Image module of the pillow package).\nWe need to normalize them and turn them into tensors:\n\ntransform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\n\n\nChose a batch size\nRemember that the data move forward through the network (forward pass), outputting some estimates which are used to calculate some loss (or error) value. Then we get gradients through automatic differentiation and the model parameters are adjusted a little through gradient descent.\nYou do not have to have the entire training set go through this process each time: you can use batches.\nThe batch size is the number of items from the data that are processed before the model is updated. There is no hard rule to set good batch sizes and sizes tend to be picked through trial and error.\nHere are some rules to chose a batch size:\n\nmake sure that the batch fits in the CPU or GPU,\nsmall batches give faster results (each training iteration is very fast), but give less accuracy,\nlarge batches lead to slower training, but better accuracy.\n\nLet’s set the batch size to 4:\n\nbatch_size = 4\n\n\n\nPut it together into DataLoaders\n\ntrainset = torchvision.datasets.CIFAR10(root='./data',\n                                        train=True,\n                                        download=True,\n                                        transform=transform)\n\ntrainloader = torch.utils.data.DataLoader(trainset,\n                                          batch_size=batch_size,\n                                          shuffle=True,\n                                          num_workers=2)\n\ntestset = torchvision.datasets.CIFAR10(root='./data',\n                                       train=False,\n                                       download=True,\n                                       transform=transform)\n\ntestloader = torch.utils.data.DataLoader(testset,\n                                         batch_size=batch_size,\n                                         shuffle=False,\n                                         num_workers=2)\n\nWe will also need the classes:\n\nclasses = ('plane', 'car', 'bird', 'cat', 'deer',\n           'dog', 'frog', 'horse', 'ship', 'truck')\n\n\n\n\nVisualize a sample of the data\nThough not necessary, it can be useful to have a look at the data:\n\n# Load the packages for this\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Define a function to display an image\ndef imshow(img):\n    img = img / 2 + 0.5     # unnormalize\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\n# Get a batch of random training images\ndataiter = iter(trainloader)\nimages, labels = next(dataiter)\n\n# Display the images\nimshow(torchvision.utils.make_grid(images))\n\n# Print the labels\nprint(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))\n\n\n\n\nplane truck dog   bird"
  },
  {
    "objectID": "ml/workflow.html#the-model",
    "href": "ml/workflow.html#the-model",
    "title": "Overall workflow",
    "section": "The model",
    "text": "The model\n\nArchitecture\nFirst, we need to define the architecture of the network. There are many types of architectures. For images, CNN are well suited.\nIn Python, you can define a subclass of an existing class with:\nclass YourSubclass(BaseClass):\n    &lt;definition of your subclass&gt;        \nThe subclass is derived from the base class and inherits its properties. PyTorch contains the class torch.nn.Module which is used as the base class when defining a neural network.\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Net(nn.Module):\n    # Define the architecture of the network\n    def __init__(self):\n        super().__init__()\n        # 3 input image channel (3 colour channels)\n        # 6 output channels,\n        # 5x5 square convolution kernel\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        # Max pooling over a (2, 2) window\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        # 5*5 from image dimension\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        # 10 is the size of the output layer\n        # since there are 10 classes\n        self.fc3 = nn.Linear(84, 10)\n\n    # Set the flow of data through the network for the forward pass\n    # x represents the data\n    def forward(self, x):\n        # F.relu is the rectified-linear activation function\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        # flatten all dimensions except the batch dimension\n        x = torch.flatten(x, 1)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nLet’s create an instance of Net and print its structure:\n\nnet = Net()\nprint(net)\n\nNet(\n  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n  (fc1): Linear(in_features=400, out_features=120, bias=True)\n  (fc2): Linear(in_features=120, out_features=84, bias=True)\n  (fc3): Linear(in_features=84, out_features=10, bias=True)\n)\n\n\n\n\nLoss function and optimizer\nWe need to chose a loss function that will be used to calculate the gradients through backpropagation as well as an optimizer to do the gradient descent.\nSGD with momentum has proved a very efficient optimizing technique and is widely used.\n\nimport torch.optim as optim\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
  },
  {
    "objectID": "ml/workflow.html#training",
    "href": "ml/workflow.html#training",
    "title": "Overall workflow",
    "section": "Training",
    "text": "Training\nWe can now train the model:\n\nfor epoch in range(2):  # loop over the dataset twice\n\n    running_loss = 0.0\n\n    for i, data in enumerate(trainloader, 0):\n        # get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        # print statistics\n        running_loss += loss.item()\n        if i % 2000 == 1999:    # print every 2000 mini-batches\n            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n            running_loss = 0.0\n\nprint('Finished Training')\n\n[1,  2000] loss: 2.238\n[1,  4000] loss: 1.882\n[1,  6000] loss: 1.670\n[1,  8000] loss: 1.584\n[1, 10000] loss: 1.513\n[1, 12000] loss: 1.470\n[2,  2000] loss: 1.369\n[2,  4000] loss: 1.365\n[2,  6000] loss: 1.336\n[2,  8000] loss: 1.287\n[2, 10000] loss: 1.289\n[2, 12000] loss: 1.271\nFinished Training"
  },
  {
    "objectID": "ml/workflow.html#testing",
    "href": "ml/workflow.html#testing",
    "title": "Overall workflow",
    "section": "Testing",
    "text": "Testing\n\nLittle test on one batch for fun\nLet’s now test our model on one batch of testing data.\nFirst, let’s get a batch of random testing data:\n\ndataiter = iter(testloader)\nimages, labels = next(dataiter)\n\nLet’s display them and print their true labels:\n\nimshow(torchvision.utils.make_grid(images))\nprint('Real: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))\n\n\n\n\nReal:  cat   ship  ship  plane\n\n\nNow, let’s run the same batch of testing images through our model:\n\noutputs = net(images)\n\nLet’s get the best predictions for these:\n\n_, predicted = torch.max(outputs, 1)\n\nprint('Predicted: ', ' '.join(f'{classes[predicted[j]]:5s}'\n                              for j in range(4)))\n\nPredicted:  cat   ship  ship  plane\n\n\n\n\nMore serious testing\nThis was fun, but of course, with a sample of one, we can’t say anything about how good our model is. We need to test it on many more images from the test set.\nLet’s use the entire test set:\n\ncorrect = 0\ntotal = 0\n# since we're not training, we don't need to calculate the gradients for our outputs\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        # calculate outputs by running images through the network\n        outputs = net(images)\n        # the class with the highest energy is what we choose as prediction\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n\nAccuracy of the network on the 10000 test images: 55 %\n\n\n\n\nPer class testing\nWe could see whether the model seem to perform better for some classes than others:\n\n# prepare to count predictions for each class\ncorrect_pred = {classname: 0 for classname in classes}\ntotal_pred = {classname: 0 for classname in classes}\n\n# again no gradients needed\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        outputs = net(images)\n        _, predictions = torch.max(outputs, 1)\n        # collect the correct predictions for each class\n        for label, prediction in zip(labels, predictions):\n            if label == prediction:\n                correct_pred[classes[label]] += 1\n            total_pred[classes[label]] += 1\n\n\n# print accuracy for each class\nfor classname, correct_count in correct_pred.items():\n    accuracy = 100 * float(correct_count) / total_pred[classname]\n    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')\n\nAccuracy for class: plane is 53.9 %\nAccuracy for class: car   is 65.9 %\nAccuracy for class: bird  is 52.8 %\nAccuracy for class: cat   is 41.7 %\nAccuracy for class: deer  is 20.5 %\nAccuracy for class: dog   is 45.0 %\nAccuracy for class: frog  is 68.6 %\nAccuracy for class: horse is 65.2 %\nAccuracy for class: ship  is 72.9 %\nAccuracy for class: truck is 68.1 %"
  },
  {
    "objectID": "newsletter.html",
    "href": "newsletter.html",
    "title": "Training events mailing list",
    "section": "",
    "text": "If you want to get informed about upcoming training events offered by SFU on behalf of Western Universities, please subscribe to our mailing list: \n(We will only email you about training events.)"
  },
  {
    "objectID": "r_intro/basics.html",
    "href": "r_intro/basics.html",
    "title": "First steps in R",
    "section": "",
    "text": "In this section, we take our first few steps in R: we will access the R documentation, see how to set R options, and talk about a few concepts."
  },
  {
    "objectID": "r_intro/basics.html#help-and-documentation",
    "href": "r_intro/basics.html#help-and-documentation",
    "title": "First steps in R",
    "section": "Help and documentation",
    "text": "Help and documentation\nFor some general documentation on R, you can run:\nhelp.start()\nTo get help on a function (e.g. sum), you can run:\nhelp(sum)\nDepending on your settings, this will open a documentation for sum in a pager or in your browser."
  },
  {
    "objectID": "r_intro/basics.html#r-settings",
    "href": "r_intro/basics.html#r-settings",
    "title": "First steps in R",
    "section": "R settings",
    "text": "R settings\nSettings are saved in a .Rprofile file. You can edit the file directly in any text editor or from within R.\nList all options:\noptions()\nReturn the value of a particular option:\n\ngetOption(\"help_type\")\n\n[1] \"html\"\n\n\nSet an option:\noptions(help_type = \"html\")"
  },
  {
    "objectID": "r_intro/basics.html#assignment",
    "href": "r_intro/basics.html#assignment",
    "title": "First steps in R",
    "section": "Assignment",
    "text": "Assignment\nR can accept the equal sign (=) for assignments, but it is more idiomatic to use the assignment sign (&lt;-) whenever you bind a name to a value and to use the equal sign everywhere else.\n\na &lt;- 3\n\nOnce you have bound a name to a value, you can recall the value with that name:\n\na  # Note that you do not need to use a print() function in R\n\n[1] 3\n\n\nYou can remove an object from the environment by deleting its name:\n\nrm(a)\n\nLet’s confirm that a doesn’t exist anymore in the environment:\n\na\n\nError in eval(expr, envir, enclos): object 'a' not found\n\n\nThe garbage collector will take care of deleting the object itself from memory."
  },
  {
    "objectID": "r_intro/basics.html#comments",
    "href": "r_intro/basics.html#comments",
    "title": "First steps in R",
    "section": "Comments",
    "text": "Comments\nAnything to the left of # is a comment and is ignored by R:\n\n# This is an inline comment\n\na &lt;- 3  # This is also a comment"
  },
  {
    "objectID": "r_intro/control_flow.html",
    "href": "r_intro/control_flow.html",
    "title": "Control flow",
    "section": "",
    "text": "Control flow statements alter the linear execution of code, allowing for one or another section of code to be executed, or for one section of code to be executed multiple times."
  },
  {
    "objectID": "r_intro/control_flow.html#conditionals",
    "href": "r_intro/control_flow.html#conditionals",
    "title": "Control flow",
    "section": "Conditionals",
    "text": "Conditionals\nConditionals determine which section of code is to be ran based on predicates. A predicate is a test that returns either TRUE or FALSE.\nHere is an example:\n\ntest_sign &lt;- function(x) {\n  if (x &gt; 0) {\n    \"x is positif\"\n  } else if (x &lt; 0) {\n    \"x is negatif\"\n  } else {\n    \"x is equal to zero\"\n  }\n}\n\ntest_sign() is a function that accepts one argument. Depending on the value of that argument, one of three snippets of code is executed:\n\ntest_sign(3)\n\n[1] \"x is positif\"\n\ntest_sign(-2)\n\n[1] \"x is negatif\"\n\ntest_sign(0)\n\n[1] \"x is equal to zero\""
  },
  {
    "objectID": "r_intro/control_flow.html#loops",
    "href": "r_intro/control_flow.html#loops",
    "title": "Control flow",
    "section": "Loops",
    "text": "Loops\nLoops allow to run the same instruction on various elements:\n\nfor (i in 1:10) {\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10"
  },
  {
    "objectID": "r_intro/data_structure.html",
    "href": "r_intro/data_structure.html",
    "title": "Data types and structures",
    "section": "",
    "text": "This section covers the various data types and structures available in R."
  },
  {
    "objectID": "r_intro/data_structure.html#summary-of-structures",
    "href": "r_intro/data_structure.html#summary-of-structures",
    "title": "Data types and structures",
    "section": "Summary of structures",
    "text": "Summary of structures\n\n\n\nDimension\nHomogeneous\nHeterogeneous\n\n\n\n\n1 d\nAtomic vector\nList\n\n\n2 d\nMatrix\nData frame\n\n\n3 d\nArray"
  },
  {
    "objectID": "r_intro/data_structure.html#atomic-vectors",
    "href": "r_intro/data_structure.html#atomic-vectors",
    "title": "Data types and structures",
    "section": "Atomic vectors",
    "text": "Atomic vectors\n\nWith a single element\n\na &lt;- 2\na\n\n[1] 2\n\ntypeof(a)\n\n[1] \"double\"\n\nstr(a)\n\n num 2\n\nlength(a)\n\n[1] 1\n\ndim(a)\n\nNULL\n\n\nThe dim attribute of a vector doesn’t exist (hence the NULL). This makes vectors different from one-dimensional arrays which have a dim of 1.\nYou might have noticed that 2 is a double (double precision floating point number, equivalent of “float” in other languages). In R, this is the default, even if you don’t type 2.0. This prevents the kind of weirdness you can find in, for instance, Python.\nIn Python:\n&gt;&gt;&gt; 2 == 2.0\nTrue\n&gt;&gt;&gt; type(2) == type(2.0)\nFalse\n&gt;&gt;&gt; type(2)\n&lt;class 'int'&gt;\n&gt;&gt;&gt; type(2.0)\n&lt;class 'float'&gt;\nIn R:\n&gt; 2 == 2.0\n[1] TRUE\n&gt; typeof(2) == typeof(2.0)\n[1] TRUE\n&gt; typeof(2)\n[1] \"double\"\n&gt; typeof(2.0)\n[1] \"double\"\nIf you want to define an integer variable, you use:\n\nb &lt;- 2L\nb\n\n[1] 2\n\ntypeof(b)\n\n[1] \"integer\"\n\nmode(b)\n\n[1] \"numeric\"\n\nstr(b)\n\n int 2\n\n\nThere are six vector types:\n\nlogical\ninteger\ndouble\ncharacter\ncomplex\nraw\n\n\n\nWith multiple elements\n\nc &lt;- c(2, 4, 1)\nc\n\n[1] 2 4 1\n\ntypeof(c)\n\n[1] \"double\"\n\nmode(c)\n\n[1] \"numeric\"\n\nstr(c)\n\n num [1:3] 2 4 1\n\n\n\nd &lt;- c(TRUE, TRUE, NA, FALSE)\nd\n\n[1]  TRUE  TRUE    NA FALSE\n\ntypeof(d)\n\n[1] \"logical\"\n\nstr(d)\n\n logi [1:4] TRUE TRUE NA FALSE\n\n\n\nNA (“Not Available”) is a logical constant of length one. It is an indicator for a missing value.\n\nVectors are homogeneous, so all elements need to be of the same type.\nIf you use elements of different types, R will convert some of them to ensure that they become of the same type:\n\ne &lt;- c(\"This is a string\", 3, \"test\")\ne\n\n[1] \"This is a string\" \"3\"                \"test\"            \n\ntypeof(e)\n\n[1] \"character\"\n\nstr(e)\n\n chr [1:3] \"This is a string\" \"3\" \"test\"\n\n\n\nf &lt;- c(TRUE, 3, FALSE)\nf\n\n[1] 1 3 0\n\ntypeof(f)\n\n[1] \"double\"\n\nstr(f)\n\n num [1:3] 1 3 0\n\n\n\ng &lt;- c(2L, 3, 4L)\ng\n\n[1] 2 3 4\n\ntypeof(g)\n\n[1] \"double\"\n\nstr(g)\n\n num [1:3] 2 3 4\n\n\n\nh &lt;- c(\"string\", TRUE, 2L, 3.1)\nh\n\n[1] \"string\" \"TRUE\"   \"2\"      \"3.1\"   \n\ntypeof(h)\n\n[1] \"character\"\n\nstr(h)\n\n chr [1:4] \"string\" \"TRUE\" \"2\" \"3.1\"\n\n\nThe binary operator : is equivalent to the seq() function and generates a regular sequence of integers:\n\ni &lt;- 1:5\ni\n\n[1] 1 2 3 4 5\n\ntypeof(i)\n\n[1] \"integer\"\n\nstr(i)\n\n int [1:5] 1 2 3 4 5\n\nidentical(2:8, seq(2, 8))\n\n[1] TRUE"
  },
  {
    "objectID": "r_intro/data_structure.html#matrices",
    "href": "r_intro/data_structure.html#matrices",
    "title": "Data types and structures",
    "section": "Matrices",
    "text": "Matrices\n\nj &lt;- matrix(1:12, nrow = 3, ncol = 4)\nj\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\ntypeof(j)\n\n[1] \"integer\"\n\nstr(j)\n\n int [1:3, 1:4] 1 2 3 4 5 6 7 8 9 10 ...\n\nlength(j)\n\n[1] 12\n\ndim(j)\n\n[1] 3 4\n\n\nThe default is byrow = FALSE. If you want the matrix to be filled in by row, you need to set this argument to TRUE:\n\nk &lt;- matrix(1:12, nrow = 3, ncol = 4, byrow = TRUE)\nk\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12"
  },
  {
    "objectID": "r_intro/data_structure.html#arrays",
    "href": "r_intro/data_structure.html#arrays",
    "title": "Data types and structures",
    "section": "Arrays",
    "text": "Arrays\n\nl &lt;- array(as.double(1:24), c(3, 2, 4))\nl\n\n, , 1\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n, , 2\n\n     [,1] [,2]\n[1,]    7   10\n[2,]    8   11\n[3,]    9   12\n\n, , 3\n\n     [,1] [,2]\n[1,]   13   16\n[2,]   14   17\n[3,]   15   18\n\n, , 4\n\n     [,1] [,2]\n[1,]   19   22\n[2,]   20   23\n[3,]   21   24\n\ntypeof(l)\n\n[1] \"double\"\n\nstr(l)\n\n num [1:3, 1:2, 1:4] 1 2 3 4 5 6 7 8 9 10 ...\n\nlength(l)\n\n[1] 24\n\ndim(l)\n\n[1] 3 2 4"
  },
  {
    "objectID": "r_intro/data_structure.html#lists",
    "href": "r_intro/data_structure.html#lists",
    "title": "Data types and structures",
    "section": "Lists",
    "text": "Lists\n\nm &lt;- list(2, 3)\nm\n\n[[1]]\n[1] 2\n\n[[2]]\n[1] 3\n\ntypeof(m)\n\n[1] \"list\"\n\nstr(m)\n\nList of 2\n $ : num 2\n $ : num 3\n\nlength(m)\n\n[1] 2\n\ndim(m)\n\nNULL\n\n\nAs with atomic vectors, lists do not have a dim attribute. Lists are in fact a different type of vectors.\nLists can be heterogeneous:\n\nn &lt;- list(2L, 3, c(2, 1), FALSE, \"string\")\nn\n\n[[1]]\n[1] 2\n\n[[2]]\n[1] 3\n\n[[3]]\n[1] 2 1\n\n[[4]]\n[1] FALSE\n\n[[5]]\n[1] \"string\"\n\ntypeof(n)\n\n[1] \"list\"\n\nstr(n)\n\nList of 5\n $ : int 2\n $ : num 3\n $ : num [1:2] 2 1\n $ : logi FALSE\n $ : chr \"string\"\n\nlength(n)\n\n[1] 5"
  },
  {
    "objectID": "r_intro/data_structure.html#data-frames",
    "href": "r_intro/data_structure.html#data-frames",
    "title": "Data types and structures",
    "section": "Data frames",
    "text": "Data frames\nData frames contain tabular data. Under the hood, a data frame is a list of vectors.\n\no &lt;- data.frame(\n  country = c(\"Canada\", \"USA\", \"Mexico\"),\n  var = c(2.9, 3.1, 4.5)\n)\no\n\n  country var\n1  Canada 2.9\n2     USA 3.1\n3  Mexico 4.5\n\ntypeof(o)\n\n[1] \"list\"\n\nstr(o)\n\n'data.frame':   3 obs. of  2 variables:\n $ country: chr  \"Canada\" \"USA\" \"Mexico\"\n $ var    : num  2.9 3.1 4.5\n\nlength(o)\n\n[1] 2\n\ndim(o)\n\n[1] 3 2"
  },
  {
    "objectID": "r_intro/functions.html",
    "href": "r_intro/functions.html",
    "title": "Function definition",
    "section": "",
    "text": "R comes with a number of built-in functions. Packages can provide additional ones. In many cases however, you will want to create your own functions to perform exactly the computations that you need.\nIn this section, we will see how to define new functions."
  },
  {
    "objectID": "r_intro/functions.html#syntax",
    "href": "r_intro/functions.html#syntax",
    "title": "Function definition",
    "section": "Syntax",
    "text": "Syntax\nHere is the syntax to define a new function:\nname &lt;- function(arguments) {\n  body\n}"
  },
  {
    "objectID": "r_intro/functions.html#example",
    "href": "r_intro/functions.html#example",
    "title": "Function definition",
    "section": "Example",
    "text": "Example\nLet’s define a function that we call compare which will compare the value between 2 numbers:\n\ncompare &lt;- function(x, y) {\n  x == y\n}\n\n\ncompare is the name of our function.\nx and y are the placeholders for the arguments that our function will accept (our function will need 2 arguments to run successfully).\nx == y is the body of the function, that is, the computation performed by our function.\n\nWe can now use our function:\n\ncompare(2, 3)\n\n[1] FALSE"
  },
  {
    "objectID": "r_intro/functions.html#what-is-returned-by-a-function",
    "href": "r_intro/functions.html#what-is-returned-by-a-function",
    "title": "Function definition",
    "section": "What is returned by a function?",
    "text": "What is returned by a function?\nIn R, the result of the last statement is printed automatically:\n\ntest &lt;- function(x, y) {\n  x\n  y\n}\ntest(2, 3)\n\n[1] 3\n\n\nIf you want to also print other results, you need to explicitly use the print() function:\n\ntest &lt;- function(x, y) {\n  print(x)\n  y\n}\ntest(2, 3)\n\n[1] 2\n\n\n[1] 3\n\n\nNote that, unlike print(), the function return() exits the function:\n\ntest &lt;- function(x, y) {\n  return(x)\n  y\n}\ntest(2, 3)\n\n[1] 2\n\n\n\ntest &lt;- function(x, y) {\n  return(x)\n  return(y)\n}\ntest(2, 3)\n\n[1] 2"
  },
  {
    "objectID": "r_intro/index.html",
    "href": "r_intro/index.html",
    "title": "Intro R",
    "section": "",
    "text": "Date:\nTuesday, June 20, 2023\nTime:\n9am–noon\nInstructor:\nMarie-Hélène Burle (Simon Fraser University)\nPrerequisites:\nThis introductory course does not require any previous experience.\nSoftware:\nAs we will provide access to an RStudio Server, no installation is required.\n\n Start course ➤"
  },
  {
    "objectID": "r_intro/indexing.html",
    "href": "r_intro/indexing.html",
    "title": "Indexing",
    "section": "",
    "text": "This section covers indexing from the various data structures."
  },
  {
    "objectID": "r_intro/indexing.html#indexing-atomic-vectors",
    "href": "r_intro/indexing.html#indexing-atomic-vectors",
    "title": "Indexing",
    "section": "Indexing atomic vectors",
    "text": "Indexing atomic vectors\n\nHere is an example with an atomic vector of size one:\n\nIndexing in R starts at 1 and is done with square brackets next to the element to index:\n\nx &lt;- 2\nx\n\n[1] 2\n\nx[1]\n\n[1] 2\n\n\nWhat happens if we index out of range?\n\nx[2]\n\n[1] NA\n\n\n\nExample for an atomic vector with multiple elements:\n\n\nx &lt;- c(2, 4, 1)\nx\n\n[1] 2 4 1\n\nx[2]\n\n[1] 4\n\nx[2:4]\n\n[1]  4  1 NA\n\n\n\nModifying mutable objects\nIndexing also allows to modify some of the values of mutable objects:\n\nx\n\n[1] 2 4 1\n\nx[2] &lt;- 0\nx\n\n[1] 2 0 1\n\n\n\n\nCopy-on-modify\nNot all languages behave the same when you assign the same mutable object to several variables, then modify one of them.\n\nIn Python: no copy-on-modify\n\nDon’t try to run this code in R. This is for information only.\n\n\n\nPython\n\na = [1, 2, 3]\nb = a\nb\n\n[1, 2, 3]\n\n\nPython\n\na[0] = 4           # In Python, indexing starts at 0\na\n\n[4, 2, 3]\n\n\nPython\n\nb\n\n[4, 2, 3]\nModifying a also modifies b: this is because no copy is made when you modify a. If you want to keep b unchanged, you need to assign an explicit copy of a to it with b = copy.copy(a).\n\n\nIn R: copy-on-modify\n\na &lt;- c(1, 2, 3)\nb &lt;- a\nb\n\n[1] 1 2 3\n\na[1] &lt;- 4          # In R, indexing starts at 1\na\n\n[1] 4 2 3\n\nb\n\n[1] 1 2 3\n\n\nHere, the default is to create a new copy in memory when a is transformed so that b remains unchanged."
  },
  {
    "objectID": "r_intro/indexing.html#indexing-matrices-and-arrays",
    "href": "r_intro/indexing.html#indexing-matrices-and-arrays",
    "title": "Indexing",
    "section": "Indexing matrices and arrays",
    "text": "Indexing matrices and arrays\n\nx &lt;- matrix(1:12, nrow = 3, ncol = 4)\nx\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\nx[2, 3]\n\n[1] 8\n\nx &lt;- array(as.double(1:24), c(3, 2, 4))\nx\n\n, , 1\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n, , 2\n\n     [,1] [,2]\n[1,]    7   10\n[2,]    8   11\n[3,]    9   12\n\n, , 3\n\n     [,1] [,2]\n[1,]   13   16\n[2,]   14   17\n[3,]   15   18\n\n, , 4\n\n     [,1] [,2]\n[1,]   19   22\n[2,]   20   23\n[3,]   21   24\n\nx[2, 1, 3]\n\n[1] 14"
  },
  {
    "objectID": "r_intro/indexing.html#indexing-lists",
    "href": "r_intro/indexing.html#indexing-lists",
    "title": "Indexing",
    "section": "Indexing lists",
    "text": "Indexing lists\n\nx &lt;- list(2L, 3:8, c(2, 1), FALSE, \"string\")\nx\n\n[[1]]\n[1] 2\n\n[[2]]\n[1] 3 4 5 6 7 8\n\n[[3]]\n[1] 2 1\n\n[[4]]\n[1] FALSE\n\n[[5]]\n[1] \"string\"\n\n\nIndexing a list returns a list:\n\nx[3]\n\n[[1]]\n[1] 2 1\n\ntypeof(x[3])\n\n[1] \"list\"\n\n\nTo extract elements of a list, double square brackets are required:\n\nx[[3]]\n\n[1] 2 1\n\ntypeof(x[[3]])\n\n[1] \"double\"\n\n\n\n\nYour turn:\n\nTry to extract the number 7 from this list."
  },
  {
    "objectID": "r_intro/indexing.html#indexing-data-frames",
    "href": "r_intro/indexing.html#indexing-data-frames",
    "title": "Indexing",
    "section": "Indexing data frames",
    "text": "Indexing data frames\n\nx &lt;- data.frame(\n  country = c(\"Canada\", \"USA\", \"Mexico\"),\n  var = c(2.9, 3.1, 4.5)\n)\nx\n\n  country var\n1  Canada 2.9\n2     USA 3.1\n3  Mexico 4.5\n\n\nA data frame is a list of atomic vectors representing the various columns:\n\nx[1]\n\n  country\n1  Canada\n2     USA\n3  Mexico\n\ntypeof(x[1])\n\n[1] \"list\"\n\nx[[1]]\n\n[1] \"Canada\" \"USA\"    \"Mexico\"\n\ntypeof(x[[1]])\n\n[1] \"character\"\n\n\nIndexing dataframes can also be done using the column names:\n\nx$country\n\n[1] \"Canada\" \"USA\"    \"Mexico\"\n\nidentical(x[[1]], x$country)\n\n[1] TRUE"
  },
  {
    "objectID": "r_intro/packages.html",
    "href": "r_intro/packages.html",
    "title": "Packages",
    "section": "",
    "text": "Packages are a set of functions, constants, and/or data developed by the community that add functionality to R.\nIn this section, we look at where to find packages and how to install them."
  },
  {
    "objectID": "r_intro/packages.html#looking-for-packages",
    "href": "r_intro/packages.html#looking-for-packages",
    "title": "Packages",
    "section": "Looking for packages",
    "text": "Looking for packages\n\nPackage finder\nYour peers and the literature"
  },
  {
    "objectID": "r_intro/packages.html#package-documentation",
    "href": "r_intro/packages.html#package-documentation",
    "title": "Packages",
    "section": "Package documentation",
    "text": "Package documentation\n\nList of CRAN packages\nPackage documentation"
  },
  {
    "objectID": "r_intro/packages.html#managing-r-packages",
    "href": "r_intro/packages.html#managing-r-packages",
    "title": "Packages",
    "section": "Managing R packages",
    "text": "Managing R packages\nR packages can be installed, updated, and removed from within R:\ninstall.packages(\"&lt;package_name&gt;\", repos=\"&lt;url-cran-mirror&gt;\")\nremove.packages(\"&lt;package-name&gt;\")\nupdate_packages()\n\nrepos argument: chose a CRAN mirror close to the location of your cluster or use https://cloud.r-project.org/.\n\n\nThe first time you install a package, R will ask you whether you want to create a personal library in your home directory. Answer yes to both questions. Your packages will now install under ~/.\n\n\nSome packages require additional modules to be loaded before they can be installed. Other packages need additional R packages as dependencies. In either case, you will get explicit error messages. Adding the argument dependencies = T helps in the second case, but you will still have to add packages manually from time to time."
  },
  {
    "objectID": "r_intro/packages.html#loading-packages",
    "href": "r_intro/packages.html#loading-packages",
    "title": "Packages",
    "section": "Loading packages",
    "text": "Loading packages\nTo make a package available in an R session, you load it with the library() function.\n\nExample:\n\nlibrary(readxl)\nAlternatively, you can access a function from a package without loading it with the syntax: package::function().\n\nExample:\n\nreadxl::read_excel(\"file.xlsx\")"
  },
  {
    "objectID": "r_intro/plotting.html",
    "href": "r_intro/plotting.html",
    "title": "Plotting",
    "section": "",
    "text": "This section focuses on plotting in R with the package ggplot2 from the tidyverse."
  },
  {
    "objectID": "r_intro/plotting.html#the-data",
    "href": "r_intro/plotting.html#the-data",
    "title": "Plotting",
    "section": "The data",
    "text": "The data\nR comes with a number of datasets. You can get a list by running data(). The ggplot2 package provides additional ones. We will use the mpg dataset from ggplot2.\nTo access the data, let’s load the package:\n\nlibrary(ggplot2)\n\nHere is what that dataset looks like:\n\nmpg\n\n# A tibble: 234 × 11\n   manufacturer model      displ  year   cyl trans      drv     cty   hwy fl   \n   &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;\n 1 audi         a4           1.8  1999     4 auto(l5)   f        18    29 p    \n 2 audi         a4           1.8  1999     4 manual(m5) f        21    29 p    \n 3 audi         a4           2    2008     4 manual(m6) f        20    31 p    \n 4 audi         a4           2    2008     4 auto(av)   f        21    30 p    \n 5 audi         a4           2.8  1999     6 auto(l5)   f        16    26 p    \n 6 audi         a4           2.8  1999     6 manual(m5) f        18    26 p    \n 7 audi         a4           3.1  2008     6 auto(av)   f        18    27 p    \n 8 audi         a4 quattro   1.8  1999     4 manual(m5) 4        18    26 p    \n 9 audi         a4 quattro   1.8  1999     4 auto(l5)   4        16    25 p    \n10 audi         a4 quattro   2    2008     4 manual(m6) 4        20    28 p    \n   class  \n   &lt;chr&gt;  \n 1 compact\n 2 compact\n 3 compact\n 4 compact\n 5 compact\n 6 compact\n 7 compact\n 8 compact\n 9 compact\n10 compact\n# ℹ 224 more rows\n\n\n?mpg will give you information on the variables. In particular:\n\ndispl contains data on engine displacement (a measure of engine size and thus power) in litres (L).\nhwy contains data on fuel economy while driving on highways in miles per gallon (mpg).\ndrv represents the type of drive train (front-wheel drive, rear wheel drive, 4WD).\nclass represents the type of car.\n\nWe are interested in the relationship between engine size and fuel economy and see how the type of drive train and/or the type of car might affect this relationship."
  },
  {
    "objectID": "r_intro/plotting.html#base-r-plotting",
    "href": "r_intro/plotting.html#base-r-plotting",
    "title": "Plotting",
    "section": "Base R plotting",
    "text": "Base R plotting\nR contains built-in plotting capability thanks to the plot() function.\nA basic version of our plot would be:\n\nplot(\n  mpg$displ,\n  mpg$hwy,\n  main = \"Fuel consumption per engine size on highways\",\n  xlab = \"Engine size (L)\",\n  ylab = \"Fuel economy (mpg) on highways\"\n)"
  },
  {
    "objectID": "r_intro/plotting.html#grammar-of-graphics",
    "href": "r_intro/plotting.html#grammar-of-graphics",
    "title": "Plotting",
    "section": "Grammar of graphics",
    "text": "Grammar of graphics\nLeland Wilkinson developed the concept of grammar of graphics in his 2005 book The Grammar of Graphics. By breaking down statistical graphs into components following a set of rules, any plot can be described and constructed in a rigorous fashion.\nThis was further refined by Hadley Wickham in his 2010 article A Layered Grammar of Graphics and implemented in the package ggplot2 (that’s what the 2 “g” stand for in “ggplot”).\nggplot2 has become the dominant graphing package in R. Let’s see how to construct a plot with this package."
  },
  {
    "objectID": "r_intro/plotting.html#plotting-with-ggplot2",
    "href": "r_intro/plotting.html#plotting-with-ggplot2",
    "title": "Plotting",
    "section": "Plotting with ggplot2",
    "text": "Plotting with ggplot2\n\nYou can find the ggplot2 cheatsheet here.\n\n\nThe Canvas\nThe first component is the data:\n\nggplot(data = mpg)\n\n\n\n\n\nThis can be simplified into ggplot(mpg).\n\nThe second component sets the way variables are mapped on the axes. This is done with the aes() (aesthetics) function:\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy))\n\n\n\n\n\nThis can be simplified into ggplot(mpg, aes(x = displ, y = hwy)).\n\n\n\nGeometric representations of the data\nOnto this canvas, we can add “geoms” (geometrical objects) representing the data. The type of “geom” defines the type of representation (e.g. boxplot, histogram, bar chart).\nTo represent the data as a scatterplot, we use the geom_point() function:\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point()\n\n\n\n\nWe can colour-code the points in the scatterplot based on the drv variable, showing the lower fuel efficiency of 4WD vehicles:\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = drv))\n\n\n\n\nOr we can colour-code them based on the class variable:\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class))\n\n\n\n\nMultiple “geoms” can be added on top of each other. For instance, we can add a smoothed conditional means function that aids at seeing patterns in the data with geom_smooth():\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nThanks to the colour-coding of the types of car, we can see that the cluster of points in the top right corner all belong to the same type: 2 seaters. Those are outliers with high power, yet high few efficiency due to their smaller size.\nThe default smoothing function uses the LOESS (locally estimated scatterplot smoothing) method, which is a nonlinear regression. But maybe a linear model would actually show the general trend better. We can change the method by passing it as an argument to geom_smooth():\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  geom_smooth(method = lm)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nOf course, we could apply the smoothing function to each class instead of the entire data. It creates a busy plot but shows that the downward trend remains true within each type of car:\n\nggplot(mpg, aes(x = displ, y = hwy, color = class)) +\n  geom_point(aes(color = class)) +\n  geom_smooth(method = lm)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nOther arguments to geom_smooth() can set the line width, color, or whether or not the standard error (se) is shown:\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  geom_smooth(\n    method = lm,\n    se = FALSE,\n    color = \"#999999\",\n    linewidth = 0.5\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nColour scales\nIf we want to change the colour scale, we add another layer for this:\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  scale_color_brewer(palette = \"Dark2\") +\n  geom_smooth(\n    method = lm,\n    se = FALSE,\n    color = \"#999999\",\n    linewidth = 0.5\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nscale_color_brewer(), based on color brewer 2.0, is one of many methods to change the color scale. Here is the list of available scales for this particular method:\n\n\n\nLabels\nWe can keep on adding layers. For instance, the labs() function allows to set title, subtitle, captions, tags, axes labels, etc.\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  scale_color_brewer(palette = \"Dark2\") +\n  geom_smooth(\n    method = lm,\n    se = FALSE,\n    color = \"#999999\",\n    linewidth = 0.5\n  ) +\n  labs(\n    title = \"Fuel consumption per engine size on highways\",\n    x = \"Engine size (L)\",\n    y = \"Fuel economy (mpg) on highways\",\n    color = \"Type of car\",\n    caption = \"EPA data from https://fueleconomy.gov/\"\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nThemes\nAnother optional layer sets one of several preset themes.\nEdward Tufte developed, amongst others, the principle of data-ink ratio which emphasizes that ink should be used primarily where it communicates meaningful messages. It is indeed common to see charts where more ink is used in labels or background than in the actual representation of the data.\nThe default ggplot2 theme could be criticized as not following this principle. Let’s change it:\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  scale_color_brewer(palette = \"Dark2\") +\n  geom_smooth(\n    method = lm,\n    se = FALSE,\n    color = \"#999999\",\n    linewidth = 0.5\n  ) +\n  labs(\n    title = \"Fuel consumption per engine size on highways\",\n    x = \"Engine size (L)\",\n    y = \"Fuel economy (mpg) on highways\",\n    color = \"Type of car\",\n    caption = \"EPA data from https://fueleconomy.gov/\"\n  ) +\n  theme_classic()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThe theme() function allows to tweak the theme in any number of ways. For instance, what if we don’t like the default position of the title and we’d rather have it centered?\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  scale_color_brewer(palette = \"Dark2\") +\n  geom_smooth(\n    method = lm,\n    se = FALSE,\n    color = \"#999999\",\n    linewidth = 0.5\n  ) +\n  labs(\n    title = \"Fuel consumption per engine size on highways\",\n    x = \"Engine size (L)\",\n    y = \"Fuel economy (mpg) on highways\",\n    color = \"Type of car\",\n    caption = \"EPA data from https://fueleconomy.gov/\"\n  ) +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nMany things can be changed thanks to the theme() function. For instance, we can move the legend to give more space to the actual graph:\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  scale_color_brewer(palette = \"Dark2\") +\n  geom_smooth(\n    method = lm,\n    se = FALSE,\n    color = \"#999999\",\n    linewidth = 0.5\n  ) +\n  labs(\n    title = \"Fuel consumption per engine size on highways\",\n    x = \"Engine size (L)\",\n    y = \"Fuel economy (mpg) on highways\",\n    color = \"Type of car\",\n    caption = \"EPA data from https://fueleconomy.gov/\"\n  ) +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5), legend.position = \"bottom\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nAs you could see, ggplot2 works by adding a number of layers on top of each other, all following a standard set of rules, or “grammar”. This way, a vast array of graphs can be created by organizing simple components."
  },
  {
    "objectID": "r_intro/plotting.html#ggplot2-extensions",
    "href": "r_intro/plotting.html#ggplot2-extensions",
    "title": "Plotting",
    "section": "ggplot2 extensions",
    "text": "ggplot2 extensions\nThanks to its vast popularity, ggplot2 has seen a proliferation of packages extending its capabilities.\n\nCombining plots\nFor instance the patchwork package allows to easily combine multiple plots on the same frame.\nLet’s add a second plot next to our plot. To add plots side by side, we simply add them to each other. We also make a few changes to the labels to improve the plots integration:\n\nlibrary(patchwork)\n\nggplot(mpg, aes(x = displ, y = hwy)) +        # First plot\n  geom_point(aes(color = class)) +\n  scale_color_brewer(palette = \"Dark2\") +\n  geom_smooth(\n    method = lm,\n    se = FALSE,\n    color = \"#999999\",\n    linewidth = 0.5\n  ) +\n  labs(\n    x = \"Engine size (L)\",\n    y = \"Fuel economy (mpg) on highways\",\n    color = \"Type of car\"\n  ) +\n  theme_classic() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    legend.position = c(0.7, 0.75),           # Better legend position\n    legend.background = element_rect(         # Add a frame to the legend\n      linewidth=0.1,\n      linetype=\"solid\",\n      colour = \"black\"\n    )\n  ) +\n  ggplot(mpg, aes(x = displ, y = hwy)) +      # Second plot\n  geom_point(aes(color = drv)) +\n  scale_color_brewer(palette = \"Dark2\") +\n  labs(\n    x = \"Engine size (L)\",\n    y = element_blank(),                      # Remove redundant label\n    color = \"Type of drive train\",\n    caption = \"EPA data from https://fueleconomy.gov/\"\n  ) +\n  theme_classic() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    legend.position = c(0.7, 0.87),\n    legend.background = element_rect(\n      linewidth=0.1,\n      linetype=\"solid\",\n      colour = \"black\"\n    )\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nExtensions list\nAnother popular extension is the gganimate package which allows to create data animations.\nA full list of extensions for ggplot2 is shown below (here is the website):"
  },
  {
    "objectID": "r_intro/publishing.html",
    "href": "r_intro/publishing.html",
    "title": "Publishing",
    "section": "",
    "text": "You might have heard of R Markdown: a way to intertwine code and prose in a single scientific document. The company behind R Markdown has now developed its successor: Quarto.\n\nQuarto allows the creation of webpages, websites, presentations, books, pdf, etc. from code in R, Python, or Julia and markdown text.\nIf you are interested in an introduction to this tool, you can have a look at our workshop or our webinar on Quarto."
  },
  {
    "objectID": "r_intro/resources.html",
    "href": "r_intro/resources.html",
    "title": "Resources",
    "section": "",
    "text": "The R community is dynamic and offers a lot of online resources, from IDEs to Q&A, to workshops, books, or publications.\nThis section provides a selection of useful sites.\n\n\nMain sites\n\nR website\nComprehensive R Archive Network (CRAN): R versions and packages\n\n\n\nPosit and RStudio IDE\n\nPosit website (Posit was formerly called RStudio Inc.)\nPosit cheatsheets\n\n\n\nForums and Q&A\n\nStack Overflow [r] tag wiki\nStack Overflow [r] tag questions\nPosit Discourse\n\n\n\nDocumentation as pdf\n\nContributed documentation\nIntro books\n\n\n\nSoftware Carpentry online workshops\n\nProgramming with R\nR for Reproducible Scientific Analysis\nData analysis using R in the digital humanities\n\n\n\nOnline books\n\nR for Data Science (heavily based on the tidyverse)\nR Packages (how to create packages)\nR Programming for Data Science\nMastering Software Development in R\n\n\n\nR research\n\nThe R Journal"
  },
  {
    "objectID": "r_intro/run_r_intro.html",
    "href": "r_intro/run_r_intro.html",
    "title": "Running R",
    "section": "",
    "text": "This section covers the various ways R can be run, then shows you how to access our temporary RStudio server for this course."
  },
  {
    "objectID": "r_intro/run_r_intro.html#running-r",
    "href": "r_intro/run_r_intro.html#running-r",
    "title": "Running R",
    "section": "Running R",
    "text": "Running R\nR being an interpreted language, it can be run non-interactively or interactively.\n\nRunning R non-interactively\nIf you write code in a text file (called a script), you can then execute it with:\nRscript my_script.R\n\nThe command to execute scripts is Rscript rather than R.\nBy convention, R scripts take the extension .R.\n\n\n\nRunning R interactively\nThere are several ways to run R interactively.\n\nDirectly in the console (the name for the R shell):\n\n\n\nIn Jupyter with the R kernel (IRkernel package).\nIn another IDE (e.g. in Emacs with ESS).\nIn the RStudio IDE.\n\nThe RStudio IDE is popular and this is what we will use today. RStudio can can be run locally, but for this course, we will use an RStudio server."
  },
  {
    "objectID": "r_intro/run_r_intro.html#accessing-our-rstudio-server",
    "href": "r_intro/run_r_intro.html#accessing-our-rstudio-server",
    "title": "Running R",
    "section": "Accessing our RStudio server",
    "text": "Accessing our RStudio server\nYou do not need to install anything on your machine for this course as we will provide access to a temporary RStudio server.\n\nA username, a password, and the URL of the RStudio server will be given to you during the workshop.\n\nSign in using the username and password you will be given while ignoring the OTP entry. This will take you to the server options page of a JupyterHub.\n\nSelect the following server options:\n\nTime: 4 hours\nNumber of cores: 1\nMemory: 3700 MB\nUser interface: JupyterLab\n\n\nThen press “Start” to launch the JupyterHub. There, click on the “RStudio” button and the RStudio server will open in a new tab.\n\nNote that this temporary cluster will only be available for the duration of this course."
  },
  {
    "objectID": "r_intro/run_r_intro.html#using-rstudio",
    "href": "r_intro/run_r_intro.html#using-rstudio",
    "title": "Running R",
    "section": "Using RStudio",
    "text": "Using RStudio\nFor those unfamiliar with the RStudio IDE, you can download the following cheatsheet:\n\n\n\n\n\n\n\nfrom Posit Cheatsheets"
  },
  {
    "objectID": "r_intro/tidyverse.html",
    "href": "r_intro/tidyverse.html",
    "title": "Introduction to the tidyverse",
    "section": "",
    "text": "The tidyverse is a set of packages which attempts to make R more consistent and more similar to programming languages which were developed by computer scientists rather than statisticians.\nYou can think of it as a more modern version of R."
  },
  {
    "objectID": "r_intro/tidyverse.html#base-r-or-tidyverse",
    "href": "r_intro/tidyverse.html#base-r-or-tidyverse",
    "title": "Introduction to the tidyverse",
    "section": "Base R or tidyverse?",
    "text": "Base R or tidyverse?\n“Base R” refers to the use of the standard R library. The expression is often used in contrast to the tidyverse.\nThere are a many things that you can do with either base R or the tidyverse. Because the syntaxes are quite different, it almost feels like using two different languages and people tend to favour one or the other.\nWhich one you should use is really up to you.\n\n\n\n\n\n\n\nBase R\nTidyverse\n\n\n\n\nPreferred by old-schoolers\nIncreasingly becoming the norm with newer R users\n\n\nMore stable\nMore consistent syntax and behaviour\n\n\nDoesn’t require installing and loading packages\nMore and more resources and documentation available\n\n\n\nIn truth, even though the tidyverse has many detractors amongst old R users, it is increasingly becoming the norm."
  },
  {
    "objectID": "r_intro/tidyverse.html#a-glimpse-of-the-tidyverse",
    "href": "r_intro/tidyverse.html#a-glimpse-of-the-tidyverse",
    "title": "Introduction to the tidyverse",
    "section": "A glimpse of the tidyverse",
    "text": "A glimpse of the tidyverse\nThe best introduction to the tidyverse is probably the book R for Data Science by Hadley Wickham and Garrett Grolemund.\nPosit (the company formerly known as RStudio Inc. behind the tidyverse) developed a series of useful cheatsheets. Below are links to the ones you are the most likely to use as you get started with R.\n\nData import\nThe first thing you often need to do is to import your data into R. This is done with readr.\n\n\n\n\n\n\n\nfrom Posit Cheatsheets\n\n\n\nData transformation\nYou then often need to transformation your data into the right format. This is done with the packages dplyr and tidyr.\n\n\n\n\n\n\n\nfrom Posit Cheatsheets\n\n\n\n\n\n\n\n\nfrom Posit Cheatsheets\n\n\n\nVisualization\nVisualization in the tidyverse is done with the ggplot2 package which we will explore in the next section.\n\n\n\n\n\n\n\nfrom Posit Cheatsheets\n\n\n\nWorking with factors\nThe package forcats offers the tidyverse approach to working with factors.\n\n\n\n\n\n\nfrom Posit Cheatsheets\n\n\n\nWorking with strings\nstringr is for strings.\n\n\n\n\n\n\n\nfrom Posit Cheatsheets\n\n\n\nWorking with dates\nlubridate will help you deal with dates.\n\n\n\n\n\n\n\nfrom Posit Cheatsheets\n\n\n\nFunctional programming\nFinally, purrr is the tidyverse equivalent to the apply functions in base R: a way to run functions on functions.\n\n\n\n\n\n\n\nfrom Posit Cheatsheets"
  },
  {
    "objectID": "r_intro/why.html",
    "href": "r_intro/why.html",
    "title": "R: why and for whom?",
    "section": "",
    "text": "There are other high level programming languages such as Python or Julia, so when might it make sense for you to turn to R?"
  },
  {
    "objectID": "r_intro/why.html#why-r",
    "href": "r_intro/why.html#why-r",
    "title": "R: why and for whom?",
    "section": "Why R?",
    "text": "Why R?\nHere are a number of reasons why you might want to consider using R:\n\nFree and open source\nUnequalled number of statistics and modelling packages\nIntegrated package manager\nEasy connection with fast compiled languages such as C and C++\nPowerful IDEs available (RStudio, Emacs ESS)"
  },
  {
    "objectID": "r_intro/why.html#for-whom",
    "href": "r_intro/why.html#for-whom",
    "title": "R: why and for whom?",
    "section": "For whom?",
    "text": "For whom?\nFor whom is R particularly well suited?\n\nFields with heavy statistics, modelling, or Bayesian analysis such as biology, linguistics, economics, or statistics\nData science using a lot of tabular data"
  },
  {
    "objectID": "r_intro/why.html#downsides-of-r",
    "href": "r_intro/why.html#downsides-of-r",
    "title": "R: why and for whom?",
    "section": "Downsides of R",
    "text": "Downsides of R\nOf course, R also has its downsides:\n\nInconsistent syntax full of quirks\nSlow\nLarge memory usage"
  },
  {
    "objectID": "r_parallel/clusters.html",
    "href": "r_parallel/clusters.html",
    "title": "R on HPC clusters",
    "section": "",
    "text": "This section will show you how to use R once you have logged in to a remote cluster via SSH."
  },
  {
    "objectID": "r_parallel/clusters.html#modules",
    "href": "r_parallel/clusters.html#modules",
    "title": "R on HPC clusters",
    "section": "Modules",
    "text": "Modules\nOn the Alliance clusters, a number of utilities are available right away (e.g. Bash utilities, git, tmux, various text editors). Before you can use more specialized software however, you have to load the module corresponding to the version of your choice as well as any potential dependencies.\n\nThe cluster setup for this course has everything loaded, so this step is not necessary today, but it is very important to learn it.\n\n\nR\nFirst, of course, we need an R module.\nTo see which versions of R are available on a cluster, run:\nmodule spider r\nTo see the dependencies of a particular version (e.g. r/4.1.2), run:\nmodule spider r/4.1.2\nThis shows us that we need StdEnv/2020 to load r/4.1.2.\n\n\nC compiler\nIf you plan on installing any R package, you will also need a C compiler.\nIn theory, one could use the proprietary Intel compiler which is loaded by default on the Alliance clusters, but it is recommended to replace it with the GCC compiler (R packages can be compiled by any C compiler—also including Clang and LLVM—but the default GCC compiler is the best way to avoid headaches).\n\n\nYour turn:\n\n\nHow can you check which gcc versions are available on our training cluster?\nWhat are the dependencies required by gcc/9.3.0?\n\n\n\n\nLoading the modules\nOnce you know which modules you need, you can load them. The order is important: the dependencies (here StdEnv/2020) must be listed before the modules which depend on them.\nmodule load StdEnv/2020 gcc/9.3.0 r/4.1.2"
  },
  {
    "objectID": "r_parallel/clusters.html#installing-r-packages",
    "href": "r_parallel/clusters.html#installing-r-packages",
    "title": "R on HPC clusters",
    "section": "Installing R packages",
    "text": "Installing R packages\n\nFor this course, all packages have already been installed in a communal library. You thus don’t have to install anything.\n\nTo install a package, launch the interactive R console with:\nR\nIn the R console, run:\ninstall.packages(\"&lt;package_name&gt;\", repos=\"&lt;url-cran-mirror&gt;\")\n\nrepos argument: chose a CRAN mirror close to the location of your cluster or use https://cloud.r-project.org/.\n\n\nThe first time you install a package, R will ask you whether you want to create a personal library in your home directory. Answer yes to both questions. Your packages will now install under ~/.\n\n\nSome packages require additional modules to be loaded before they can be installed. Other packages need additional R packages as dependencies. In either case, you will get explicit error messages. Adding the argument dependencies = T helps in the second case, but you will still have to add packages manually from time to time.\n\nTo leave the R console, press &lt;Ctrl+D&gt;."
  },
  {
    "objectID": "r_parallel/clusters.html#running-r-jobs",
    "href": "r_parallel/clusters.html#running-r-jobs",
    "title": "R on HPC clusters",
    "section": "Running R jobs",
    "text": "Running R jobs\nThere are two types of jobs that can be launched on an Alliance cluster: interactive jobs and batch jobs. We will practice both and discuss their respective merits and when to use which.\nFor this course, I purposefully built a rather small cluster (10 nodes with 4 CPUs and 30GB each) to give a tangible illustration of the constraints of resource sharing.\n\nInteractive jobs\n\nWhile it is fine to run R on the login node when you install packages, you must start a SLURM job before any heavy computation.\n\nTo run R interactively, you should launch an salloc session.\n\nExample:\n\nsalloc --time=1:10:00 --mem-per-cpu=3700M --ntasks=8\nThis takes you to a compute node where you can now launch R to run computations:\nR\n\nThis however leads to the same inefficient use of resources as happens when running an RStudio server: all the resources that you requested are blocked for you while your job is running, whether you are making use of them (running heavy computations) or not (thinking, typing code, running computations that use only a fraction of the requested resources).\nInteractive jobs are thus best kept to develop code.\n\n\n\nScripts\nTo run an R script called &lt;your_script&gt;.R, you first need to write a job script:\n\nExample:\n\n\n&lt;your_job&gt;.sh\n\n#!/bin/bash\n#SBATCH --account=def-&lt;your_account&gt;\n#SBATCH --time=15\n#SBATCH --mem-per-cpu=3000M\n#SBATCH --cpus-per-task=4\n#SBATCH --job-name=\"&lt;your_job&gt;\"\nmodule load StdEnv/2020 gcc/11.3.0 r/4.1.2\nRscript &lt;your_script&gt;.R   # Note that R scripts are run with the command `Rscript`\n\n\nThen launch your job with:\nsbatch &lt;your_job&gt;.sh\nYou can monitor your job with sq (an alias for squeue -u $USER $@).\n\nBatch jobs are the best approach to run parallel computations, particularly when they require a lot of hardware.\nIt will save you lots of waiting time (Alliance clusters) or money (commercial clusters)."
  },
  {
    "objectID": "r_parallel/data.html",
    "href": "r_parallel/data.html",
    "title": "Data on HPC clusters",
    "section": "",
    "text": "So far, we have played with randomly created data. In your work, you will often need to work with real world data.\nHow do you move it to the cluster? Where should you store it?\nIt’s time to talk about data on HPC clusters."
  },
  {
    "objectID": "r_parallel/data.html#transferring-data-tofrom-the-cluster",
    "href": "r_parallel/data.html#transferring-data-tofrom-the-cluster",
    "title": "Data on HPC clusters",
    "section": "Transferring data to/from the cluster",
    "text": "Transferring data to/from the cluster\n\nSecure Copy Protocol\nSecure Copy Protocol (SCP) allows to copy files over the Secure Shell Protocol (SSH) with the scp utility. scp follows a syntax similar to that of the cp command.\nNote that you need to run it from your local machines (not from the cluster).\n\nCopy from your machine to the cluster\n# Copy a local file to your home directory on the cluster\nscp /local/path/file username@hostname:\n# Copy a local file to some path on the cluster\nscp /local/path/file username@hostname:/remote/path\n\n\nCopy from the cluster to your machine\n# Copy a file from the cluster to some path on your machine\nscp username@hostname:/remote/path/file /local/path\n# Copy a file from the cluster to your current location on your machine\nscp username@hostname:/remote/path/file .\nYou can also use wildcards to transfer multiple files:\n# Copy all the Bash scripts from your cluster home dir to some local path\nscp username@hostname:*.sh /local/path\n\n\nCopying directories\nTo copy a directory, you need to add the -r (recursive) flag:\nscp -r /local/path/folder username@hostname:/remote/path\n\n\nCopying for Windows users\nMobaXterm users (on Windows) can copy files by dragging them between the local and remote machines in the GUI. Alternatively, they can use the download and upload buttons.\n\n\n\nSecure File Transfer Protocol\nThe Secure File Transfer Protocol (SFTP) is more sophisticated and allows additional operations in an interactive shell. The sftp command provided by OpenSSH and other packages launches an SFTP client:\nsftp username@hostname\n\nLook at your prompt: your usual Bash/Zsh prompt has been replaced with sftp&gt;.\n\nFrom this prompt, you can access a number of SFTP commands. Type help for a list:\nsftp&gt; help\nAvailable commands:\nbye                                Quit sftp\ncd path                            Change remote directory to 'path'\nchgrp [-h] grp path                Change group of file 'path' to 'grp'\nchmod [-h] mode path               Change permissions of file 'path' to 'mode'\nchown [-h] own path                Change owner of file 'path' to 'own'\ncopy oldpath newpath               Copy remote file\ncp oldpath newpath                 Copy remote file\ndf [-hi] [path]                    Display statistics for current directory or\n                                   filesystem containing 'path'\nexit                               Quit sftp\nget [-afpR] remote [local]         Download file\nhelp                               Display this help text\nlcd path                           Change local directory to 'path'\nlls [ls-options [path]]            Display local directory listing\nlmkdir path                        Create local directory\nln [-s] oldpath newpath            Link remote file (-s for symlink)\nlpwd                               Print local working directory\nls [-1afhlnrSt] [path]             Display remote directory listing\nlumask umask                       Set local umask to 'umask'\nmkdir path                         Create remote directory\nprogress                           Toggle display of progress meter\nput [-afpR] local [remote]         Upload file\npwd                                Display remote working directory\nquit                               Quit sftp\nreget [-fpR] remote [local]        Resume download file\nrename oldpath newpath             Rename remote file\nreput [-fpR] local [remote]        Resume upload file\nrm path                            Delete remote file\nrmdir path                         Remove remote directory\nsymlink oldpath newpath            Symlink remote file\nversion                            Show SFTP version\n!command                           Execute 'command' in local shell\n!                                  Escape to local shell\n?                                  Synonym for help\nAs this list shows, you have access to a number of classic Unix command such as cd, pwd, ls, etc. These commands will be executed on the remote machine.\nIn addition, there are a number of commands of the form l&lt;command&gt;. “l” stands for “local”.\nThese commands will be executed on your local machine.\nFor instance, ls will list the files in your current directory in the remote machine while lls (“local ls”) will list the files in your current directory on your computer.\nThis means that you are now able to navigate two file systems at once: your local machine and the remote machine.\n\nHere are a few examples:\n\nsftp&gt; pwd              # print remote working directory\nsftp&gt; lpwd             # print local working directory\nsftp&gt; ls               # list files in remote working directory\nsftp&gt; lls              # list files in local working directory\nsftp&gt; cd               # change the remote directory\nsftp&gt; lcd              # change the local directory\nsftp&gt; put local_file   # upload a file\nsftp&gt; get remote_file  # download a file\n\nCopying directories\nTo upload/download directories, you first need to create them in the destination, then copy the content with the -r (recursive) flag.\n\nIf you have a local directory called dir and you want to copy it to the cluster you need to run:\n\nsftp&gt; mkdir dir    # First create the directory\nsftp&gt; put -r dir   # Then copy the content\nTo terminate the session, press &lt;Ctrl+D&gt;.\n\n\n\nSyncing\nIf, instead of an occasional copying of files between your machine and the cluster, you want to keep a directory in sync between both machines, you might want to use rsync instead. You can look at the Alliance wiki page on rsync for complete instructions.\n\n\nHeavy transfers\nWhile the methods covered above work very well for limited amounts of data, if you need to make large transfers, you should use globus instead, following the instructions in the Alliance wiki page on this service.\n\n\nWindows line endings\nOn modern Mac operating systems and on Linux, lines in files are terminated with a newline (\\n). On Windows, they are terminated with a carriage return + newline (\\r\\n).\nWhen you transfer files between Windows and Linux (the cluster uses Linux), this creates a mismatch. Most modern software handle this correctly, but you may occasionally run into problems.\nThe solution is to convert a file from Windows encoding to Unix encoding with:\ndos2unix file\nTo convert a file back to Windows encoding, run:\nunix2dos file"
  },
  {
    "objectID": "r_parallel/data.html#files-management",
    "href": "r_parallel/data.html#files-management",
    "title": "Data on HPC clusters",
    "section": "Files management",
    "text": "Files management\nThe Alliance clusters are designed to handle large files very well. They are however slowed by the presence of many small files. It is thus important to know how to handle large collections of files by archiving them with tools such as tar and dar."
  },
  {
    "objectID": "r_parallel/data.html#where-to-store-data",
    "href": "r_parallel/data.html#where-to-store-data",
    "title": "Data on HPC clusters",
    "section": "Where to store data",
    "text": "Where to store data\nSupercomputers have several filesystems and you should familiarize yourself with the quotas and policies of the clusters you use.\nAll filesystems are mounted on all nodes so that you can access the data on any network storage from any node (e.g. something in /home, /project, or /scratch will be accessible from any login node or compute node).\nA temporary folder gets created directly on the compute nodes while a job is running. In situations with heavy I/O or involving many files, it is worth considering copying data to it as part of the job. In that case, make sure to copy the results back to network storage before the end of the job."
  },
  {
    "objectID": "r_parallel/index.html",
    "href": "r_parallel/index.html",
    "title": "Parallel R",
    "section": "",
    "text": "Date:\nTuesday, June 20, 2023\nTime:\n2pm–5pm\nInstructor:\nMarie-Hélène Burle (Simon Fraser University)\nPrerequisites:\nBasic knowledge of R and some basic working knowledge of HPC (how to submit Slurm jobs and view their output).\nSoftware:\nWe will provide access to one of our Linux systems. To make use of it, attendees will need a remote secure shell (SSH) client installed on their computer. On Windows we recommend the free Home Edition of MobaXterm. On Mac and Linux computers, SSH is usually pre-installed (try typing ssh in a terminal to make sure it is there).\n\n Start course ➤"
  },
  {
    "objectID": "r_parallel/memory.html",
    "href": "r_parallel/memory.html",
    "title": "Memory management",
    "section": "",
    "text": "Memory can be a limiting factor and releasing it when not needed can be critical to avoid out of memory states. On the other hand, memoisation is an optimization technique which stores the results of heavy computations for re-use at the cost of increasing memory usage.\nMemory and speed are thus linked in a trade-off."
  },
  {
    "objectID": "r_parallel/memory.html#releasing-memory",
    "href": "r_parallel/memory.html#releasing-memory",
    "title": "Memory management",
    "section": "Releasing memory",
    "text": "Releasing memory\nIt is best to avoid creating very large intermediate objects (e.g. with nested functions or functions chained with the magrittr pipe), but if you must, remove them from the global environment with rm() when you don’t need them anymore. Once all the pointers to an object in memory are deleted, the garbage collector will clear its value and release the memory it used.\nAnother way to release the memory used by heavy intermediate objects is with functions: if you create those objects in the local environment of a function (instead of directly in the global environment), they will be cleared from memory as soon as the function has finished running.\nNote that in the case of a very large function, it might still be beneficial to run rm() inside the function to clear the memory for other processes coming next within that function. But this is a pretty rare case."
  },
  {
    "objectID": "r_parallel/memory.html#caching-in-memory",
    "href": "r_parallel/memory.html#caching-in-memory",
    "title": "Memory management",
    "section": "Caching in memory",
    "text": "Caching in memory\nMemoisation is a technique by which the results of heavy computations are stored in memory to avoid have to re-calculate them. This can be convenient in a variety of settings (e.g. to reduce calls to an API), but mostly, it can greatly improve the efficiency of some code such as recursive function calls.\nLet’s consider the calculation of the Fibonacci numbers as an example. Those numbers form a sequence starting with 0 and 11, after which each number is the sum of the previous two.1 Alternative versions have the sequence start with 1, 1 or with 1, 2.\nHere is a function that would return the nth Fibonacci number2:2 There are more efficient ways to calculate the Fibonacci numbers, but this inefficient function is a great example to show the advantage of memoisation.\nfib &lt;- function(n) {\n  if(n == 0) {\n    return(0)\n  } else if(n == 1) {\n    return(1)\n  } else {\n    Recall(n - 1) + Recall(n - 2)\n  }\n}\nIt can be written more tersely as:\n\nfib &lt;- function(n) {\n  if(n == 0) return(0)\n  if(n == 1) return(1)\n  Recall(n - 1) + Recall(n - 2)\n}\n\n\nRecall() is a placeholder for the name of the recursive function. We could have used fib() instead, but Recall() is more robust as it allows for function renaming.\n\nMemoisation is very useful here because, for each Fibonacci number, we need to calculate the two preceding Fibonacci numbers and to calculate each of those we need to calculate the two Fibonacci numbers preceding that one and to calculate… etc. That is a large number of calculations, but, thanks to caching, we don’t have to calculate any one of them more than once.\nThe packages R.cache and memoise both allow for memoisation with an incredibly simple syntax.\nApplying the latter to our function gives us:\n\nlibrary(memoise)\n\nfibmem &lt;- memoise(\n  function(n) {\n    if(n == 0) return(0)\n    if(n == 1) return(1)\n    Recall(n - 1) + Recall(n - 2)\n  }\n)\n\nWe can do some benchmarking to see the speedup for the 30th Fibonacci number:\n\nlibrary(bench)\n\nn &lt;- 30\nmark(fib(n), fibmem(n))\n\nWarning: Some expressions had a GC in every iteration; so filtering is\ndisabled.\n\n\n# A tibble: 2 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 fib(n)        1.53s    1.53s     0.654      33KB     23.5\n2 fibmem(n)   37.04µs  38.81µs 23955.       68.4KB     16.8\n\n\nThe speedup is over 35,000!"
  },
  {
    "objectID": "r_parallel/optimizations.html",
    "href": "r_parallel/optimizations.html",
    "title": "Optimizations",
    "section": "",
    "text": "A lot of hardware is not the answer to poorly written code. Before considering parallelization, you should think about ways to optimize your code sequentially.\nWhy?\n\nNot all code can be parallelized.\nParallelization is costly (waiting time to access a cluster or money).\nThe optimization of the sequential code will also benefit the parallel code.\n\nIn many cases, writing better code will save you more computing time than parallelization.\nIn this section, we will cover several principles by playing with the programmatic implementation of the fizz buzz game."
  },
  {
    "objectID": "r_parallel/optimizations.html#toy-example",
    "href": "r_parallel/optimizations.html#toy-example",
    "title": "Optimizations",
    "section": "Toy example",
    "text": "Toy example\nFizz buzz is a children game to practice divisions. Players take turn counting out loud while replacing:\n\nany number divisible by 3 with the word “Fizz”,\nany number divisible by 5 with the word “Buzz”,\nany number divisible by both 3 and 5 with the word “FizzBuzz”.\n\nLet’s write functions to solve the game and time them to draw some general principles about more efficient code.\nWe will use bench::mark() to benchmark our solutions. Let’s load it:\n\nlibrary(bench)"
  },
  {
    "objectID": "r_parallel/optimizations.html#pre-allocate-memory",
    "href": "r_parallel/optimizations.html#pre-allocate-memory",
    "title": "Optimizations",
    "section": "Pre-allocate memory",
    "text": "Pre-allocate memory\nIn this first function, we create an empty object z of class integer and of length 0 that will hold the result of a loop, then we run the loop and at each iteration, we add a new value to z:\n\nf1 &lt;- function(n) {\n  z &lt;- integer()\n  for(i in 1:n) {\n    if(i %% 3 == 0 && i %% 5 == 0) {\n      z[i] &lt;- \"FizzBuzz\"\n    } else if(i %% 3 == 0) {\n      z[i] &lt;- \"Fizz\"\n    } else if(i %% 5 == 0) {\n      z[i] &lt;- \"Buzz\"\n    } else {\n      z[i] &lt;- i\n    }\n  }\n  z\n}\n\nThe second function is very similar, but this time, we create an empty object z of class integer and of length matching the final length z will have after running the loop. This means that we are pre-allocating memory for the full vector before we run the loop instead of growing the vector at each iteration:\n\nf2 &lt;- function(n) {\n  z &lt;- integer(n)\n  for(i in 1:n) {\n    if(i %% 3 == 0 && i %% 5 == 0) {\n      z[i] &lt;- \"FizzBuzz\"\n    } else if(i %% 3 == 0) {\n      z[i] &lt;- \"Fizz\"\n    } else if(i %% 5 == 0) {\n      z[i] &lt;- \"Buzz\"\n    } else {\n      z[i] &lt;- i\n    }\n  }\n  z\n}\n\nLet’s make sure that our functions work by testing it on a small number:\n\nf1(20)\n\n [1] \"1\"        \"2\"        \"Fizz\"     \"4\"        \"Buzz\"     \"Fizz\"    \n [7] \"7\"        \"8\"        \"Fizz\"     \"Buzz\"     \"11\"       \"Fizz\"    \n[13] \"13\"       \"14\"       \"FizzBuzz\" \"16\"       \"17\"       \"Fizz\"    \n[19] \"19\"       \"Buzz\"    \n\nf2(20)\n\n [1] \"1\"        \"2\"        \"Fizz\"     \"4\"        \"Buzz\"     \"Fizz\"    \n [7] \"7\"        \"8\"        \"Fizz\"     \"Buzz\"     \"11\"       \"Fizz\"    \n[13] \"13\"       \"14\"       \"FizzBuzz\" \"16\"       \"17\"       \"Fizz\"    \n[19] \"19\"       \"Buzz\"    \n\n\nNow, let’s benchmark them for a large number:\n\nn &lt;- 1e5\nmark(f1(n), f2(n))\n\nWarning: Some expressions had a GC in every iteration; so filtering is\ndisabled.\n\n\n# A tibble: 2 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 f1(n)         136ms    139ms      7.14   16.55MB     25.0\n2 f2(n)         122ms    125ms      7.99    1.15MB     28.0\n\n\nf2() is consistently faster. While in this example the difference is very slight, pre-allocating the object that will hold the result of a loop before running the loop can make a big difference.\nAlso, note the difference in memory allocation."
  },
  {
    "objectID": "r_parallel/optimizations.html#arent-loops-a-big-no-no-in-r",
    "href": "r_parallel/optimizations.html#arent-loops-a-big-no-no-in-r",
    "title": "Optimizations",
    "section": "Aren’t loops a big ‘no no’ in R?",
    "text": "Aren’t loops a big ‘no no’ in R?\nBy now, you might be thinking: “Wait… aren’t loops a big ‘no no’ in R? I’ve always been told that they are slow and that one should always use functional programming! We are talking about optimization in this course and we are using loops?!?”\nThere are a lot of misconceptions around R loops. They can be very slow if you don’t pre-allocate memory. Otherwise they are almost always faster than functions (the apply() family or the tidyverse equivalent of the purrr::map() family). You can choose to use a functional programming approach for style and readability, but not for speed.\nLet’s test it.\nFirst we create a function:\n\nfb &lt;- function(n) {\n  if(n %% 3 == 0 && n %% 5 == 0) {\n    \"FizzBuzz\"\n  } else if(n %% 3 == 0) {\n    \"Fizz\"\n  } else if(n %% 5 == 0) {\n    \"Buzz\"\n  } else {\n    n\n  }\n}\n\nThen we pass it through sapply(). We can test that it works on a small number:\n\nsapply(1:20, fb)\n\n [1] \"1\"        \"2\"        \"Fizz\"     \"4\"        \"Buzz\"     \"Fizz\"    \n [7] \"7\"        \"8\"        \"Fizz\"     \"Buzz\"     \"11\"       \"Fizz\"    \n[13] \"13\"       \"14\"       \"FizzBuzz\" \"16\"       \"17\"       \"Fizz\"    \n[19] \"19\"       \"Buzz\"    \n\n\nFinally, we compare the timing with that of f2():\n\nmark(f2(n), sapply(1:n, fb))\n\nWarning: Some expressions had a GC in every iteration; so filtering is\ndisabled.\n\n\n# A tibble: 2 × 6\n  expression           min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt;      &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 f2(n)              130ms    138ms      7.30    1.15MB     29.2\n2 sapply(1:n, fb)    171ms    178ms      5.46    3.29MB     23.7\n\n\nAs you can see, the loop is faster."
  },
  {
    "objectID": "r_parallel/optimizations.html#avoid-unnecessary-operations",
    "href": "r_parallel/optimizations.html#avoid-unnecessary-operations",
    "title": "Optimizations",
    "section": "Avoid unnecessary operations",
    "text": "Avoid unnecessary operations\n\nExample 1\nCalling z as the last command in our function is the same as calling return(z).\nFrom the R documentation:\n\nIf the end of a function is reached without calling return, the value of the last evaluated expression is returned.\n\nNow, what about using print() instead?\n\nf3 &lt;- function(n) {\n  z &lt;- integer(n)\n  for(i in 1:n) {\n    if(i %% 3 == 0 && i %% 5 == 0) {\n      z[i] &lt;- \"FizzBuzz\"\n    } else if(i %% 3 == 0) {\n      z[i] &lt;- \"Fizz\"\n    } else if(i %% 5 == 0) {\n      z[i] &lt;- \"Buzz\"\n    } else {\n      z[i] &lt;- i\n    }\n  }\n  print(z)\n}\n\nLet’s benchmark it against f2():\nmark(f2(n), f3(n))\n [1] \"1\"        \"2\"        \"Fizz\"     \"4\"        \"Buzz\"     \"Fizz\"    \n [7] \"7\"        \"8\"        \"Fizz\"     \"Buzz\"     \"11\"       \"Fizz\"    \n[13] \"13\"       \"14\"       \"FizzBuzz\" \"16\"       \"17\"       \"Fizz\"    \n[19] \"19\"       \"Buzz\"     \"Fizz\"     \"22\"       \"23\"       \"Fizz\"    \n[25] \"Buzz\"     \"26\"       \"Fizz\"     \"28\"       \"29\"       \"FizzBuzz\"\n[31] \"31\"       \"32\"       \"Fizz\"     \"34\"       \"Buzz\"     \"Fizz\"    \n[37] \"37\"       \"38\"       \"Fizz\"     \"Buzz\"     \"41\"       \"Fizz\"\n...\n\nWarning: Some expressions had a GC in every iteration; so filtering is\ndisabled.\n# A tibble: 2 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 f2(1e+05)  151.88ms 157.65ms     6.30     1.25MB     29.9\n2 f3(1e+05)     3.25s    3.25s     0.308    1.04GB     26.8\nWhat happened?\nprint() returns its argument, but it additionally prints it to the standard output. This is why the mark() function printed the output of f3() before printing the timings.\nAs you can see, printing takes a long time.\n\nThe code in this website is run by Quarto. Since, by default, RStudio will only print the first 1,000 results, the timing you will get for f3() in RStudio will be much less bad as it won’t include the time it takes to print the remaining 99,000 results.\n\nIf you are evaluating f2() on its own (e.g. f2(20)), the returned result will also be printed to standard output and both functions will be equivalent. However, if you are using the function in another context, printing becomes an unnecessary and timely operation and f3() would be a very bad option. f3() is thus not a good function.\nHere is an example in which f3() would perform a totally unnecessary operation that f2() avoids:\n\na &lt;- f2(20)\n\n\nNo unnecessary printing.\n\n\na &lt;- f3(20)\n\n [1] \"1\"        \"2\"        \"Fizz\"     \"4\"        \"Buzz\"     \"Fizz\"    \n [7] \"7\"        \"8\"        \"Fizz\"     \"Buzz\"     \"11\"       \"Fizz\"    \n[13] \"13\"       \"14\"       \"FizzBuzz\" \"16\"       \"17\"       \"Fizz\"    \n[19] \"19\"       \"Buzz\"    \n\n\n\nUnnecessary printing.\n\nFor 1e5, the difference in timing between running an unnecessary printing vs not is a factor of 21!\nEven worse would be to use:\n\nf4 &lt;- function(n) {\n  for(i in 1:n) {\n    if(i %% 3 == 0 && i %% 5 == 0) {\n      print(\"FizzBuzz\")\n    } else if(i %% 3 == 0) {\n      print(\"Fizz\")\n    } else if(i %% 5 == 0) {\n      print(\"Buzz\")\n    } else {\n      print(i)\n    }\n  }\n}\n\nHere the difference in timing is a factor of 50…\n\n\nExample 2\nOne modulo operation and equality test can be removed by replacing i %% 3 == 0 && i %% 5 == 0 by i %% 15 == 0. The difference isn’t huge, but there is a slight speedup:\n\nf5 &lt;- function(n) {\n  z &lt;- integer(n)\n  for(i in 1:n) {\n    if(i %% 15 == 0) {\n      z[i] &lt;- \"FizzBuzz\"\n    } else if(i %% 3 == 0) {\n      z[i] &lt;- \"Fizz\"\n    } else if(i %% 5 == 0) {\n      z[i] &lt;- \"Buzz\"\n    } else {\n      z[i] &lt;- i\n    }\n  }\n  z\n}\n\nmark(f2(n), f5(n))\n\nWarning: Some expressions had a GC in every iteration; so filtering is\ndisabled.\n\n\n# A tibble: 2 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 f2(n)         166ms    167ms      5.81    1.15MB     25.2\n2 f5(n)          99ms    101ms      9.92    1.22MB     33.7\n\n\n\n\nExample 3\nLouis Arsenault-Mahjoubi—who attended this workshop—found ways to get rid of several operations and get a speedup of 1.7 over f2().\nFirst, we can assign 1:n to z instead of pre-allocating memory with an empty vector, thus rendering the assignment of i to z[i] unnecessary in the last else statement:\n\nf_louis1 &lt;- function(n) {\n  z &lt;- 1:n\n  for(i in z) {\n    if(i %% 3 == 0 && i %% 5 == 0) {\n      z[i] &lt;- \"FizzBuzz\"\n    } else if(i %% 3 == 0) {\n      z[i] &lt;- \"Fizz\"\n    } else if(i %% 5 == 0) {\n      z[i] &lt;- \"Buzz\"\n    } \n  }\n  z\n}\n\nThis function works:\n\nf_louis1(20)\n\n [1] \"1\"        \"2\"        \"Fizz\"     \"4\"        \"Buzz\"     \"Fizz\"    \n [7] \"7\"        \"8\"        \"Fizz\"     \"Buzz\"     \"11\"       \"Fizz\"    \n[13] \"13\"       \"14\"       \"FizzBuzz\" \"16\"       \"17\"       \"Fizz\"    \n[19] \"19\"       \"Buzz\"    \n\n\n… and is faster (speedup of 1.3):\n\nmark(f5(n), f_louis1(n))\n\nWarning: Some expressions had a GC in every iteration; so filtering is\ndisabled.\n\n\n# A tibble: 2 × 6\n  expression       min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt;  &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 f5(n)          107ms    109ms      9.18    1.15MB     34.9\n2 f_louis1(n)     77ms     78ms     12.8     1.15MB     40.2\n\n\nThen, we can prevent the repetitions of the modulo operations and equality tests by saving them to variables:\n\nf_louis2 &lt;- function(n) {\n  z &lt;- 1:n\n  for(i in z) {\n    div3 &lt;- (i %% 3 == 0)\n    div5 &lt;- (i %% 5 == 0)\n    if(div3 && div5) {\n      z[i] &lt;- \"FizzBuzz\"\n    } else if(div3) {\n      z[i] &lt;- \"Fizz\"\n    } else if(div5) {\n      z[i] &lt;- \"Buzz\"\n    } \n  }\n  z\n}\n\nThis gets us an even greater speedup of 1.7:\n\nmark(f5(n), f_louis2(n))\n\nWarning: Some expressions had a GC in every iteration; so filtering is\ndisabled.\n\n\n# A tibble: 2 × 6\n  expression       min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt;  &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 f5(n)        106.4ms  110.9ms      9.09    1.15MB     34.5\n2 f_louis2(n)   63.3ms   64.5ms     15.4     1.21MB     36.7\n\n\nBut it gets even better: we can actually get rid of the for loop!\n\nf_louis3 &lt;- function(n) {\n  z &lt;- 1:n\n  div3 &lt;- (z %% 3 == 0)\n  div5 &lt;- (z %% 5 == 0)\n  z[which(div3)] &lt;- \"Fizz\"\n  z[which(div5)] &lt;- \"Buzz\"\n  z[which(div3 & div5)] &lt;- \"FizzBuzz\"\n  z\n}\n\nNow we get a speedup of 5.5 compared to our best f2 function:\n\nmark(f5(n), f_louis3(n))\n\nWarning: Some expressions had a GC in every iteration; so filtering is\ndisabled.\n\n\n# A tibble: 2 × 6\n  expression       min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt;  &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 f5(n)        100.1ms  110.1ms      8.65    1.15MB    31.1 \n2 f_louis3(n)   18.5ms   19.2ms     49.7     5.19MB     7.65\n\n\nYou can ensure that we still get the same result:\n\nf_louis3(20)\n\n [1] \"1\"        \"2\"        \"Fizz\"     \"4\"        \"Buzz\"     \"Fizz\"    \n [7] \"7\"        \"8\"        \"Fizz\"     \"Buzz\"     \"11\"       \"Fizz\"    \n[13] \"13\"       \"14\"       \"FizzBuzz\" \"16\"       \"17\"       \"Fizz\"    \n[19] \"19\"       \"Buzz\""
  },
  {
    "objectID": "r_parallel/optimizations.html#replace-costly-operations-where-possible",
    "href": "r_parallel/optimizations.html#replace-costly-operations-where-possible",
    "title": "Optimizations",
    "section": "Replace costly operations where possible",
    "text": "Replace costly operations where possible\nNow imagine that we have a dataframe called dat with a first column called datvar filled with integers.\nWe want to write a function that will accept our dataframe as argument and play the fizz buzz game on the column datvar.\nOne could imagine the following function:\n\nf6 &lt;- function(dat) {\n  z &lt;- integer(length(dat[[1]]))\n  for(i in seq_along(dat[[1]])) {\n    if(dat[[1]][i] %% 3 == 0 && dat[[1]][i] %% 5 == 0) {\n      z[i] &lt;- \"FizzBuzz\"\n    } else if(dat[[1]][i] %% 3 == 0) {\n      z[i] &lt;- \"Fizz\"\n    } else if(dat[[1]][i] %% 5 == 0) {\n      z[i] &lt;- \"Buzz\"\n    } else {\n      z[i] &lt;- dat[[1]][i]\n    }\n  }\n  z\n}\n\nIndexing a column from a dataframe in this fashion is a very costly operation. It is much more efficient to index with the name of the column:\n\nf7 &lt;- function(dat) {\n  z &lt;- integer(length(dat$datvar))\n  for(i in seq_along(dat$datvar)) {\n    if(dat$datvar[i] %% 3 == 0 && dat$datvar[i] %% 5 == 0) {\n      z[i] &lt;- \"FizzBuzz\"\n    } else if(dat$datvar[i] %% 3 == 0) {\n      z[i] &lt;- \"Fizz\"\n    } else if(dat$datvar[i] %% 5 == 0) {\n      z[i] &lt;- \"Buzz\"\n    } else {\n      z[i] &lt;- dat$datvar[i]\n    }\n  }\n  z\n}\n\nNow, let’s create a random dataframe to benchmark f6() and f7():\n\nset.seed(123)\ndat &lt;- data.frame(datvar = round(runif(n, 1, n)))\nmark(f6(dat), f7(dat))\n\nWarning: Some expressions had a GC in every iteration; so filtering is\ndisabled.\n\n\n# A tibble: 2 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 f6(dat)       1.75s    1.75s     0.571    1.26MB     28.6\n2 f7(dat)    336.54ms 339.05ms     2.95     1.26MB     26.5"
  },
  {
    "objectID": "r_parallel/optimizations.html#avoid-repetitions-of-costly-operations",
    "href": "r_parallel/optimizations.html#avoid-repetitions-of-costly-operations",
    "title": "Optimizations",
    "section": "Avoid repetitions of costly operations",
    "text": "Avoid repetitions of costly operations\nThis made a big difference (speedup of 5), but notice that we are indexing the column 6 times in our function. Let’s remove the repetition of this operation:\n\nf8 &lt;- function(dat) {\n  var &lt;- dat$datvar\n  z &lt;- integer(length(var))\n  for(i in seq_along(var)) {\n    if(var[i] %% 3 == 0 && var[i] %% 5 == 0) {\n      z[i] &lt;- \"FizzBuzz\"\n    } else if(var[i] %% 3 == 0) {\n      z[i] &lt;- \"Fizz\"\n    } else if(var[i] %% 5 == 0) {\n      z[i] &lt;- \"Buzz\"\n    } else {\n      z[i] &lt;- var[i]\n    }\n  }\n  z\n}\n\nLet’s benchmark all 3 versions:\n\nmark(f6(dat), f7(dat), f8(dat))\n\nWarning: Some expressions had a GC in every iteration; so filtering is\ndisabled.\n\n\n# A tibble: 3 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 f6(dat)       1.66s    1.66s     0.603    1.15MB     29.0\n2 f7(dat)    331.71ms 333.81ms     3.00     1.15MB     27.0\n3 f8(dat)    128.37ms 128.79ms     7.67     1.25MB     19.2\n\n\nf8() gave us another speedup of almost 3 over f7(). f8() runs 14 times faster than our initial function!\n\nIndexing from a vector isn’t costly. There is thus no advantage at removing the repetition of that operation.\n\n\n\nYour turn:\n\nShow that this last statement is true."
  },
  {
    "objectID": "r_parallel/parallel_loops.html",
    "href": "r_parallel/parallel_loops.html",
    "title": "Parallel loops with foreach & doFuture",
    "section": "",
    "text": "The foreach package implements a looping construct without an explicit counter. It doesn’t require the preallocation of an output container, it brings to R an equivalent of the Python or Julia list comprehensions, and mostly, it allows for an easy execution of loops in parallel. Unlike loops, it creates variables (loops are used for their side-effect).\nWe will explore an example to calculate the sum of 1e3 random vectors of length 3.\nFirst, let’s launch an interactive job with a single task (by default Slurm grants one CPU per task, so we are asking for one CPU):\nsalloc --time=50 --mem-per-cpu=3700M --ntasks=1\n\nYou can see the full list of salloc options here.\n\nThen we can launch R interactively:\nR\n\nWe are now in the R terminal and can start typing R commands.\n\nLet’s load the foreach package:\n\nlibrary(foreach)\n\nBelow is a classic while loop:\n\nset.seed(2)\nresult1 &lt;- numeric(3)             # Preallocate output container\ni &lt;- 0                            # Initialise counter variable\n\nwhile(i &lt; 1e3) {\n  result1 &lt;- result1 + runif(3)   # Calculate the sum\n  i &lt;- i + 1                      # Update the counter\n}\n\nresult1\n\n[1] 509.4325 504.8698 496.6869\n\n\nHere is the equivalent code using foreach:\n\nset.seed(2)\nresult2 &lt;- foreach(i = 1:1e3, .combine = '+') %do% runif(3)\n\nresult2\n\n[1] 509.4325 504.8698 496.6869\n\n\nWe can verify that both expressions return the same result:\n\nidentical(result1, result2)\n\n[1] TRUE\n\n\nThe best part of foreach is that you can turn sequential loops into parallel ones.\nThere are many parallelization backends available: doFuture, doMC, doMPI, doParallel, doRedis, doRNG, doSNOW, and doAzureParallel.\nIn this lesson, we will use doFuture, a modern package which allows to evaluate foreach expressions following any of the strategies of the future package.\nSo first, what is the future package?"
  },
  {
    "objectID": "r_parallel/parallel_loops.html#the-foreach-package",
    "href": "r_parallel/parallel_loops.html#the-foreach-package",
    "title": "Parallel loops with foreach & doFuture",
    "section": "",
    "text": "The foreach package implements a looping construct without an explicit counter. It doesn’t require the preallocation of an output container, it brings to R an equivalent of the Python or Julia list comprehensions, and mostly, it allows for an easy execution of loops in parallel. Unlike loops, it creates variables (loops are used for their side-effect).\nWe will explore an example to calculate the sum of 1e3 random vectors of length 3.\nFirst, let’s launch an interactive job with a single task (by default Slurm grants one CPU per task, so we are asking for one CPU):\nsalloc --time=50 --mem-per-cpu=3700M --ntasks=1\n\nYou can see the full list of salloc options here.\n\nThen we can launch R interactively:\nR\n\nWe are now in the R terminal and can start typing R commands.\n\nLet’s load the foreach package:\n\nlibrary(foreach)\n\nBelow is a classic while loop:\n\nset.seed(2)\nresult1 &lt;- numeric(3)             # Preallocate output container\ni &lt;- 0                            # Initialise counter variable\n\nwhile(i &lt; 1e3) {\n  result1 &lt;- result1 + runif(3)   # Calculate the sum\n  i &lt;- i + 1                      # Update the counter\n}\n\nresult1\n\n[1] 509.4325 504.8698 496.6869\n\n\nHere is the equivalent code using foreach:\n\nset.seed(2)\nresult2 &lt;- foreach(i = 1:1e3, .combine = '+') %do% runif(3)\n\nresult2\n\n[1] 509.4325 504.8698 496.6869\n\n\nWe can verify that both expressions return the same result:\n\nidentical(result1, result2)\n\n[1] TRUE\n\n\nThe best part of foreach is that you can turn sequential loops into parallel ones.\nThere are many parallelization backends available: doFuture, doMC, doMPI, doParallel, doRedis, doRNG, doSNOW, and doAzureParallel.\nIn this lesson, we will use doFuture, a modern package which allows to evaluate foreach expressions following any of the strategies of the future package.\nSo first, what is the future package?"
  },
  {
    "objectID": "r_parallel/parallel_loops.html#the-future-package",
    "href": "r_parallel/parallel_loops.html#the-future-package",
    "title": "Parallel loops with foreach & doFuture",
    "section": "The future package",
    "text": "The future package\nA future is an object that acts as an abstract representation for a value in the future. A future can be resolved (if the value has been computed) or unresolved. If the value is queried while the future is unresolved, the process is blocked until the future is resolved. Futures thus allow for asynchronous and parallel evaluations.\nThe future package allows to evaluate futures sequentially or in various forms of parallelism while keeping code simple and consistent. The evaluation strategy is set thanks to the plan function:\n\nplan(sequential):\nFutures are evaluated sequentially in the current R session.\nplan(multisession):\nFutures are evaluated by new R sessions spawned in the background (multi-processing in shared memory).\nplan(multicore):\nFutures are evaluated in processes forked from the existing process (multi-processing in shared memory).\nplan(cluster):\nFutures are evaluated on an ad-hoc cluster (distributed parallelism across multiple nodes).\n\n\nConsistency\nTo ensure a consistent behaviour across plans, all evaluations are done in a local environment:\n\nlibrary(future)\n\na &lt;- 1\n\nb %&lt;-% {      # %&lt;-% is used instead of &lt;- to use futures\n  a &lt;- 2\n}\n\na\n\n[1] 1"
  },
  {
    "objectID": "r_parallel/parallel_loops.html#the-dofuture-package",
    "href": "r_parallel/parallel_loops.html#the-dofuture-package",
    "title": "Parallel loops with foreach & doFuture",
    "section": "The doFuture package",
    "text": "The doFuture package\nThe doFuture package allows to evaluate foreach expressions across the evaluation strategies of the future package very easily.\nLet’s load the doFuture package:\nlibrary(doFuture)\n\nThis automatically loads the foreach and future packages.\n\nWe need to choose an evaluation strategy for our futures (e.g. plan(multicore)):\nplan(multicore)\nTo run the code in parallel, we can now replace %do% with %dofuture%.\nThere is however one last twist: whenever you create random numbers in parallel, it isn’t enough to use set.seed() to ensure reproducibility. You also need to make sure to generate parallel-safe random numbers. Using the %seed% operator (with %seed% TRUE) or the option .options.future = list(seed = TRUE) pregenerates the random seeds for all iterations using L’Ecuyer-CMRG RNG streams1.1 L’Ecuyer, P. (1999). Good parameters and implementations for combined multiple recursive random number generators. Operations Research, 47, 159–164.\nHere are the two equivalent syntaxes:\nset.seed(2)\nresult3 &lt;- foreach(\n  i = 1:1e3,\n  .options.future = list(seed = TRUE),\n  .combine = '+'\n) %dofuture% {\n  runif(3)\n}\nset.seed(2)\nresult3 &lt;- foreach(i = 1:1e3, .combine = '+') %dofuture% {\n  runif(3)\n} %seed% TRUE\nOf course remember that we asked Slurm for a single CPU (--ntasks=1). So we don’t have the hardware to run any code in parallel with our current job.\nIt is now time to play with our code with all serial and parallel methods and do some benchmarking."
  },
  {
    "objectID": "r_parallel/parallel_loops.html#benchmarks",
    "href": "r_parallel/parallel_loops.html#benchmarks",
    "title": "Parallel loops with foreach & doFuture",
    "section": "Benchmarks",
    "text": "Benchmarks\nWith the overhead of parallelization, it doesn’t make sense to parallelize such a fast code: the parallel version will take longer than the serial one.\nLet’s artificially make our code much slower without adding any complexity that would distract us from the parallelization question. To do that, we will simply add a delay at each iteration:\n\nset.seed(2)\nresult2 &lt;- foreach(i = 1:1e3, .combine = '+') %do% {\n  Sys.sleep(0.01)         # Wait for 0.01s\n  runif(3)\n}\n\nNow, let’s load the bench package that we will use for benchmarking our various tests:\nlibrary(bench)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReference timing\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s first time this to get a reference:\nset.seed(2)\nbm &lt;- mark(\n  result2 &lt;- foreach(i = 1:1e3, .combine = '+') %do% {\n    Sys.sleep(0.01)\n    runif(3)\n  }\n)\n\nbm$median\n[1] 11.4s\n\n\nPlan sequential\nThis is the parallelizable foreach code, but run sequentially:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplan(sequential)     # Set the evaluation strategy\n\nset.seed(2)\nbm &lt;- mark(\n  result3 &lt;- foreach(i = 1:1e3, .combine = '+') %dofuture% {\n    Sys.sleep(0.01)\n    runif(3)\n  } %seed% TRUE\n)\n\nbm$median\n[1] 10.6s\n\nNo surprise: this is similar to the previous timing.\n\n\n\nMulti-processing in shared memory\n\nNumber of cores\nfuture provides availableCores() to detect the number of available cores:\navailableCores()\ncgroups.cpuset\n             1\n\nSimilar to parallel::detectCores().\n\nThis detects the number of CPU cores available to us on the current compute node, that is, what we can use for shared memory multi-processing. Since we asked for a single task (--ntasks=1) and since by default Slurm grants one CPU per task, we have a single CPU available.\nTo be able to run our code in parallel, we need to have access to at least 2 CPUs each. So let’s quit the R session (with Ctrl+D or quit()—when asked whether to save a workspace image, answer n), terminate our interactive job (also with Ctrl+D) and ask for a different job.\n\nDon’t forget to relinquish your interactive job with Ctrl+D otherwise it will be running for the full 50 min, making the hardware it uses unavailable to all of us until the job expires.\n\nThe cluster for this course is made of 40 nodes with 4 CPUs each. We want to test shared memory parallelism, so our job needs to stay within one node. We can thus ask for a maximum of 4 CPUs and we want to ensure that we aren’t getting them on different nodes.\nIf we all ask for 4 CPUs in an interactive session, we are fine if there are 40 or fewer of us. If we are too numerous, the first 40 people to ask for an interactive job will get it, but the remainder of us will have their job requests pending, waiting for resources to become available, for as long as the lucky 40 are running their session. That’s the big downside of interactive sessions.\nA better approach when we need a lot of resource is to write the code in a script and run it with sbatch. That way, everybody will get to run their code with minimal delay.\nOpen a text file (let’s call it rf.R since it creates a random forest object) with the text editor of your choice, for instance nano:\nnano rf.R\nWe will first play with it to see how many cores are available to us, so write in your script:\n\n\nrf.R\n\nlibrary(future)   # Don't forget to load the packages in your script\navailableCores()\n\nSave and close the text editor.\nNow, we want to create a shell script for Slurm. Let’s call it rf.sh:\nnano rf.sh\nIn it lives the hardware request and the code that needs to run:\n\n\nrf.sh\n\n#!/bin/bash\n#SBATCH --time=10             # 10 min\n#SBATCH --mem-per-cpu=3700M\n#SBATCH --nodes=1\n#SBATCH --cpus-per-task=4\n\nRscript rf.R                  # This is the code that we are running\n\n\nYou can see the full list of sbatch options here.\n\nSave and close the text editor.\nWe can now run the batch script:\nsbatch rf.sh\nYou can monitor it with sq, but this should be quasi instant. The result will be written to a file called slurm-xx.out with xx being the number of the job that just ran.\n\nYou can specify the output file name in the options of your sbatch script.\n\nTo see the result, we can simply print the content of that file to screen (you can run ls to see the list of files in the current directory):\ncat slurm-xx.out    # Replace xx by the job number\nsystem\n     4\nWe now have 4 CPUs available on one node, so we can test shared memory parallelism.\n\n\nPlan multisession\nShared memory multi-processing can be run with plan(multisession) that will spawn new R sessions in the background to evaluate futures.\nEdit the R script (with nano rf.R):\n\n\nrf.R\n\nlibrary(doFuture)\nlibrary(bench)\n\nplan(multisession)\n\nset.seed(2)\n\nbm &lt;- mark(\n  result3 &lt;- foreach(i = 1:1e3, .combine = '+') %dofuture% {\n    Sys.sleep(0.01)\n    runif(3)\n  } %seed% TRUE\n)\n\nbm$median\n\nRun the job with the new R script:\nsbatch rf.sh\nWe now get in the output file:\nLoading required package: foreach\nLoading required package: future\nWarning message:\nSome expressions had a GC in every iteration; so filtering is disabled.\n[1] 3.31s\n\nWe got a speedup of 11.4 / 3.31 = 3.4. Not bad considering that we have 4 CPU cores (the ideal speedup would be 4, but there is always some overhead to parallelization).\n\n\n\nPlan multicore\nShared memory multi-processing can also be run with plan(multicore) (except on Windows) that will fork the current R process to evaluate futures.\nLet’s modify our R script again:\n\n\nrf.R\n\nlibrary(doFuture)\nlibrary(bench)\n\nplan(multicore)\n\nset.seed(2)\n\nbm &lt;- mark(\n  result3 &lt;- foreach(i = 1:1e3, .combine = '+') %dofuture% {\n    Sys.sleep(0.01)\n    runif(3)\n  } %seed% TRUE\n)\n\nbm$median\n\nRun the job:\nsbatch rf.sh\nWe get:\nLoading required package: foreach\nLoading required package: future\nWarning message:\nSome expressions had a GC in every iteration; so filtering is disabled.\n[1] 1.89s\n\nWe got a similar speedup of 11.4 / 3.01 = 3.8. This time we are really close to the ideal speedup of 4!\n\n\n\n\nMulti-processing in distributed memory\n\nCreate a cluster of workers\nTo test parallel execution in distributed memory, let’s ask Slurm for 8 tasks by editing our rf.sh script:\n\n\nrf.sh\n\n#!/bin/bash\n#SBATCH --time=10\n#SBATCH --mem-per-cpu=3700M\n#SBATCH --ntasks=8\n\nRscript rf.R      # This is the code that we are running\n\nLet’s verify that we do get 8 tasks by accessing the SLURM_NTASKS environment variable from within R.\nEdit rf.R to contain the following:\n\n\nrf.R\n\nas.numeric(Sys.getenv(\"SLURM_NTASKS\"))\n\nRun the job:\nsbatch rf.sh\nWe get:\n[1] 8\nLet’s see which nodes we are using:\n\n\nrf.R\n\nsystem(\"srun hostname -s\", intern = T)\n\nWe get:\n[1] \"node1\" \"node1\" \"node1\" \"node1\" \"node2\" \"node2\" \"node2\" \"node2\"\nTo run the RandomForest code with distributed parallelism using 8 CPU cores across both nodes, we will need to create a cluster of workers. We do this with the makeCluster() function from the base R parallel package: we create a character vector with the names of the nodes our tasks are running on and pass this vector to the makeCluster() function:\n## Create a character vector with the nodes names\nhosts &lt;- system(\"srun hostname -s\", intern = T)\n\n## Create the cluster of workers\ncl &lt;- parallel::makeCluster(hosts)\nLet’s test it:\n\n\nrf.R\n\nlibrary(doFuture)\n\nhosts &lt;- system(\"srun hostname -s\", intern = T)\ncl &lt;- parallel::makeCluster(hosts)\n\ncl\n\nIf we run this code, we get:\nLoading required package: foreach\nLoading required package: future\nsocket cluster with 8 nodes on hosts ‘node1’, ‘node2’\n\nMake sure that your code has finished running before printing the output file. Remember that you can monitor the job with sq.\n\n\n\nPlan cluster\nWe can now run the code in distributed memory parallelism:\n\n\nrf.R\n\nlibrary(doFuture)\nlibrary(bench)\n\nhosts &lt;- system(\"srun hostname -s\", intern = T)\ncl &lt;- parallel::makeCluster(hosts)\nplan(cluster, workers = cl)\n\nset.seed(2)\n\nbm &lt;- mark(\n  result3 &lt;- foreach(i = 1:1e3, .combine = '+') %dofuture% {\n    Sys.sleep(0.01)\n    runif(3)\n  } %seed% TRUE\n)\n\nbm$median\n\nWe get:\nLoading required package: foreach\nLoading required package: future\nWarning message:\nSome expressions had a GC in every iteration; so filtering is disabled.\n[1] 1.94s\n\nSpeedup: 11.4 / 1.94 = 5.9. Here again, this is not bad with 8 CPU cores, considering the added overhead of message passing between both nodes.\n\nThe cluster of workers can be stopped with:\nparallel::stopCluster(cl)\nHere, this is not necessary since our job stops running as soon as the execution is complete, but in other systems, this will prevent you from monopolizing hardware unnecessarily."
  },
  {
    "objectID": "r_parallel/parallel_types.html",
    "href": "r_parallel/parallel_types.html",
    "title": "Types of parallelism",
    "section": "",
    "text": "There are various ways to run code in parallel and it is important to have a clear understanding of what each method entails."
  },
  {
    "objectID": "r_parallel/parallel_types.html#multi-threading",
    "href": "r_parallel/parallel_types.html#multi-threading",
    "title": "Types of parallelism",
    "section": "Multi-threading",
    "text": "Multi-threading\nWe talk about multi-threading when a single process (with its own memory) runs multiple threads.\nThe execution can happen in parallel—if each thread has access to a CPU core—or by alternating some of the threads on some CPU cores.\nBecause all threads in a process write to the same memory addresses, multi-threading can lead to race conditions.\nMulti-threading does not seem to be a common approach to parallelizing R code."
  },
  {
    "objectID": "r_parallel/parallel_types.html#multi-processing-in-shared-memory",
    "href": "r_parallel/parallel_types.html#multi-processing-in-shared-memory",
    "title": "Types of parallelism",
    "section": "Multi-processing in shared memory",
    "text": "Multi-processing in shared memory\nMulti-processing in shared memory happens when multiple processes execute code on multiple CPU cores of a single node (or a single machine).\nThe different processes need to communicate with each other, but because they are all running on the CPU cores of a single node, messages can pass via shared memory."
  },
  {
    "objectID": "r_parallel/parallel_types.html#multi-processing-in-distributed-memory",
    "href": "r_parallel/parallel_types.html#multi-processing-in-distributed-memory",
    "title": "Types of parallelism",
    "section": "Multi-processing in distributed memory",
    "text": "Multi-processing in distributed memory\nWhen processes involved in the execution of some code run on multiple nodes of a cluster, messages between them need to travel over the cluster interconnect. In that case, we talk about distributed memory."
  },
  {
    "objectID": "r_parallel/partition.html",
    "href": "r_parallel/partition.html",
    "title": "Partitioning data with multidplyr",
    "section": "",
    "text": "The package multidplyr provides simple techniques to partition data across a set of workers (multicore parallelism) on the same or different nodes."
  },
  {
    "objectID": "r_parallel/partition.html#create-a-cluster-of-workers",
    "href": "r_parallel/partition.html#create-a-cluster-of-workers",
    "title": "Partitioning data with multidplyr",
    "section": "Create a cluster of workers",
    "text": "Create a cluster of workers\nLet’s load the multidplyr package:\nlibrary(multidplyr)\nFirst of all, you need to create a set of worker:\ncl &lt;- new_cluster(4)\ncl\n4 session cluster [....]"
  },
  {
    "objectID": "r_parallel/partition.html#data-assignment",
    "href": "r_parallel/partition.html#data-assignment",
    "title": "Partitioning data with multidplyr",
    "section": "Data assignment",
    "text": "Data assignment\nThere are multiple ways to assign data to the workers.\n\nAssign the same value to each worker\nThis is done with the cluster_assign() function:\ncluster_assign(cl, a = 1:4)\nTo execute the code on each worker and return the result, you use the function cluster_call():\ncluster_call(cl, a)\n[[1]]\n[1] 1 2 3 4\n\n[[2]]\n[1] 1 2 3 4\n\n[[3]]\n[1] 1 2 3 4\n\n[[4]]\n[1] 1 2 3 4\ncluster_assign(cl, b = runif(4))\ncluster_call(cl, b)\n[[1]]\n[1] 0.93146519 0.75181518 0.33158435 0.02970799\n\n[[2]]\n[1] 0.93146519 0.75181518 0.33158435 0.02970799\n\n[[3]]\n[1] 0.93146519 0.75181518 0.33158435 0.02970799\n\n[[4]]\n[1] 0.93146519 0.75181518 0.33158435 0.02970799\n\n\nAssign different values to each worker\nFor this, use instead cluster_assign_each():\ncluster_assign_each(cl, c = 1:4)\ncluster_call(cl, c)\n[[1]]\n[1] 1\n\n[[2]]\n[1] 2\n\n[[3]]\n[1] 3\n\n[[4]]\n[1] 4\ncluster_assign_each(cl, d = runif(4))\ncluster_call(cl, d)\n[[1]]\n[1] 0.8892167\n\n[[2]]\n[1] 0.09334862\n\n[[3]]\n[1] 0.614763\n\n[[4]]\n[1] 0.6986541\n\n\nPartition vectors\ncluster_assign_partition() splits up a vector to assign about the same amount of data to each worker:\ncluster_assign_partition(cl, e = 1:10)\ncluster_call(cl, e)\n[[1]]\n[1] 1 2 3\n\n[[2]]\n[1] 4 5\n\n[[3]]\n[1] 6 7\n\n[[4]]\n[1]  8  9 10"
  },
  {
    "objectID": "r_parallel/performance.html",
    "href": "r_parallel/performance.html",
    "title": "Measuring performance:",
    "section": "",
    "text": "Before we talk about ways to improve performance, let’s see how to measure it."
  },
  {
    "objectID": "r_parallel/performance.html#when-should-you-care",
    "href": "r_parallel/performance.html#when-should-you-care",
    "title": "Measuring performance:",
    "section": "When should you care?",
    "text": "When should you care?\n\n“There is no doubt that the grail of efficiency leads to abuse. Programmers waste enormous amounts of time thinking about, or worrying about, the speed of noncritical parts of their programs, and these attempts at efficiency actually have a strong negative impact when debugging and maintenance are considered. We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil. Yet we should not pass up our opportunities in that critical 3%.”\n— Donald Knuth\n\nOptimizing code takes time, can lead to mistakes, and may make code harder to read. Consequently, not all code is worth optimizing and before jumping into optimizations, you need a strategy.\nYou should consider optimizations when:\n\nyou have debugged your code (optimization comes last, don’t optimize a code that doesn’t run),\nyou will run a section of code (e.g. a function) many times (your optimization efforts will really pay off),\na section of code is particularly slow.\n\nHow do you know which sections of your code are slow? Don’t rely on intuition. You need to profile your code to identify bottlenecks."
  },
  {
    "objectID": "r_parallel/performance.html#profiling",
    "href": "r_parallel/performance.html#profiling",
    "title": "Measuring performance:",
    "section": "Profiling",
    "text": "Profiling\n\n“It is often a mistake to make a priori judgments about what parts of a program are really critical, since the universal experience of programmers who have been using measurement tools has been that their intuitive guesses fail.”\n— Donald Knuth\n\nR comes with a profiler: Rprof().\nprofvis is a newer tool, built by posit (formerly RStudio Inc). Under the hood, it runs Rprof() to collect data, then produces an interactive html widget with a flame graph that allows for an easy visual identification of slow sections of code.\nWhile this tool integrates well within the RStudio IDE, it is not very well suited for remote work on a cluster. One option is to profile your code with small data on your own machine. Another option is to use the base profiler Rprof() directly as in this example."
  },
  {
    "objectID": "r_parallel/performance.html#benchmarking",
    "href": "r_parallel/performance.html#benchmarking",
    "title": "Measuring performance:",
    "section": "Benchmarking",
    "text": "Benchmarking\nOnce you have identified expressions that are particularly slow, you can use benchmarking tools to compare variations of the code.\nIn the most basic fashion, you can use system.time(), but this is limited and imprecise.\nThe microbenchmark package is a much better option. It gives the minimum time, lower quartile, mean, median, upper quartile, and maximum time of R expressions.\nThe newer bench package is very similar, but it has less overhead, is more accurate, and—for sequential code—gives information on memory usage and garbage collections. This is the package that we will use for this course.\nThe main function from this package is mark(). You can pass as argument(s) one or multiple expressions that you want to benchmark. By default, it ensures that all expressions output the same result. If you want to remove this test, add the argument check = FALSE.\nWhile mark() gives memory usage and garbage collection information for sequential code, this functionality is not yet implemented for parallel code. When benchmarking parallel expressions, we will have to use the argument memory = FALSE.\nYou will see many examples throughout this course."
  },
  {
    "objectID": "r_parallel/rcpp.html",
    "href": "r_parallel/rcpp.html",
    "title": "Writing C++ in R with Rcpp",
    "section": "",
    "text": "Sometimes, parallelization is not an option, either because the code is hard to parallelize or because of lack of hardware. In such cases, one way to increase speed is to replace slow R code with C++. The package Rcpp makes this particularly easy by creating mappings between both languages."
  },
  {
    "objectID": "r_parallel/rcpp.html#back-to-fibonacci",
    "href": "r_parallel/rcpp.html#back-to-fibonacci",
    "title": "Writing C++ in R with Rcpp",
    "section": "Back to Fibonacci",
    "text": "Back to Fibonacci\nDo you remember the Fibonacci numbers? Here was a naive implementation in R:\n\nfib &lt;- function(n) {\n  if(n == 0) return(0)\n  if(n == 1) return(1)\n  Recall(n - 1) + Recall(n - 2)\n}\n\nThis function gives the nth number in the sequence.\n\nExample:\n\n\nfib(30)\n\n[1] 832040"
  },
  {
    "objectID": "r_parallel/rcpp.html#rcpp",
    "href": "r_parallel/rcpp.html#rcpp",
    "title": "Writing C++ in R with Rcpp",
    "section": "Rcpp",
    "text": "Rcpp\nLet’s translate this function in C++ within R!\nFirst we need to load the Rcpp package:\n\nlibrary(Rcpp)\n\nWe then use the function cppFunction() to assign to an R function a function written in C++:\n\nfibRcpp &lt;- cppFunction( '\nint fibonacci(const int x) {\n   if (x == 0) return(0);\n   if (x == 1) return(1);\n   return (fibonacci(x - 1)) + fibonacci(x - 2);\n}\n' )\n\nWe can call our function as any R function:\nfibRcpp(30)\n[1] 832040\nWe can compare both functions:\nlibrary(bench)\n\nn &lt;- 30\nmark(fib(n), fibRcpp(n))\n# A tibble: 2 × 13\n  expression      min   median `itr/sec` mem_alloc `gc/sec` n_itr  n_gc\n  &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n1 fib(n)        1.66s    1.66s     0.601    44.7KB     22.8     1    38\n2 fibRcpp(n)   1.08ms   1.08ms   901.       2.49KB      0     451     0\n  total_time result    memory                 time            \n    &lt;bch:tm&gt; &lt;list&gt;    &lt;list&gt;                 &lt;list&gt;          \n1      1.66s &lt;dbl [1]&gt; &lt;Rprofmem [6,778 × 3]&gt; &lt;bench_tm [1]&gt;  \n2   500.37ms &lt;int [1]&gt; &lt;Rprofmem [1 × 3]&gt;     &lt;bench_tm [451]&gt;\n  gc                \n  &lt;list&gt;            \n1 &lt;tibble [1 × 3]&gt;  \n2 &lt;tibble [451 × 3]&gt;\nWarning message:\nSome expressions had a GC in every iteration; so filtering is disabled.\nThe speedup is 1,537, which is amazing.\nIn this particular example, we saw that memoisation gives an even more incredible speedup (35,000!), but while memoisation will only work in very specific situations (e.g. recursive function calls), using C++ code is a general method to provide speedup. It is particularly useful when:\n\nthere are large numbers of function calls (R is particularly slow with function calls),\nyou need data structures that are missing in R,\nyou want to create efficient packages (fast R packages are written in C++ and many use Rcpp).\n\n\nIn this example, we declared the C++ function directly in R. It is possible to use source files instead."
  },
  {
    "objectID": "r_parallel/resources_hpc.html",
    "href": "r_parallel/resources_hpc.html",
    "title": "Resources for HPC in R",
    "section": "",
    "text": "This section contains resources specific to high-performance R. For introductory/general R resources, see this page instead.\n\n\nCRAN Task Views\n\nCRAN Task Views give information on packages relevant to certain topics.\n\n\nHigh-Performance and Parallel Computing with R\n\n\n\nRunning R on the Alliance clusters\n\nThree relevant pages from the Alliance wiki:\n\n\nGetting started\nRunning jobs\nRunning R\n\n\n\nOnline books\n\nAdvanced R\nEfficient R programming\n\n\n\nRcpp\n\nDocumentation and examples"
  },
  {
    "objectID": "r_parallel/run_r_hpc.html",
    "href": "r_parallel/run_r_hpc.html",
    "title": "SSH login",
    "section": "",
    "text": "This section will show you how to access our temporary remote cluster through SSH."
  },
  {
    "objectID": "r_parallel/run_r_hpc.html#why-not-use-rstudio-server",
    "href": "r_parallel/run_r_hpc.html#why-not-use-rstudio-server",
    "title": "SSH login",
    "section": "Why not use RStudio server?",
    "text": "Why not use RStudio server?\nIn our introduction to R, we used an RStudio server running on a remote cluster. In this course, we will log in a similar remote supercomputer using Secure Shell, then run R scripts from the command line.\nWhy are we not making use of the interactivity of R which is an interpreted language and why are we not using the added comfort of an IDE? The short answer is: resource efficiency.\nOnce you have developed your code in an interactive fashion in the IDE of your choice using small hardware resources on a sample of your data, running scripts allows you to only request large resources when you need them (i.e. when your code is running). This prevents heavy resources from sitting idle when not in use, as would happen in an interactive session while you type, think, etc. It will save you money on commercial clusters and waiting time on the Alliance clusters.\nThis course being about high-performance R, let’s learn to use it through scripts."
  },
  {
    "objectID": "r_parallel/run_r_hpc.html#logging-in-the-temporary-cluster-through-ssh",
    "href": "r_parallel/run_r_hpc.html#logging-in-the-temporary-cluster-through-ssh",
    "title": "SSH login",
    "section": "Logging in the temporary cluster through SSH",
    "text": "Logging in the temporary cluster through SSH\nYou do not need to install anything on your machine for this course as we will provide access to a temporary remote cluster.\n\nA username, hostname, and password will be given to you during the workshop.\n\n\nNote that this temporary cluster will only be available for the duration of this course.\n\n\nOpen a terminal emulator\nWindows users:  Install the free version of MobaXTerm and launch it.\nMacOS users:   Launch Terminal.\nLinux users:     Open the terminal emulator of your choice.\n\n\nAccess the cluster through secure shell\n\nWindows users\nFollow the first 18% of this demo.\nFor “Remote host”, use the hostname we gave you.\nSelect the box “Specify username” and provide your username.\n\nNote that the password is entered through blind typing, meaning that you will not see anything happening as you type it. This is a Linux feature. While it is a little disturbing at first, do know that it is working. Make sure to type it slowly to avoid typos, then press the “enter” key on your keyboard.\n\n\n\nMacOS and Linux users\nIn the terminal, run:\nssh &lt;username&gt;@&lt;hostname&gt;\n\nReplace the username and hostname by their values. For instance:\nssh user021@somecluster.c3.ca\n\nYou will be asked a question, answer “Yes”.\nWhen prompted, type the password.\n\nNote that the password is entered through blind typing, meaning that you will not see anything happening as you type it. This is a Linux feature. While it is a little disturbing at first, do know that it is working. Make sure to type it slowly to avoid typos, then press the “enter” key on your keyboard.\n\n\n\n\nTroubleshooting\nProblems logging in are almost always due to typos. If you cannot log in, retry slowly, entering your password carefully."
  },
  {
    "objectID": "scivis/index.html",
    "href": "scivis/index.html",
    "title": "Scientific visualization with ParaView",
    "section": "",
    "text": "Date:\nFriday, June 23, 2023\nTime:\n9am–5pm (with a two-hour break from noon to 2pm)\nInstructor:\nAlex Razoumov (Simon Fraser University)\nPrerequisites:\nThis is an introductory course with no prior visualization experience required.\nSoftware:\nPlease install ParaView on your computer, and make sure you can start it before the course. As of this writing, the latest ParaView version is 5.11 – it should work nicely for our workshop. We will provide all sample data and codes for the exercises. Let us know before or during the course if you want to load your own dataset into ParaView.\n\n\n\nDownload today’s materials (in case of problems: alternative link 1 and alternative link 2) as a single ZIP file with slides (two PDFs), data and scripts inside. Unpack it where you can find it on your computer.\n\n\n\nInstall recent ParaView on your computer. Make sure it runs, i.e. you have installed the right version for your operating system and processor architecture."
  }
]